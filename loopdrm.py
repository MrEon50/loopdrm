import torch
import torch.nn as nn
import torch.optim as optim
import json
import numpy as np
from typing import List, Dict, Any, Tuple, Callable
import os
import math
import random
from dataclasses import dataclass
from enum import Enum

class SystemMode(Enum):
    EXPLORATION = "exploration"
    EXPLOITATION = "exploitation"

@dataclass
class RuleMetadata:
    """Metadane reguÅ‚y zgodnie ze wzorem DRM"""
    W: float  # Waga skutecznoÅ›ci (rezonans z dziaÅ‚aniem)
    C: float  # ZasiÄ™g kontekstowy 
    U: float  # UÅ¼ycie (aktywnoÅ›Ä‡)
    R: float  # Åšredni rezonans z pamiÄ™ciÄ… i obecnym FRZ
    creation_time: int = 0
    last_activation: int = 0
    success_count: int = 0
    total_activations: int = 0

class AdvancedDRM:
    """Zaawansowana implementacja DRM z matematycznymi wzorami"""
    
    def __init__(self, adaptation_threshold: float = 0.3, exploration_threshold: float = 0.5):
        self.rules = {}  # rule_id -> {'condition': callable, 'action': callable, 'metadata': RuleMetadata}
        self.T = 0  # Czas/iteracja
        self.adaptation_threshold = adaptation_threshold
        self.exploration_threshold = exploration_threshold
        self.mode = SystemMode.EXPLOITATION
        self.memory = []  # PamiÄ™Ä‡ poprzednich stanÃ³w
        self.current_FRZ = 0.0  # Obecny wskaÅºnik sukcesu
        self.rule_combinations = []  # Historia kombinacji reguÅ‚
        
    def add_rule(self, rule_id: str, condition: Callable, action: Callable, 
                 initial_W: float = 1.0, initial_C: float = 1.0):
        """Dodaj reguÅ‚Ä™ z metadanymi DRM"""
        metadata = RuleMetadata(
            W=initial_W,
            C=initial_C, 
            U=0.0,
            R=0.5,  # Neutralny rezonans na start
            creation_time=self.T
        )
        
        self.rules[rule_id] = {
            'condition': condition,
            'action': action,
            'metadata': metadata
        }
        
    def calculate_rule_strength(self, rule_id: str) -> float:
        """
        Oblicz siÅ‚Ä™ reguÅ‚y wedÅ‚ug wzoru:
        Si = Wi Â· log(Ci + 1) Â· (1 + Ui/T) Â· Ri
        """
        if rule_id not in self.rules:
            return 0.0
            
        meta = self.rules[rule_id]['metadata']
        
        # Zabezpieczenie przed dzieleniem przez zero
        time_factor = 1.0 if self.T == 0 else (1 + meta.U / max(self.T, 1))
        
        Si = (meta.W * 
              math.log(meta.C + 1) * 
              time_factor * 
              meta.R)
              
        return max(Si, 0.0)  # Nie moÅ¼e byÄ‡ ujemna
    
    def update_rule_metadata(self, rule_id: str, success: bool, context_diversity: float):
        """Aktualizuj metadane reguÅ‚y na podstawie wyniku"""
        if rule_id not in self.rules:
            return
            
        meta = self.rules[rule_id]['metadata']
        meta.total_activations += 1
        meta.last_activation = self.T
        meta.U += 1  # ZwiÄ™ksz uÅ¼ycie
        
        if success:
            meta.success_count += 1
            meta.W = min(meta.W * 1.1, 5.0)  # ZwiÄ™ksz wagÄ™ skutecznoÅ›ci (max 5.0)
        else:
            meta.W = max(meta.W * 0.9, 0.1)  # Zmniejsz wagÄ™ skutecznoÅ›ci (min 0.1)
            
        # Aktualizuj zasiÄ™g kontekstowy na podstawie rÃ³Å¼norodnoÅ›ci kontekstÃ³w
        meta.C = 0.8 * meta.C + 0.2 * context_diversity
        
        # Aktualizuj rezonans na podstawie sukcesu
        success_rate = meta.success_count / max(meta.total_activations, 1)
        meta.R = 0.7 * meta.R + 0.3 * success_rate
    
    def calculate_system_average_strength(self) -> float:
        """
        Oblicz Å›redniÄ… skutecznoÅ›Ä‡ caÅ‚ej DRM:
        SÌ„ = (1/n) Î£ Si
        """
        if not self.rules:
            return 0.0
            
        total_strength = sum(self.calculate_rule_strength(rule_id) 
                           for rule_id in self.rules.keys())
        return total_strength / len(self.rules)
    
    def adapt_system_mode(self):
        """Adaptacja trybu systemu na podstawie Å›redniej skutecznoÅ›ci"""
        avg_strength = self.calculate_system_average_strength()
        
        if avg_strength < self.exploration_threshold:
            if self.mode != SystemMode.EXPLORATION:
                print(f"ğŸ” PrzeÅ‚Ä…czanie na tryb EKSPLORACJI (SÌ„={avg_strength:.3f})")
                self.mode = SystemMode.EXPLORATION
        else:
            if self.mode != SystemMode.EXPLOITATION:
                print(f"âš¡ PrzeÅ‚Ä…czanie na tryb EKSPLOATACJI (SÌ„={avg_strength:.3f})")
                self.mode = SystemMode.EXPLOITATION
    
    def mutate_rule(self, rule_id: str):
        """Mutacja reguÅ‚y - zmiana zasiÄ™gu kontekstowego"""
        if rule_id not in self.rules:
            return
            
        meta = self.rules[rule_id]['metadata']
        # Mutacja Ci - dodaj losowy szum
        mutation_factor = random.uniform(0.8, 1.2)
        meta.C = max(0.1, min(meta.C * mutation_factor, 10.0))
        print(f"ğŸ§¬ Mutacja reguÅ‚y {rule_id}: nowy zasiÄ™g kontekstowy C={meta.C:.3f}")
    
    def create_combined_rule(self, rule_id1: str, rule_id2: str) -> str:
        """Tworzenie nowej reguÅ‚y przez kombinacjÄ™ dwÃ³ch istniejÄ…cych"""
        if rule_id1 not in self.rules or rule_id2 not in self.rules:
            return None
            
        new_rule_id = f"combined_{rule_id1}_{rule_id2}_{self.T}"
        
        # Kombinacja metadanych
        meta1 = self.rules[rule_id1]['metadata']
        meta2 = self.rules[rule_id2]['metadata']
        
        combined_W = (meta1.W + meta2.W) / 2
        combined_C = max(meta1.C, meta2.C)  # WiÄ™kszy zasiÄ™g
        combined_R = (meta1.R + meta2.R) / 2
        
        # Kombinacja warunkÃ³w (AND)
        def combined_condition(context):
            return (self.rules[rule_id1]['condition'](context) and 
                   self.rules[rule_id2]['condition'](context))
        
        # Kombinacja akcji
        def combined_action(context):
            self.rules[rule_id1]['action'](context)
            self.rules[rule_id2]['action'](context)
            print(f"ğŸ”— Wykonano kombinowanÄ… akcjÄ™: {rule_id1} + {rule_id2}")
        
        self.add_rule(new_rule_id, combined_condition, combined_action, 
                     combined_W, combined_C)
        
        print(f"âœ¨ Utworzono nowÄ… reguÅ‚Ä™ kombinowanÄ…: {new_rule_id}")
        return new_rule_id
    
    def apply_rules(self, context: List[float], FRZ: float) -> Dict[str, Any]:
        """Zastosuj reguÅ‚y z peÅ‚nÄ… logikÄ… DRM"""
        self.T += 1
        self.current_FRZ = FRZ
        self.memory.append({'context': context.copy(), 'FRZ': FRZ, 'time': self.T})
        
        # Ogranicz pamiÄ™Ä‡ do ostatnich 100 stanÃ³w
        if len(self.memory) > 100:
            self.memory.pop(0)
        
        applied_rules = []
        rule_strengths = {}
        
        # Oblicz siÅ‚y wszystkich reguÅ‚
        for rule_id in self.rules.keys():
            strength = self.calculate_rule_strength(rule_id)
            rule_strengths[rule_id] = strength
        
        # Sortuj reguÅ‚y wedÅ‚ug siÅ‚y
        sorted_rules = sorted(rule_strengths.items(), key=lambda x: x[1], reverse=True)
        
        # Zastosuj reguÅ‚y w kolejnoÅ›ci siÅ‚y
        for rule_id, strength in sorted_rules:
            if strength < self.adaptation_threshold:
                continue
                
            rule = self.rules[rule_id]
            try:
                if rule['condition'](context):
                    rule['action'](context)
                    applied_rules.append(rule_id)
                    
                    # Oblicz rÃ³Å¼norodnoÅ›Ä‡ kontekstu
                    context_diversity = self._calculate_context_diversity(context)
                    
                    # OkreÅ›l sukces na podstawie FRZ
                    success = FRZ > 0.5  # PrÃ³g sukcesu
                    
                    # Aktualizuj metadane
                    self.update_rule_metadata(rule_id, success, context_diversity)
                    
            except Exception as e:
                print(f"âŒ BÅ‚Ä…d w regule {rule_id}: {e}")
        
        # Adaptacja systemu
        self._perform_adaptation()
        
        # Adaptuj tryb systemu
        self.adapt_system_mode()
        
        return {
            'applied_rules': applied_rules,
            'rule_strengths': rule_strengths,
            'system_mode': self.mode.value,
            'average_strength': self.calculate_system_average_strength(),
            'total_rules': len(self.rules)
        }
    
    def _calculate_context_diversity(self, context: List[float]) -> float:
        """Oblicz rÃ³Å¼norodnoÅ›Ä‡ kontekstu w porÃ³wnaniu z pamiÄ™ciÄ…"""
        if len(self.memory) < 2:
            return 1.0
            
        # PorÃ³wnaj z ostatnimi 10 kontekstami
        recent_contexts = [mem['context'] for mem in self.memory[-10:]]
        
        diversities = []
        for past_context in recent_contexts:
            if len(past_context) == len(context):
                # Oblicz odlegÅ‚oÅ›Ä‡ euklidesowÄ…
                distance = np.linalg.norm(np.array(context) - np.array(past_context))
                diversities.append(distance)
        
        return np.mean(diversities) if diversities else 1.0
    
    def _perform_adaptation(self):
        """Wykonaj adaptacjÄ™ reguÅ‚ zgodnie z mechanizmem DRM"""
        rules_to_remove = []
        rules_to_mutate = []
        
        for rule_id in self.rules.keys():
            strength = self.calculate_rule_strength(rule_id)
            
            # ReguÅ‚y poniÅ¼ej progu
            if strength < self.adaptation_threshold:
                if random.random() < 0.3:  # 30% szans na mutacjÄ™
                    rules_to_mutate.append(rule_id)
                elif strength < self.adaptation_threshold * 0.5:  # Bardzo sÅ‚abe reguÅ‚y
                    rules_to_remove.append(rule_id)
        
        # UsuÅ„ sÅ‚abe reguÅ‚y
        for rule_id in rules_to_remove:
            del self.rules[rule_id]
            print(f"ğŸ—‘ï¸ UsuniÄ™to sÅ‚abÄ… reguÅ‚Ä™: {rule_id}")
        
        # Mutuj reguÅ‚y
        for rule_id in rules_to_mutate:
            self.mutate_rule(rule_id)
        
        # W trybie eksploracji - twÃ³rz nowe reguÅ‚y
        if self.mode == SystemMode.EXPLORATION and len(self.rules) >= 2:
            if random.random() < 0.2:  # 20% szans na kombinacjÄ™
                rule_ids = list(self.rules.keys())
                if len(rule_ids) >= 2:
                    rule1, rule2 = random.sample(rule_ids, 2)
                    self.create_combined_rule(rule1, rule2)
    
    def get_detailed_statistics(self) -> Dict[str, Any]:
        """Pobierz szczegÃ³Å‚owe statystyki DRM"""
        rule_stats = {}
        
        for rule_id, rule in self.rules.items():
            meta = rule['metadata']
            strength = self.calculate_rule_strength(rule_id)
            
            rule_stats[rule_id] = {
                'strength': strength,
                'effectiveness_weight': meta.W,
                'context_range': meta.C,
                'usage': meta.U,
                'resonance': meta.R,
                'success_rate': meta.success_count / max(meta.total_activations, 1),
                'total_activations': meta.total_activations,
                'age': self.T - meta.creation_time
            }
        
        return {
            'rules': rule_stats,
            'system_time': self.T,
            'system_mode': self.mode.value,
            'average_strength': self.calculate_system_average_strength(),
            'total_rules': len(self.rules),
            'memory_size': len(self.memory),
            'adaptation_threshold': self.adaptation_threshold,
            'exploration_threshold': self.exploration_threshold
        }

class EnhancedRLS:
    """Rozszerzony RLS z obliczaniem FRZ"""
    
    def __init__(self, threshold: float = 0.1, window_size: int = 10):
        self.threshold = threshold
        self.window_size = window_size
        self.history = []
        self.differences = []
        self.FRZ_history = []  # Historia wskaÅºnikÃ³w sukcesu
        
    def calculate_FRZ(self, loss: float, prediction_accuracy: float = None) -> float:
        """
        Oblicz wskaÅºnik FRZ (Function Resonance Zone)
        FRZ = 1 - normalized_loss + accuracy_bonus
        """
        # Normalizuj loss do zakresu 0-1
        if len(self.history) > 0:
            max_loss = max(self.history)
            min_loss = min(self.history)
            if max_loss > min_loss:
                normalized_loss = (loss - min_loss) / (max_loss - min_loss)
            else:
                normalized_loss = 0.0
        else:
            normalized_loss = 1.0
        # Oblicz FRZ
        base_FRZ = 1.0 - normalized_loss
        
        # Dodaj bonus za dokÅ‚adnoÅ›Ä‡ predykcji jeÅ›li dostÄ™pny
        accuracy_bonus = 0.0
        if prediction_accuracy is not None:
            accuracy_bonus = prediction_accuracy * 0.3  # 30% wpÅ‚yw dokÅ‚adnoÅ›ci
        
        FRZ = max(0.0, min(1.0, base_FRZ + accuracy_bonus))
        self.FRZ_history.append(FRZ)
        
        # Ogranicz historiÄ™ FRZ
        if len(self.FRZ_history) > self.window_size * 2:
            self.FRZ_history.pop(0)
            
        return FRZ
    
    def detect_difference(self, value: float) -> bool:
        """Wykryj rÃ³Å¼nice z obliczaniem FRZ"""
        self.history.append(value)
        
        if len(self.history) > self.window_size:
            self.history.pop(0)
            
        if len(self.history) < 2:
            return False
            
        recent_avg = np.mean(self.history[:-1])
        current_diff = abs(value - recent_avg)
        
        if current_diff > self.threshold:
            self.differences.append({
                'value': value,
                'average': recent_avg,
                'difference': current_diff,
                'timestamp': len(self.history)
            })
            return True
            
        return False
    
    def get_current_FRZ(self) -> float:
        """Pobierz aktualny FRZ"""
        return self.FRZ_history[-1] if self.FRZ_history else 0.5
    
    def get_average_FRZ(self) -> float:
        """Pobierz Å›redni FRZ z ostatniego okna"""
        if not self.FRZ_history:
            return 0.5
        window_FRZ = self.FRZ_history[-self.window_size:]
        return np.mean(window_FRZ)
    
    def get_statistics(self) -> Dict[str, Any]:
        """Pobierz statystyki RLS z FRZ"""
        return {
            'history_length': len(self.history),
            'differences_detected': len(self.differences),
            'current_average': np.mean(self.history) if self.history else 0,
            'threshold': self.threshold,
            'current_FRZ': self.get_current_FRZ(),
            'average_FRZ': self.get_average_FRZ(),
            'FRZ_trend': self._calculate_FRZ_trend()
        }
    
    def _calculate_FRZ_trend(self) -> str:
        """Oblicz trend FRZ (rosnÄ…cy/malejÄ…cy/stabilny)"""
        if len(self.FRZ_history) < 3:
            return "insufficient_data"
        
        recent_FRZ = self.FRZ_history[-3:]
        if recent_FRZ[-1] > recent_FRZ[0] + 0.05:
            return "improving"
        elif recent_FRZ[-1] < recent_FRZ[0] - 0.05:
            return "declining"
        else:
            return "stable"

class SimpleNeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.layer2 = nn.Linear(hidden_size, output_size)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = self.activation(self.layer1(x))
        x = self.dropout(x)
        x = self.layer2(x)
        return x

class AdvancedLoopDRMSystem:
    """Zaawansowany system LoopDRM z matematycznymi wzorami"""
    
    def __init__(self, input_size=10, hidden_size=20, output_size=3):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # Inicjalizuj sieÄ‡ neuronowÄ…
        self.model = SimpleNeuralNetwork(input_size, hidden_size, output_size)
        self.criterion = nn.MSELoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        
        # Inicjalizuj zaawansowane DRM i RLS
        self.drm = AdvancedDRM(adaptation_threshold=0.3, exploration_threshold=0.5)
        self.rls = EnhancedRLS(threshold=0.1)
        
        # Przechowywanie danych treningowych
        self.training_inputs = []
        self.training_targets = []
        
        # Metryki systemu
        self.training_history = []
        self.performance_metrics = []
        
        # Inicjalizuj zaawansowane reguÅ‚y DRM
        self._initialize_advanced_rules()
        
    def _initialize_advanced_rules(self):
        """Inicjalizuj zaawansowane reguÅ‚y DRM z matematycznymi wzorami"""
        
        # ReguÅ‚a 1: Detekcja wysokiej wariancji z adaptacyjnym progiem
        def adaptive_variance_condition(context):
            variance = np.var(context)
            # Adaptacyjny prÃ³g na podstawie historii
            if len(self.rls.history) > 5:
                hist_variance = np.var(self.rls.history[-5:])
                threshold = max(0.5, hist_variance * 1.5)
            else:
                threshold = 1.0
            return variance > threshold
            
        def variance_action(context):
            variance = np.var(context)
            print(f"ğŸ“Š Adaptacyjna detekcja wariancji: {variance:.4f}")
            # Dostosuj learning rate na podstawie wariancji
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = max(0.0001, min(0.01, 0.001 / (1 + variance)))
            
        self.drm.add_rule("adaptive_variance", adaptive_variance_condition, 
                         variance_action, initial_W=2.0, initial_C=1.5)
        
        # ReguÅ‚a 2: Detekcja trendu z rezonansem
        def trend_resonance_condition(context):
            if len(self.rls.FRZ_history) < 3:
                return False
            recent_FRZ = self.rls.FRZ_history[-3:]
            trend_strength = abs(recent_FRZ[-1] - recent_FRZ[0])
            return trend_strength > 0.2
            
        def trend_action(context):
            trend = self.rls._calculate_FRZ_trend()
            print(f"ğŸ“ˆ Trend rezonansu: {trend}")
            if trend == "declining":
                # ZwiÄ™ksz eksploracjÄ™ przy spadajÄ…cym trendzie
                self.drm.exploration_threshold *= 0.9
            elif trend == "improving":
                # Wzmocnij eksploatacjÄ™ przy poprawie
                self.drm.exploration_threshold *= 1.1
                
        self.drm.add_rule("trend_resonance", trend_resonance_condition,
                         trend_action, initial_W=1.8, initial_C=2.0)
        
        # ReguÅ‚a 3: Meta-reguÅ‚a optymalizacji
        def meta_optimization_condition(context):
            avg_strength = self.drm.calculate_system_average_strength()
            return avg_strength < 0.4  # Niska Å›rednia siÅ‚a systemu
            
        def meta_optimization_action(context):
            print("ğŸ”§ Meta-optymalizacja: dostrajanie systemu")
            # Zmniejsz progi adaptacji dla wiÄ™kszej elastycznoÅ›ci
            self.drm.adaptation_threshold *= 0.95
            # ZwiÄ™ksz szanse na mutacjÄ™
            for rule_id in self.drm.rules.keys():
                if random.random() < 0.1:  # 10% szans
                    self.drm.mutate_rule(rule_id)
                    
        self.drm.add_rule("meta_optimization", meta_optimization_condition,
                         meta_optimization_action, initial_W=2.5, initial_C=3.0)
        
        # ReguÅ‚a 4: Detekcja anomalii kontekstowych
        def context_anomaly_condition(context):
            if len(self.drm.memory) < 5:
                return False
            
            # Oblicz odlegÅ‚oÅ›Ä‡ od Å›redniego kontekstu
            recent_contexts = [mem['context'] for mem in self.drm.memory[-5:]]
            avg_context = np.mean(recent_contexts, axis=0)
            distance = np.linalg.norm(np.array(context) - avg_context)
            
            return distance > 2.0  # PrÃ³g anomalii
            
        def anomaly_action(context):
            print("ğŸš¨ Anomalia kontekstowa wykryta!")
            # PrzeÅ‚Ä…cz na tryb eksploracji
            self.drm.mode = SystemMode.EXPLORATION
            # ZwiÄ™ksz learning rate dla szybkiej adaptacji
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = min(param_group['lr'] * 1.5, 0.01)
                
        self.drm.add_rule("context_anomaly", context_anomaly_condition,
                         anomaly_action, initial_W=3.0, initial_C=1.0)
    
    def calculate_prediction_accuracy(self, outputs, targets):
        """Oblicz dokÅ‚adnoÅ›Ä‡ predykcji dla FRZ"""
        with torch.no_grad():
            # Dla regresji - uÅ¼yj odwrotnoÅ›ci Å›redniego bÅ‚Ä™du wzglÄ™dnego
            relative_errors = torch.abs((outputs - targets) / (torch.abs(targets) + 1e-8))
            mean_relative_error = torch.mean(relative_errors)
            accuracy = 1.0 / (1.0 + mean_relative_error.item())
            return accuracy
    
    def add_training_data(self):
        """Interaktywne dodawanie danych treningowych"""
        print("\n=== Wprowadzanie danych treningowych ===")
        
        try:
            print("1. WprowadÅº dane rÄ™cznie")
            print("2. Generuj dane losowo")
            print("3. Generuj dane z wzorcem (dla testowania DRM)")
            choice = input("Wybierz opcjÄ™ (1/2/3): ").strip()
            
            if choice == "1":
                print(f"WprowadÅº {self.input_size} wartoÅ›ci wejÅ›ciowych (oddzielone spacjami):")
                input_str = input().strip()
                inputs = [float(x) for x in input_str.split()]
                
                if len(inputs) != self.input_size:
                    print(f"BÅ‚Ä…d: Oczekiwano {self.input_size} wartoÅ›ci, otrzymano {len(inputs)}")
                    return
                
                print(f"WprowadÅº {self.output_size} wartoÅ›ci docelowych (oddzielone spacjami):")
                target_str = input().strip()
                targets = [float(x) for x in target_str.split()]
                
                if len(targets) != self.output_size:
                    print(f"BÅ‚Ä…d: Oczekiwano {self.output_size} wartoÅ›ci, otrzymano {len(targets)}")
                    return
                    
                self.training_inputs.append(inputs)
                self.training_targets.append(targets)
                print("Dane dodane pomyÅ›lnie!")
                
            elif choice == "2":
                num_samples = int(input("Ile prÃ³bek wygenerowaÄ‡? "))
                for _ in range(num_samples):
                    inputs = np.random.randn(self.input_size).tolist()
                    targets = np.random.randn(self.output_size).tolist()
                    self.training_inputs.append(inputs)
                    self.training_targets.append(targets)
                print(f"Wygenerowano {num_samples} prÃ³bek treningowych")
                
            elif choice == "3":
                num_samples = int(input("Ile prÃ³bek z wzorcem wygenerowaÄ‡? "))
                print("Generowanie danych z wzorcami dla testowania DRM...")
                
                for i in range(num_samples):
                    # TwÃ³rz dane z rÃ³Å¼nymi wzorcami
                    if i % 4 == 0:  # Wysokie wariancje
                        inputs = (np.random.randn(self.input_size) * 3).tolist()
                    elif i % 4 == 1:  # Niskie wartoÅ›ci
                        inputs = (np.random.randn(self.input_size) - 2).tolist()
                    elif i % 4 == 2:  # Outliers
                        inputs = np.random.randn(self.input_size).tolist()
                        inputs[0] = 10.0  # Outlier
                    else:  # Normalne dane
                        inputs = np.random.randn(self.input_size).tolist()
                    
                    # Targets na podstawie wzorca
                    targets = [np.mean(inputs), np.std(inputs), np.max(inputs)][:self.output_size]
                    
                    self.training_inputs.append(inputs)
                    self.training_targets.append(targets)
                    
                print(f"Wygenerowano {num_samples} prÃ³bek z wzorcami")
            else:
                print("NieprawidÅ‚owy wybÃ³r")
                return
                
        except ValueError as e:
            print(f"BÅ‚Ä…d: NieprawidÅ‚owy format danych - {e}")
        except Exception as e:
            print(f"BÅ‚Ä…d: {e}")
    
    def train_network(self):
        """Trenowanie sieci z zaawansowanym DRM"""
        if not self.training_inputs or not self.training_targets:
            print("Brak danych treningowych! Najpierw dodaj dane.")
            return
            
        print("\n=== Zaawansowane trenowanie z DRM ===")
        
        try:
            epochs = int(input("Liczba epok (domyÅ›lnie 100): ") or "100")
            
            # Konwertuj na tensory
            inputs_tensor = torch.FloatTensor(self.training_inputs)
            targets_tensor = torch.FloatTensor(self.training_targets)
            
            print(f"Rozpoczynanie treningu na {len(self.training_inputs)} prÃ³bkach...")
            print(f"Tryb poczÄ…tkowy DRM: {self.drm.mode.value}")
            
            for epoch in range(epochs):
                # Forward pass
                outputs = self.model(inputs_tensor)
                loss = self.criterion(outputs, targets_tensor)
                
                # Oblicz dokÅ‚adnoÅ›Ä‡ predykcji
                accuracy = self.calculate_prediction_accuracy(outputs, targets_tensor)
                
                # Oblicz FRZ
                FRZ = self.rls.calculate_FRZ(loss.item(), accuracy)
                
                # Backward pass and optimization
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                # Zastosuj zaawansowane reguÅ‚y DRM dla kaÅ¼dego kontekstu
                drm_results = []
                for i, input_data in enumerate(self.training_inputs):
                    result = self.drm.apply_rules(input_data, FRZ)
                    drm_results.append(result)
                
                # Wykryj rÃ³Å¼nice uÅ¼ywajÄ…c RLS
                difference_detected = self.rls.detect_difference(loss.item())
                
                # Zapisz metryki
                epoch_metrics = {
                    'epoch': epoch + 1,
                    'loss': loss.item(),
                    'accuracy': accuracy,
                    'FRZ': FRZ,
                    'system_mode': self.drm.mode.value,
                    'avg_rule_strength': self.drm.calculate_system_average_strength(),
                    'active_rules': len(self.drm.rules),
                    'difference_detected': difference_detected
                }
                self.training_history.append(epoch_metrics)
                
                # WyÅ›wietl postÄ™p
                if difference_detected or epoch % (epochs // 10) == 0 or epoch == epochs - 1:
                    mode_icon = "ğŸ”" if self.drm.mode == SystemMode.EXPLORATION else "âš¡"
                    diff_icon = "ğŸ”¥" if difference_detected else ""
                    
                    print(f'Epoka {epoch+1:3d}/{epochs} | '
                          f'Loss: {loss.item():.6f} | '
                          f'FRZ: {FRZ:.3f} | '
                          f'Acc: {accuracy:.3f} | '
                          f'{mode_icon} {self.drm.mode.value} | '
                          f'ReguÅ‚y: {len(self.drm.rules)} | '
                          f'SÌ„: {self.drm.calculate_system_average_strength():.3f} {diff_icon}')
                
                # Co 20 epok - pokaÅ¼ szczegÃ³Å‚y DRM
                if epoch > 0 and epoch % 20 == 0:
                    self._show_drm_status()
            
            print("\nâœ… Trenowanie zakoÅ„czone!")
            self._show_final_training_summary()
            
        except ValueError as e:
            print(f"BÅ‚Ä…d: NieprawidÅ‚owa liczba epok - {e}")
        except Exception as e:
            print(f"BÅ‚Ä…d podczas trenowania: {e}")
    
    def _show_drm_status(self):
        """PokaÅ¼ aktualny status DRM"""
        print(f"\n--- Status DRM (T={self.drm.T}) ---")
        stats = self.drm.get_detailed_statistics()
        
        print(f"Tryb systemu: {stats['system_mode']} | "
              f"Åšrednia siÅ‚a: {stats['average_strength']:.3f} | "
              f"ReguÅ‚y: {stats['total_rules']}")
        
        # PokaÅ¼ top 3 najsilniejsze reguÅ‚y
        rule_strengths = [(rule_id, data['strength']) 
                         for rule_id, data in stats['rules'].items()]
        rule_strengths.sort(key=lambda x: x[1], reverse=True)
        
        print("Top 3 najsilniejsze reguÅ‚y:")
        for i, (rule_id, strength) in enumerate(rule_strengths[:3]):
            rule_data = stats['rules'][rule_id]
            print(f"  {i+1}. {rule_id}: S={strength:.3f} "
                  f"(W={rule_data['effectiveness_weight']:.2f}, "
                  f"C={rule_data['context_range']:.2f}, "
                  f"R={rule_data['resonance']:.2f})")
    
    def _show_final_training_summary(self):
        """PokaÅ¼ podsumowanie trenowania"""
        if not self.training_history:
            return
            
        print("\n=== Podsumowanie trenowania ===")
        
        # Podstawowe statystyki
        final_metrics = self.training_history[-1]
        initial_loss = self.training_history[0]['loss']
        final_loss = final_metrics['loss']
        improvement = ((initial_loss - final_loss) / initial_loss) * 100
        
        print(f"ğŸ“Š Poprawa loss: {improvement:.1f}% "
              f"({initial_loss:.6f} â†’ {final_loss:.6f})")
        print(f"ğŸ¯ KoÅ„cowy FRZ: {final_metrics['FRZ']:.3f}")
        print(f"ğŸ”§ Tryb koÅ„cowy: {final_metrics['system_mode']}")
        print(f"ğŸ“‹ Aktywne reguÅ‚y: {final_metrics['active_rules']}")
        
        # Statystyki RLS
        rls_stats = self.rls.get_statistics()
        print(f"ğŸ” Wykryte rÃ³Å¼nice: {rls_stats['differences_detected']}")
        print(f"ğŸ“ˆ Trend FRZ: {rls_stats['FRZ_trend']}")
        
        # Statystyki DRM
        drm_stats = self.drm.get_detailed_statistics()
        print(f"âš¡ Åšrednia siÅ‚a systemu: {drm_stats['average_strength']:.3f}")
        
        # Analiza ewolucji reguÅ‚
        mode_changes = 0
        exploration_epochs = 0
        for i in range(1, len(self.training_history)):
            if (self.training_history[i]['system_mode'] != 
                self.training_history[i-1]['system_mode']):
                mode_changes += 1
            if self.training_history[i]['system_mode'] == 'exploration':
                exploration_epochs += 1
        
        print(f"ğŸ”„ Zmiany trybu: {mode_changes}")
        print(f"ğŸ” Epoki eksploracji: {exploration_epochs}/{len(self.training_history)} "
              f"({exploration_epochs/len(self.training_history)*100:.1f}%)")
    
    def save_model(self):
        """Zapisz model z peÅ‚nymi danymi DRM"""
        print("\n=== Zapisywanie zaawansowanego modelu ===")
        
        try:
            filename = input("Nazwa pliku (bez rozszerzenia, domyÅ›lnie 'advanced_model'): ").strip() or "advanced_model"
            
            # Przygotuj dane DRM do zapisu
            drm_data = {
                'rules_metadata': {},
                'system_time': self.drm.T,
                'mode': self.drm.mode.value,
                'adaptation_threshold': self.drm.adaptation_threshold,
                'exploration_threshold': self.drm.exploration_threshold,
                'memory': self.drm.memory,
                'rule_combinations': self.drm.rule_combinations
            }
            
            # Zapisz metadane reguÅ‚ (bez funkcji callable)
            for rule_id, rule in self.drm.rules.items():
                meta = rule['metadata']
                drm_data['rules_metadata'][rule_id] = {
                    'W': meta.W,
                    'C': meta.C,
                    'U': meta.U,
                    'R': meta.R,
                    'creation_time': meta.creation_time,
                    'last_activation': meta.last_activation,
                    'success_count': meta.success_count,
                    'total_activations': meta.total_activations
                }
            
            # Kompletne dane modelu
            model_data = {
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'input_size': self.input_size,
                'hidden_size': self.hidden_size,
                'output_size': self.output_size,
                'training_inputs': self.training_inputs,
                'training_targets': self.training_targets,
                'training_history': self.training_history,
                'drm_data': drm_data,
                'rls_data': {
                    'threshold': self.rls.threshold,
                    'window_size': self.rls.window_size,
                    'history': self.rls.history,
                    'differences': self.rls.differences,
                    'FRZ_history': self.rls.FRZ_history
                }
            }
            
            torch.save(model_data, f"{filename}.pth")
            print(f"âœ… Zaawansowany model zapisany jako {filename}.pth")
            print(f"ğŸ“Š Zapisano {len(self.training_history)} epok historii trenowania")
            print(f"ğŸ”§ Zapisano {len(drm_data['rules_metadata'])} reguÅ‚ DRM")
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas zapisywania: {e}")
    
    def load_model(self):
        """Wczytaj model z danymi DRM"""
        print("\n=== Wczytywanie zaawansowanego modelu ===")
        
        try:
            filename = input("Nazwa pliku (bez rozszerzenia): ").strip()
            
            if not os.path.exists(f"{filename}.pth"):
                print(f"âŒ Plik {filename}.pth nie istnieje!")
                return
            
            model_data = torch.load(f"{filename}.pth")
            
            # OdtwÃ³rz architekturÄ™ sieci
            self.input_size = model_data['input_size']
            self.hidden_size = model_data['hidden_size']
            self.output_size = model_data['output_size']
            
            self.model = SimpleNeuralNetwork(self.input_size, self.hidden_size, self.output_size)
            self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
            
            # Wczytaj stany
            self.model.load_state_dict(model_data['model_state_dict'])
            self.optimizer.load_state_dict(model_data['optimizer_state_dict'])
            
            # Wczytaj dane treningowe
            self.training_inputs = model_data.get('training_inputs', [])
            self.training_targets = model_data.get('training_targets', [])
            self.training_history = model_data.get('training_history', [])
            
            # OdtwÃ³rz DRM
            if 'drm_data' in model_data:
                drm_data = model_data['drm_data']
                self.drm.T = drm_data['system_time']
                self.drm.mode = SystemMode(drm_data['mode'])
                self.drm.adaptation_threshold = drm_data['adaptation_threshold']
                self.drm.exploration_threshold = drm_data['exploration_threshold']
                self.drm.memory = drm_data['memory']
                self.drm.rule_combinations = drm_data.get('rule_combinations', [])
                
                # OdtwÃ³rz metadane reguÅ‚ (reguÅ‚y bÄ™dÄ… ponownie zainicjalizowane)
                self._initialize_advanced_rules()  # OdtwÃ³rz funkcje reguÅ‚
                
                # PrzywrÃ³Ä‡ metadane
                for rule_id, meta_data in drm_data['rules_metadata'].items():
                    if rule_id in self.drm.rules:
                        meta = self.drm.rules[rule_id]['metadata']
                        meta.W = meta_data['W']
                        meta.C = meta_data['C']
                        meta.U = meta_data['U']
                        meta.R = meta_data['R']
                        meta.creation_time = meta_data['creation_time']
                        meta.last_activation = meta_data['last_activation']
                        meta.success_count = meta_data['success_count']
                        meta.total_activations = meta_data['total_activations']
            
            # OdtwÃ³rz RLS
            if 'rls_data' in model_data:
                rls_data = model_data['rls_data']
                self.rls.threshold = rls_data['threshold']
                self.rls.window_size = rls_data['window_size']
                self.rls.history = rls_data['history']
                self.rls.differences = rls_data['differences']
                self.rls.FRZ_history = rls_data['FRZ_history']
            
            print(f"âœ… Model wczytany z {filename}.pth")
            print(f"ğŸ—ï¸ Architektura: {self.input_size} â†’ {self.hidden_size} â†’ {self.output_size}")
            print(f"ğŸ“Š Dane treningowe: {len(self.training_inputs)} prÃ³bek")
            print(f"ğŸ“ˆ Historia: {len(self.training_history)} epok")
            print(f"ğŸ”§ DRM: {len(self.drm.rules)} reguÅ‚, T={self.drm.T}, tryb={self.drm.mode.value}")
            print(f"ğŸ¯ RLS: FRZ={self.rls.get_current_FRZ():.3f}")
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas wczytywania: {e}")
    
    def test_model(self):
        """Testuj model z analizÄ… DRM"""
        print("\n=== Testowanie modelu z analizÄ… DRM ===")
        
        try:
            print(f"WprowadÅº {self.input_size} wartoÅ›ci testowych (oddzielone spacjami):")
            input_str = input().strip()
            test_input = [float(x) for x in input_str.split()]
            
            if len(test_input) != self.input_size:
                print(f"âŒ BÅ‚Ä…d: Oczekiwano {self.input_size} wartoÅ›ci, otrzymano {len(test_input)}")
                return
            
            # Predykcja
            with torch.no_grad():
                input_tensor = torch.FloatTensor([test_input])
                output = self.model(input_tensor)
                prediction = output[0].tolist()
            
            print(f"ğŸ¯ Predykcja: {[f'{x:.4f}' for x in prediction]}")
            
            # Analiza DRM dla danych testowych
            print("\n--- Analiza DRM ---")
            current_FRZ = self.rls.get_current_FRZ()
            drm_result = self.drm.apply_rules(test_input, current_FRZ)
            
            print(f"ğŸ¯ Aktualny FRZ: {current_FRZ:.3f}")
            print(f"ğŸ”§ Tryb systemu: {drm_result['system_mode']}")
            print(f"âš¡ Åšrednia siÅ‚a systemu: {drm_result['average_strength']:.3f}")
            print(f"ğŸ“‹ Zastosowane reguÅ‚y: {len(drm_result['applied_rules'])}")
            
            if drm_result['applied_rules']:
                print("Aktywne reguÅ‚y:")
                for rule_id in drm_result['applied_rules']:
                    strength = drm_result['rule_strengths'][rule_id]
                    print(f"  â€¢ {rule_id}: siÅ‚a = {strength:.3f}")
            else:
                print("  Brak aktywnych reguÅ‚ dla tego kontekstu")
            
            # Analiza kontekstu
            print(f"\n--- Analiza kontekstu ---")
            context_stats = {
                'mean': np.mean(test_input),
                'std': np.std(test_input),
                'var': np.var(test_input),
                'min': np.min(test_input),
                'max': np.max(test_input),
                'range': np.max(test_input) - np.min(test_input)
            }
            
            for stat_name, value in context_stats.items():
                print(f"  {stat_name}: {value:.4f}")
            
            # PorÃ³wnanie z danymi treningowymi
            if self.training_inputs:
                print(f"\n--- PorÃ³wnanie z danymi treningowymi ---")
                training_means = [np.mean(inp) for inp in self.training_inputs]
                training_stds = [np.std(inp) for inp in self.training_inputs]
                
                mean_similarity = 1.0 / (1.0 + abs(context_stats['mean'] - np.mean(training_means)))
                std_similarity = 1.0 / (1.0 + abs(context_stats['std'] - np.mean(training_stds)))
                
                print(f"  PodobieÅ„stwo Å›redniej: {mean_similarity:.3f}")
                print(f"  PodobieÅ„stwo odchylenia: {std_similarity:.3f}")
                
                if mean_similarity < 0.5 or std_similarity < 0.5:
                    print("  âš ï¸ Dane testowe znacznie rÃ³Å¼niÄ… siÄ™ od treningowych!")
            
        except ValueError as e:
            print(f"âŒ BÅ‚Ä…d: NieprawidÅ‚owy format danych - {e}")
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas testowania: {e}")
    
    def show_advanced_statistics(self):
        """PokaÅ¼ zaawansowane statystyki systemu"""
        print("\n" + "="*60)
        print("    ZAAWANSOWANE STATYSTYKI SYSTEMU LOOPDRM")
        print("="*60)
        
        # Statystyki sieci neuronowej
        print(f"\nğŸ§  SIEÄ† NEURONOWA")
        print(f"Architektura: {self.input_size} â†’ {self.hidden_size} â†’ {self.output_size}")
        
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"Parametry: {total_params} (treningowe: {trainable_params})")
        
        if self.training_history:
            print(f"Epoki trenowania: {len(self.training_history)}")
            final_loss = self.training_history[-1]['loss']
            initial_loss = self.training_history[0]['loss']
            improvement = ((initial_loss - final_loss) / initial_loss) * 100
            print(f"Poprawa loss: {improvement:.2f}%")
        
        # Zaawansowane statystyki DRM
        print(f"\nğŸ”§ DYNAMICZNA MATRYCA REGUÅ (DRM)")
        drm_stats = self.drm.get_detailed_statistics()
        
        print(f"Czas systemowy (T): {drm_stats['system_time']}")
        print(f"Tryb systemu: {drm_stats['system_mode']}")
        print(f"Åšrednia siÅ‚a systemu (SÌ„): {drm_stats['average_strength']:.4f}")
        print(f"PrÃ³g adaptacji: {drm_stats['adaptation_threshold']:.3f}")
        print(f"PrÃ³g eksploracji: {drm_stats['exploration_threshold']:.3f}")
        print(f"Rozmiar pamiÄ™ci: {drm_stats['memory_size']}")
        
        print(f"\nğŸ“Š SZCZEGÃ“ÅY REGUÅ ({drm_stats['total_rules']} aktywnych):")
        print("-" * 80)
        print(f"{'ID ReguÅ‚y':<20} {'SiÅ‚a(Si)':<10} {'W':<8} {'C':<8} {'U':<8} {'R':<8} {'Sukces%':<10} {'Wiek':<6}")
        print("-" * 80)
        
        # Sortuj reguÅ‚y wedÅ‚ug siÅ‚y
        rules_by_strength = sorted(drm_stats['rules'].items(), 
                                 key=lambda x: x[1]['strength'], reverse=True)
        
        for rule_id, rule_data in rules_by_strength:
            print(f"{rule_id:<20} "
                  f"{rule_data['strength']:<10.3f} "
                  f"{rule_data['effectiveness_weight']:<8.2f} "
                  f"{rule_data['context_range']:<8.2f} "
                  f"{rule_data['usage']:<8.1f} "
                  f"{rule_data['resonance']:<8.3f} "
                  f"{rule_data['success_rate']*100:<10.1f} "
                  f"{rule_data['age']:<6}")
        
        # Statystyki RLS
        print(f"\nğŸ¯ SYSTEM UCZENIA REGUÅ (RLS)")
        rls_stats = self.rls.get_statistics()
        
        print(f"PrÃ³g detekcji: {rls_stats['threshold']}")
        print(f"Rozmiar okna: {self.rls.window_size}")
        print(f"Historia: {rls_stats['history_length']} punktÃ³w")
        print(f"Wykryte rÃ³Å¼nice: {rls_stats['differences_detected']}")
        print(f"Aktualny FRZ: {rls_stats['current_FRZ']:.4f}")
        print(f"Åšredni FRZ: {rls_stats['average_FRZ']:.4f}")
        print(f"Trend FRZ: {rls_stats['FRZ_trend']}")
        
        # Analiza historii trenowania
        if self.training_history:
            print(f"\nğŸ“ˆ ANALIZA HISTORII TRENOWANIA")
            
            # Statystyki trybÃ³w
            exploration_count = sum(1 for h in self.training_history 
                                  if h['system_mode'] == 'exploration')
            exploitation_count = len(self.training_history) - exploration_count
            
            print(f"Epoki eksploracji: {exploration_count} ({exploration_count/len(self.training_history)*100:.1f}%)")
            print(f"Epoki eksploatacji: {exploitation_count} ({exploitation_count/len(self.training_history)*100:.1f}%)")
            
            # Zmiany trybu
            mode_changes = 0
            for i in range(1, len(self.training_history)):
                if (self.training_history[i]['system_mode'] != 
                    self.training_history[i-1]['system_mode']):
                    mode_changes += 1
            
            print(f"Zmiany trybu: {mode_changes}")
            
            # Statystyki FRZ
            frz_values = [h['FRZ'] for h in self.training_history]
            print(f"FRZ - min: {min(frz_values):.3f}, max: {max(frz_values):.3f}, "
                  f"Å›redni: {np.mean(frz_values):.3f}")
            
            # Ewolucja liczby reguÅ‚
            rule_counts = [h['active_rules'] for h in self.training_history]
            print(f"ReguÅ‚y - min: {min(rule_counts)}, max: {max(rule_counts)}, "
                  f"koÅ„cowa: {rule_counts[-1]}")
            
            # Wykryte rÃ³Å¼nice
            differences = sum(1 for h in self.training_history if h['difference_detected'])
            print(f"Wykryte rÃ³Å¼nice: {differences} ({differences/len(self.training_history)*100:.1f}%)")
        
        # Analiza wzorcÃ³w matematycznych
        print(f"\nğŸ”¬ ANALIZA WZORCÃ“W MATEMATYCZNYCH")
        
        if drm_stats['rules']:
            # Korelacje miÄ™dzy parametrami reguÅ‚
            W_values = [data['effectiveness_weight'] for data in drm_stats['rules'].values()]
            C_values = [data['context_range'] for data in drm_stats['rules'].values()]
            R_values = [data['resonance'] for data in drm_stats['rules'].values()]
            S_values = [data['strength'] for data in drm_stats['rules'].values()]
            
            print(f"RozkÅ‚ad wag skutecznoÅ›ci (W): min={min(W_values):.2f}, "
                  f"max={max(W_values):.2f}, Å›rednia={np.mean(W_values):.2f}")
            print(f"RozkÅ‚ad zasiÄ™gu kontekstowego (C): min={min(C_values):.2f}, "
                  f"max={max(C_values):.2f}, Å›rednia={np.mean(C_values):.2f}")
            print(f"RozkÅ‚ad rezonansu (R): min={min(R_values):.3f}, "
                  f"max={max(R_values):.3f}, Å›rednia={np.mean(R_values):.3f}")
            print(f"RozkÅ‚ad siÅ‚y (S): min={min(S_values):.3f}, "
                  f"max={max(S_values):.3f}, Å›rednia={np.mean(S_values):.3f}")
            
            # SprawdÅº wzÃ³r Si = Wi Â· log(Ci + 1) Â· (1 + Ui/T) Â· Ri
            print(f"\nğŸ§® WERYFIKACJA WZORU MATEMATYCZNEGO:")
            print("Si = Wi Â· log(Ci + 1) Â· (1 + Ui/T) Â· Ri")
            
            for rule_id, rule_data in list(drm_stats['rules'].items())[:3]:  # PokaÅ¼ 3 przykÅ‚ady
                W = rule_data['effectiveness_weight']
                C = rule_data['context_range']
                U = rule_data['usage']
                R = rule_data['resonance']
                T = max(drm_stats['system_time'], 1)
                
                calculated_S = W * math.log(C + 1) * (1 + U/T) * R
                actual_S = rule_data['strength']
                
                print(f"  {rule_id}: obliczone={calculated_S:.4f}, "
                      f"rzeczywiste={actual_S:.4f}, rÃ³Å¼nica={abs(calculated_S-actual_S):.6f}")
        
        print("\n" + "="*60)
    
    def run_advanced_menu(self):
        """Zaawansowane menu gÅ‚Ã³wne"""
        while True:
            print("\n" + "="*60)
            print("    ğŸš€ ZAAWANSOWANY LOOPDRM - SYSTEM DRM Z ZAMKNIÄ˜TÄ„ PÄ˜TLÄ„")
            print("="*60)
            print("1. ğŸ“ Wprowadzenie danych treningowych")
            print("2. ğŸ¯ Trenowanie sieci z DRM")
            print("3. ğŸ’¾ Zapisanie modelu")
            print("4. ğŸ“‚ Wczytanie modelu")
            print("5. ğŸ§ª Testowanie modelu")
            print("6. ğŸ“Š Zaawansowane statystyki")
            print("7. ğŸ”§ Konfiguracja DRM")
            print("8. ğŸ“ˆ Analiza wydajnoÅ›ci")
            print("9. ğŸ” Eksport danych")
            print("0. ğŸšª ZakoÅ„czenie")
            print("-" * 60)
            
            try:
                choice = input("Wybierz opcjÄ™ (0-9): ").strip()
                
                if choice == "1":
                    self.add_training_data()
                elif choice == "2":
                    self.train_network()
                elif choice == "3":
                    self.save_model()
                elif choice == "4":
                    self.load_model()
                elif choice == "5":
                    self.test_model()
                elif choice == "6":
                    self.show_advanced_statistics()
                elif choice == "7":
                    self.configure_drm()
                elif choice == "8":
                    self.analyze_performance()
                elif choice == "9":
                    self.export_data()
                elif choice == "0":
                    print("ğŸ‰ DziÄ™kujemy za korzystanie z Zaawansowanego LoopDRM!")
                    print("ğŸ“Š System wykonaÅ‚ {} iteracji DRM".format(self.drm.T))
                    if self.training_history:
                        print("ğŸ† Najlepszy FRZ: {:.4f}".format(
                            max(h['FRZ'] for h in self.training_history)))
                    break
                else:
                    print("âŒ NieprawidÅ‚owy wybÃ³r. Wybierz opcjÄ™ 0-9.")
                    
            except KeyboardInterrupt:
                print("\n\nâš ï¸ Program przerwany przez uÅ¼ytkownika.")
                break
            except Exception as e:
                print(f"âŒ WystÄ…piÅ‚ nieoczekiwany bÅ‚Ä…d: {e}")
    
    def configure_drm(self):
        """Konfiguracja parametrÃ³w DRM"""
        print("\n=== Konfiguracja DRM ===")
        
        try:
            print(f"Aktualne ustawienia:")
            print(f"  PrÃ³g adaptacji: {self.drm.adaptation_threshold}")
            print(f"  PrÃ³g eksploracji: {self.drm.exploration_threshold}")
            print(f"  Tryb systemu: {self.drm.mode.value}")
            
            print("\nCo chcesz zmieniÄ‡?")
            print("1. PrÃ³g adaptacji")
            print("2. PrÃ³g eksploracji")
            print("3. Wymuszenie trybu systemu")
            print("4. Parametry RLS")
            print("5. Reset systemu DRM")
            print("0. PowrÃ³t")
            
            config_choice = input("Wybierz opcjÄ™: ").strip()
            
            if config_choice == "1":
                new_threshold = float(input(f"Nowy prÃ³g adaptacji (aktualny: {self.drm.adaptation_threshold}): "))
                if 0.0 <= new_threshold <= 1.0:
                    self.drm.adaptation_threshold = new_threshold
                    print(f"âœ… PrÃ³g adaptacji zmieniony na {new_threshold}")
                else:
                    print("âŒ PrÃ³g musi byÄ‡ w zakresie 0.0-1.0")
                    
            elif config_choice == "2":
                new_threshold = float(input(f"Nowy prÃ³g eksploracji (aktualny: {self.drm.exploration_threshold}): "))
                if 0.0 <= new_threshold <= 2.0:
                    self.drm.exploration_threshold = new_threshold
                    print(f"âœ… PrÃ³g eksploracji zmieniony na {new_threshold}")
                else:
                    print("âŒ PrÃ³g musi byÄ‡ w zakresie 0.0-2.0")
                    
            elif config_choice == "3":
                print("1. Eksploracja")
                print("2. Eksploatacja")
                mode_choice = input("Wybierz tryb: ").strip()
                
                if mode_choice == "1":
                    self.drm.mode = SystemMode.EXPLORATION
                    print("âœ… Wymuszono tryb eksploracji")
                elif mode_choice == "2":
                    self.drm.mode = SystemMode.EXPLOITATION
                    print("âœ… Wymuszono tryb eksploatacji")
                else:
                    print("âŒ NieprawidÅ‚owy wybÃ³r")
                    
            elif config_choice == "4":
                print(f"Aktualne parametry RLS:")
                print(f"  PrÃ³g: {self.rls.threshold}")
                print(f"  Rozmiar okna: {self.rls.window_size}")
                
                new_rls_threshold = float(input(f"Nowy prÃ³g RLS (aktualny: {self.rls.threshold}): "))
                new_window_size = int(input(f"Nowy rozmiar okna (aktualny: {self.rls.window_size}): "))
                
                if new_rls_threshold > 0 and new_window_size > 0:
                    self.rls.threshold = new_rls_threshold
                    self.rls.window_size = new_window_size
                    print("âœ… Parametry RLS zaktualizowane")
                else:
                    print("âŒ Parametry muszÄ… byÄ‡ dodatnie")
                    
            elif config_choice == "5":
                confirm = input("âš ï¸ Czy na pewno chcesz zresetowaÄ‡ system DRM? (tak/nie): ").strip().lower()
                if confirm in ['tak', 'yes', 'y']:
                    self.drm = AdvancedDRM(self.drm.adaptation_threshold, self.drm.exploration_threshold)
                    self._initialize_advanced_rules()
                    print("âœ… System DRM zostaÅ‚ zresetowany")
                else:
                    print("âŒ Reset anulowany")
                    
            elif config_choice == "0":
                return
            else:
                print("âŒ NieprawidÅ‚owy wybÃ³r")
                
        except ValueError as e:
            print(f"âŒ BÅ‚Ä…d: NieprawidÅ‚owa wartoÅ›Ä‡ - {e}")
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d konfiguracji: {e}")
    
    def analyze_performance(self):
        """Analiza wydajnoÅ›ci systemu"""
        print("\n=== Analiza wydajnoÅ›ci systemu ===")
        
        if not self.training_history:
            print("âŒ Brak danych historycznych do analizy")
            return
        
        try:
            # Analiza konwergencji
            print("ğŸ“ˆ ANALIZA KONWERGENCJI")
            losses = [h['loss'] for h in self.training_history]
            frz_values = [h['FRZ'] for h in self.training_history]
            
            # Oblicz trendy
            if len(losses) >= 10:
                recent_losses = losses[-10:]
                early_losses = losses[:10]
                
                recent_avg = np.mean(recent_losses)
                early_avg = np.mean(early_losses)
                improvement = ((early_avg - recent_avg) / early_avg) * 100
                
                print(f"  Poprawa loss (ostatnie 10 vs pierwsze 10): {improvement:.2f}%")
                
                # StabilnoÅ›Ä‡
                recent_std = np.std(recent_losses)
                print(f"  StabilnoÅ›Ä‡ (odchylenie ostatnich 10): {recent_std:.6f}")
                
                if recent_std < 0.001:
                    print("  âœ… System osiÄ…gnÄ…Å‚ wysokÄ… stabilnoÅ›Ä‡")
                elif recent_std < 0.01:
                    print("  âš ï¸ System jest umiarkowanie stabilny")
                else:
                    print("  âŒ System jest niestabilny")
            
            # Analiza efektywnoÅ›ci DRM
            print(f"\nğŸ”§ ANALIZA EFEKTYWNOÅšCI DRM")
            
            rule_strength_history = [h['avg_rule_strength'] for h in self.training_history]
            rule_count_history = [h['active_rules'] for h in self.training_history]
            
            print(f"  Åšrednia siÅ‚a reguÅ‚: {np.mean(rule_strength_history):.4f}")
            print(f"  StabilnoÅ›Ä‡ siÅ‚y reguÅ‚: {np.std(rule_strength_history):.4f}")
            print(f"  Åšrednia liczba reguÅ‚: {np.mean(rule_count_history):.1f}")
            print(f"  ZmiennoÅ›Ä‡ liczby reguÅ‚: {np.std(rule_count_history):.1f}")
            
            # Analiza trybÃ³w
            exploration_epochs = sum(1 for h in self.training_history if h['system_mode'] == 'exploration')
            exploitation_epochs = len(self.training_history) - exploration_epochs
            
            print(f"\nğŸ” ANALIZA TRYBÃ“W SYSTEMU")
            print(f"  Eksploracja: {exploration_epochs} epok ({exploration_epochs/len(self.training_history)*100:.1f}%)")
            print(f"  Eksploatacja: {exploitation_epochs} epok ({exploitation_epochs/len(self.training_history)*100:.1f}%)")
            
            # EfektywnoÅ›Ä‡ trybÃ³w
            exploration_frz = [h['FRZ'] for h in self.training_history if h['system_mode'] == 'exploration']
            exploitation_frz = [h['FRZ'] for h in self.training_history if h['system_mode'] == 'exploitation']
            
            if exploration_frz and exploitation_frz:
                print(f"  Åšredni FRZ w eksploracji: {np.mean(exploration_frz):.4f}")
                print(f"  Åšredni FRZ w eksploatacji: {np.mean(exploitation_frz):.4f}")
                
                if np.mean(exploitation_frz) > np.mean(exploration_frz):
                    print("  âœ… Eksploatacja jest bardziej efektywna")
                else:
                    print("  âš ï¸ Eksploracja daje lepsze wyniki")
            
            # Analiza rÃ³Å¼nic wykrytych przez RLS
            differences_detected = sum(1 for h in self.training_history if h['difference_detected'])
            print(f"\nğŸ¯ ANALIZA RLS")
            print(f"  Wykryte rÃ³Å¼nice: {differences_detected} ({differences_detected/len(self.training_history)*100:.1f}%)")
            
            if differences_detected > 0:
                # ZnajdÅº epoki z rÃ³Å¼nicami
                diff_epochs = [i for i, h in enumerate(self.training_history) if h['difference_detected']]
                
                # SprawdÅº czy rÃ³Å¼nice korelujÄ… z poprawÄ…
                improvements_after_diff = 0
                for epoch in diff_epochs:
                    if epoch < len(self.training_history) - 5:  # SprawdÅº 5 epok pÃ³Åºniej
                        before_frz = self.training_history[epoch]['FRZ']
                        after_frz = np.mean([self.training_history[i]['FRZ'] 
                                           for i in range(epoch+1, min(epoch+6, len(self.training_history)))])
                        if after_frz > before_frz:
                            improvements_after_diff += 1
                
                improvement_rate = improvements_after_diff / len(diff_epochs) * 100
                print(f"  Poprawa po wykryciu rÃ³Å¼nic: {improvement_rate:.1f}%")
                
                if improvement_rate > 60:
                    print("  âœ… RLS skutecznie wykrywa momenty poprawy")
                else:
                    print("  âš ï¸ RLS moÅ¼e wymagaÄ‡ dostrojenia")
            
            # Rekomendacje
            print(f"\nğŸ’¡ REKOMENDACJE")
            
            if improvement < 10:
                print("  â€¢ RozwaÅ¼ zwiÄ™kszenie learning rate lub liczby epok")
            
            if recent_std > 0.01:
                print("  â€¢ System jest niestabilny - rozwaÅ¼ zmniejszenie learning rate")
            
            if np.mean(rule_strength_history) < 0.5:
                print("  â€¢ Niska siÅ‚a reguÅ‚ - rozwaÅ¼ zmniejszenie progu adaptacji")
            
            if exploration_epochs / len(self.training_history) > 0.7:
                print("  â€¢ Zbyt duÅ¼o eksploracji - rozwaÅ¼ zwiÄ™kszenie progu eksploracji")
            elif exploration_epochs / len(self.training_history) < 0.1:
                print("  â€¢ Zbyt maÅ‚o eksploracji - rozwaÅ¼ zmniejszenie progu eksploracji")
            
            if differences_detected / len(self.training_history) > 0.5:
                print("  â€¢ RLS wykrywa zbyt wiele rÃ³Å¼nic - rozwaÅ¼ zwiÄ™kszenie progu")
            elif differences_detected / len(self.training_history) < 0.05:
                print("  â€¢ RLS wykrywa zbyt maÅ‚o rÃ³Å¼nic - rozwaÅ¼ zmniejszenie progu")
                
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas analizy: {e}")
    
    def export_data(self):
        """Eksport danych do analizy"""
        print("\n=== Eksport danych ===")
        
        try:
            print("1. Eksport historii trenowania (JSON)")
            print("2. Eksport statystyk DRM (JSON)")
            print("3. Eksport danych do CSV")
            print("4. Eksport peÅ‚nego raportu (TXT)")
            print("0. PowrÃ³t")
            
            export_choice = input("Wybierz opcjÄ™: ").strip()
            
            if export_choice == "1":
                filename = input("Nazwa pliku (bez rozszerzenia): ").strip() or "training_history"
                
                with open(f"{filename}.json", 'w', encoding='utf-8') as f:
                    json.dump(self.training_history, f, indent=2, ensure_ascii=False)
                
                print(f"âœ… Historia trenowania zapisana do {filename}.json")
                
            elif export_choice == "2":
                filename = input("Nazwa pliku (bez rozszerzenia): ").strip() or "drm_statistics"
                
                drm_stats = self.drm.get_detailed_statistics()
                
                with open(f"{filename}.json", 'w', encoding='utf-8') as f:
                    json.dump(drm_stats, f, indent=2, ensure_ascii=False)
                
                print(f"âœ… Statystyki DRM zapisane do {filename}.json")
                
            elif export_choice == "3":
                filename = input("Nazwa pliku (bez rozszerzenia): ").strip() or "training_data"
                
                if not self.training_history:
                    print("âŒ Brak danych do eksportu")
                    return
                
                import csv
                
                with open(f"{filename}.csv", 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    
                    # NagÅ‚Ã³wki
                    headers = ['epoch', 'loss', 'accuracy', 'FRZ', 'system_mode', 
                              'avg_rule_strength', 'active_rules', 'difference_detected']
                    writer.writerow(headers)
                    
                    # Dane
                    for h in self.training_history:
                        row = [h.get(header, '') for header in headers]
                        writer.writerow(row)
                
                print(f"âœ… Dane treningowe zapisane do {filename}.csv")
                
            elif export_choice == "4":
                filename = input("Nazwa pliku (bez rozszerzenia): ").strip() or "full_report"
                
                with open(f"{filename}.txt", 'w', encoding='utf-8') as f:
                    f.write("PEÅNY RAPORT SYSTEMU LOOPDRM\n")
                    f.write("=" * 50 + "\n\n")
                    
                    # Podstawowe informacje
                    f.write(f"Data generowania: {np.datetime64('now')}\n")
                    f.write(f"Architektura sieci: {self.input_size} â†’ {self.hidden_size} â†’ {self.output_size}\n")
                    f.write(f"Liczba prÃ³bek treningowych: {len(self.training_inputs)}\n")
                    f.write(f"Liczba epok: {len(self.training_history)}\n\n")
                    
                    # Statystyki DRM
                    drm_stats = self.drm.get_detailed_statistics()
                    f.write("STATYSTYKI DRM:\n")
                    f.write(f"Czas systemowy: {drm_stats['system_time']}\n")
                    f.write(f"Tryb: {drm_stats['system_mode']}\n")
                    f.write(f"Åšrednia siÅ‚a: {drm_stats['average_strength']:.4f}\n")
                    f.write(f"Liczba reguÅ‚: {drm_stats['total_rules']}\n")
                    f.write(f"PrÃ³g adaptacji: {drm_stats['adaptation_threshold']}\n")
                    f.write(f"PrÃ³g eksploracji: {drm_stats['exploration_threshold']}\n\n")
                    
                    # SzczegÃ³Å‚y reguÅ‚
                    f.write("SZCZEGÃ“ÅY REGUÅ:\n")
                    f.write("-" * 80 + "\n")
                    f.write(f"{'ID':<20} {'SiÅ‚a':<10} {'W':<8} {'C':<8} {'U':<8} {'R':<8} {'Sukces%':<10} {'Wiek':<6}\n")
                    f.write("-" * 80 + "\n")
                    
                    for rule_id, rule_data in drm_stats['rules'].items():
                        f.write(f"{rule_id:<20} "
                               f"{rule_data['strength']:<10.3f} "
                               f"{rule_data['effectiveness_weight']:<8.2f} "
                               f"{rule_data['context_range']:<8.2f} "
                               f"{rule_data['usage']:<8.1f} "
                               f"{rule_data['resonance']:<8.3f} "
                               f"{rule_data['success_rate']*100:<10.1f} "
                               f"{rule_data['age']:<6}\n")
                    
                    # Statystyki RLS
                    rls_stats = self.rls.get_statistics()
                    f.write(f"\nSTATYSTYKI RLS:\n")
                    f.write(f"PrÃ³g: {rls_stats['threshold']}\n")
                    f.write(f"Rozmiar okna: {self.rls.window_size}\n")
                    f.write(f"Wykryte rÃ³Å¼nice: {rls_stats['differences_detected']}\n")
                    f.write(f"Aktualny FRZ: {rls_stats['current_FRZ']:.4f}\n")
                    f.write(f"Åšredni FRZ: {rls_stats['average_FRZ']:.4f}\n")
                    f.write(f"Trend FRZ: {rls_stats['FRZ_trend']}\n\n")
                    
                    # Historia trenowania (ostatnie 20 epok)
                    if self.training_history:
                        f.write("OSTATNIE 20 EPOK TRENOWANIA:\n")
                        f.write("-" * 60 + "\n")
                        f.write(f"{'Epoka':<8} {'Loss':<12} {'FRZ':<8} {'Tryb':<12} {'ReguÅ‚y':<8}\n")
                        f.write("-" * 60 + "\n")
                        
                        for h in self.training_history[-20:]:
                            f.write(f"{h['epoch']:<8} "
                                   f"{h['loss']:<12.6f} "
                                   f"{h['FRZ']:<8.3f} "
                                   f"{h['system_mode']:<12} "
                                   f"{h['active_rules']:<8}\n")
                
                print(f"âœ… PeÅ‚ny raport zapisany do {filename}.txt")
                
            elif export_choice == "0":
                return
            else:
                print("âŒ NieprawidÅ‚owy wybÃ³r")
                
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas eksportu: {e}")

# Funkcja gÅ‚Ã³wna do uruchomienia systemu
def main():
    """Funkcja gÅ‚Ã³wna uruchamiajÄ…ca zaawansowany system LoopDRM"""
    print("ğŸš€ Inicjalizacja Zaawansowanego Systemu LoopDRM...")
    
    try:
        # Zapytaj o konfiguracjÄ™ sieci
        print("\n=== Konfiguracja sieci neuronowej ===")
        input_size = int(input("Rozmiar warstwy wejÅ›ciowej (domyÅ›lnie 10): ") or "10")
        hidden_size = int(input("Rozmiar warstwy ukrytej (domyÅ›lnie 20): ") or "20")
        output_size = int(input("Rozmiar warstwy wyjÅ›ciowej (domyÅ›lnie 3): ") or "3")
        
        # Inicjalizuj system
        system = AdvancedLoopDRMSystem(input_size, hidden_size, output_size)
        
        print(f"\nâœ… System zainicjalizowany!")
        print(f"ğŸ§  Architektura sieci: {input_size} â†’ {hidden_size} â†’ {output_size}")
        print(f"ğŸ”§ DRM: {len(system.drm.rules)} reguÅ‚ poczÄ…tkowych")
        print(f"ğŸ¯ RLS: prÃ³g = {system.rls.threshold}, okno = {system.rls.window_size}")
        
        # Uruchom menu gÅ‚Ã³wne
        system.run_advanced_menu()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Program przerwany przez uÅ¼ytkownika.")
    except ValueError as e:
        print(f"âŒ BÅ‚Ä…d konfiguracji: {e}")
        print("UÅ¼ywam domyÅ›lnych wartoÅ›ci...")
        system = AdvancedLoopDRMSystem()
        system.run_advanced_menu()
    except Exception as e:
        print(f"âŒ Krytyczny bÅ‚Ä…d systemu: {e}")
        print("SprawdÅº instalacjÄ™ wymaganych bibliotek:")
        print("pip install torch numpy")

# Dodatkowe funkcje pomocnicze
def create_demo_system():
    """UtwÃ³rz system demonstracyjny z przykÅ‚adowymi danymi"""
    print("ğŸ­ Tworzenie systemu demonstracyjnego...")
    
    system = AdvancedLoopDRMSystem(input_size=5, hidden_size=10, output_size=2)
    
    # Dodaj przykÅ‚adowe dane
    np.random.seed(42)  # Dla powtarzalnoÅ›ci
    for i in range(50):
        # RÃ³Å¼ne wzorce danych
        if i % 3 == 0:  # Wzorzec liniowy
            inputs = np.linspace(-2, 2, 5).tolist()
            targets = [np.sum(inputs), np.mean(inputs)]
        elif i % 3 == 1:  # Wzorzec kwadratowy
            inputs = (np.random.randn(5) ** 2).tolist()
            targets = [np.max(inputs), np.std(inputs)]
        else:  # Wzorzec losowy
            inputs = np.random.randn(5).tolist()
            targets = [np.median(inputs), np.var(inputs)]
        
        system.training_inputs.append(inputs)
        system.training_targets.append(targets)
    
    print(f"âœ… System demonstracyjny utworzony z {len(system.training_inputs)} prÃ³bkami")
    return system

def run_benchmark():
    """Uruchom test wydajnoÅ›ci systemu"""
    print("â±ï¸ Uruchamianie testu wydajnoÅ›ci...")
    
    import time
    
    system = AdvancedLoopDRMSystem(input_size=8, hidden_size=16, output_size=4)
    
    # Generuj dane testowe
    start_time = time.time()
    
    for i in range(100):
        inputs = np.random.randn(8).tolist()
        targets = np.random.randn(4).tolist()
        system.training_inputs.append(inputs)
        system.training_targets.append(targets)
    
    data_gen_time = time.time() - start_time
    
    # Test trenowania
    start_time = time.time()
    
    inputs_tensor = torch.FloatTensor(system.training_inputs)
    targets_tensor = torch.FloatTensor(system.training_targets)
    
    for epoch in range(50):
        outputs = system.model(inputs_tensor)
        loss = system.criterion(outputs, targets_tensor)
        
        system.optimizer.zero_grad()
        loss.backward()
        system.optimizer.step()
        
        # Symuluj DRM
        FRZ = system.rls.calculate_FRZ(loss.item())
        for input_data in system.training_inputs[:5]:  # Tylko pierwsze 5 dla szybkoÅ›ci
            system.drm.apply_rules(input_data, FRZ)
    
    training_time = time.time() - start_time
    
    print(f"ğŸ“Š Wyniki benchmarku:")
    print(f"  Generowanie danych: {data_gen_time:.4f}s")
    print(f"  Trenowanie (50 epok): {training_time:.4f}s")
    print(f"  Åšredni czas na epokÄ™: {training_time/50:.4f}s")
    print(f"  ReguÅ‚y DRM: {len(system.drm.rules)}")
    print(f"  Czas systemowy DRM: {system.drm.T}")

# Uruchomienie programu
if __name__ == "__main__":
    print("ğŸŒŸ Witaj w Zaawansowanym Systemie LoopDRM!")
    print("=" * 60)
    print("Ten system implementuje:")
    print("â€¢ ğŸ”§ DynamicznÄ… MatrycÄ™ ReguÅ‚ (DRM) z matematycznymi wzorami")
    print("â€¢ ğŸ¯ Rozszerzony System Uczenia ReguÅ‚ (RLS) z FRZ")
    print("â€¢ ğŸ§  SieÄ‡ neuronowÄ… z adaptacyjnym uczeniem")
    print("â€¢ ğŸ“Š ZaawansowanÄ… analizÄ™ i eksport danych")
    print("=" * 60)
    
    print("\nWybierz tryb uruchomienia:")
    print("1. ğŸš€ Normalny start")
    print("2. ğŸ­ System demonstracyjny")
    print("3. â±ï¸ Test wydajnoÅ›ci")
    print("0. ğŸšª WyjÅ›cie")
    
    try:
        choice = input("\nTwÃ³j wybÃ³r: ").strip()
        
        if choice == "1":
            main()
        elif choice == "2":
            demo_system = create_demo_system()
            demo_system.run_advanced_menu()
        elif choice == "3":
            run_benchmark()
        elif choice == "0":
            print("ğŸ‘‹ Do zobaczenia!")
        else:
            print("âŒ NieprawidÅ‚owy wybÃ³r, uruchamiam normalny start...")
            main()
            
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Program zakoÅ„czony przez uÅ¼ytkownika.")
    except Exception as e:
        print(f"\nâŒ BÅ‚Ä…d: {e}")
        print("Uruchamiam system w trybie awaryjnym...")
        try:
            system = AdvancedLoopDRMSystem()
            system.run_advanced_menu()
        except:
            print("âŒ Nie moÅ¼na uruchomiÄ‡ systemu. SprawdÅº instalacjÄ™ bibliotek.")
            print("\nğŸ”§ Wymagane biblioteki:")
            print("  â€¢ torch (PyTorch)")
            print("  â€¢ numpy")
            print("  â€¢ json (wbudowana)")
            print("  â€¢ dataclasses (wbudowana)")
            print("  â€¢ enum (wbudowana)")
            print("\nğŸ“¦ Instalacja:")
            print("  pip install torch numpy")
            print("\nğŸ†˜ JeÅ›li problem nadal wystÄ™puje, sprawdÅº:")
            print("  â€¢ WersjÄ™ Pythona (wymagana 3.7+)")
            print("  â€¢ DostÄ™pnoÅ›Ä‡ pamiÄ™ci RAM")
            print("  â€¢ Uprawnienia do zapisu plikÃ³w")

# Klasy pomocnicze i dodatkowe funkcjonalnoÅ›ci

class DRMVisualizer:
    """Klasa do wizualizacji dziaÅ‚ania DRM"""
    
    def __init__(self, drm_system):
        self.drm = drm_system
        
    def print_rule_evolution(self, history_length=10):
        """WyÅ›wietl ewolucjÄ™ reguÅ‚"""
        print(f"\nğŸ“ˆ EWOLUCJA REGUÅ (ostatnie {history_length} krokÃ³w)")
        print("=" * 80)
        
        if len(self.drm.memory) < history_length:
            print("âš ï¸ NiewystarczajÄ…ca historia do analizy")
            return
        
        # Analiza zmian siÅ‚y reguÅ‚ w czasie
        recent_memory = self.drm.memory[-history_length:]
        
        for i, mem_point in enumerate(recent_memory):
            print(f"\nKrok {mem_point['time']} (FRZ: {mem_point['FRZ']:.3f}):")
            
            # Oblicz siÅ‚y reguÅ‚ dla tego momentu
            temp_T = self.drm.T
            self.drm.T = mem_point['time']
            
            rule_strengths = {}
            for rule_id in self.drm.rules.keys():
                strength = self.drm.calculate_rule_strength(rule_id)
                rule_strengths[rule_id] = strength
            
            # PrzywrÃ³Ä‡ oryginalny czas
            self.drm.T = temp_T
            
            # PokaÅ¼ top 3 reguÅ‚y
            sorted_rules = sorted(rule_strengths.items(), key=lambda x: x[1], reverse=True)
            for j, (rule_id, strength) in enumerate(sorted_rules[:3]):
                bar_length = int(strength * 20)  # Skala 0-20 znakÃ³w
                bar = "â–ˆ" * bar_length + "â–‘" * (20 - bar_length)
                print(f"  {j+1}. {rule_id:<20} [{bar}] {strength:.3f}")
    
    def print_context_analysis(self, context):
        """Analiza kontekstu wejÅ›ciowego"""
        print(f"\nğŸ” ANALIZA KONTEKSTU")
        print("=" * 50)
        
        print(f"WartoÅ›ci: {[f'{x:.3f}' for x in context]}")
        print(f"Statystyki:")
        print(f"  â€¢ Åšrednia: {np.mean(context):.4f}")
        print(f"  â€¢ Odchylenie: {np.std(context):.4f}")
        print(f"  â€¢ Minimum: {np.min(context):.4f}")
        print(f"  â€¢ Maksimum: {np.max(context):.4f}")
        print(f"  â€¢ Zakres: {np.max(context) - np.min(context):.4f}")
        
        # PorÃ³wnanie z historiÄ…
        if len(self.drm.memory) > 0:
            print(f"\nPorÃ³wnanie z historiÄ…:")
            historical_contexts = [mem['context'] for mem in self.drm.memory[-10:]]
            
            if historical_contexts:
                avg_historical = np.mean([np.mean(ctx) for ctx in historical_contexts])
                current_avg = np.mean(context)
                
                difference = abs(current_avg - avg_historical)
                similarity = 1.0 / (1.0 + difference)
                
                print(f"  â€¢ PodobieÅ„stwo do historii: {similarity:.3f}")
                
                if similarity > 0.8:
                    print("  âœ… Kontekst bardzo podobny do historycznych")
                elif similarity > 0.5:
                    print("  âš ï¸ Kontekst umiarkowanie podobny")
                else:
                    print("  ğŸš¨ Kontekst znacznie rÃ³Å¼ni siÄ™ od historycznych")

class PerformanceMonitor:
    """Monitor wydajnoÅ›ci systemu"""
    
    def __init__(self):
        self.start_time = None
        self.metrics = {}
        self.checkpoints = []
        
    def start_monitoring(self):
        """Rozpocznij monitorowanie"""
        import time
        self.start_time = time.time()
        self.metrics = {
            'total_time': 0,
            'epochs_processed': 0,
            'rules_created': 0,
            'rules_deleted': 0,
            'mode_switches': 0,
            'memory_usage': []
        }
        print("ğŸ“Š Monitoring wydajnoÅ›ci rozpoczÄ™ty")
        
    def checkpoint(self, description=""):
        """UtwÃ³rz punkt kontrolny"""
        if self.start_time is None:
            return
            
        import time
        current_time = time.time()
        elapsed = current_time - self.start_time
        
        checkpoint_data = {
            'time': elapsed,
            'description': description,
            'timestamp': current_time
        }
        
        self.checkpoints.append(checkpoint_data)
        print(f"â±ï¸ Checkpoint: {description} ({elapsed:.3f}s)")
        
    def update_metrics(self, **kwargs):
        """Aktualizuj metryki"""
        for key, value in kwargs.items():
            if key in self.metrics:
                if isinstance(self.metrics[key], list):
                    self.metrics[key].append(value)
                else:
                    self.metrics[key] = value
                    
    def get_memory_usage(self):
        """Pobierz uÅ¼ycie pamiÄ™ci (jeÅ›li dostÄ™pne)"""
        try:
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            return memory_mb
        except ImportError:
            return None
            
    def print_summary(self):
        """WyÅ›wietl podsumowanie wydajnoÅ›ci"""
        if self.start_time is None:
            print("âŒ Monitoring nie zostaÅ‚ rozpoczÄ™ty")
            return
            
        import time
        total_time = time.time() - self.start_time
        
        print(f"\nğŸ“Š PODSUMOWANIE WYDAJNOÅšCI")
        print("=" * 50)
        print(f"CaÅ‚kowity czas: {total_time:.3f}s")
        print(f"Przetworzone epoki: {self.metrics.get('epochs_processed', 0)}")
        
        if self.metrics.get('epochs_processed', 0) > 0:
            avg_time_per_epoch = total_time / self.metrics['epochs_processed']
            print(f"Åšredni czas na epokÄ™: {avg_time_per_epoch:.4f}s")
            
        print(f"Utworzone reguÅ‚y: {self.metrics.get('rules_created', 0)}")
        print(f"UsuniÄ™te reguÅ‚y: {self.metrics.get('rules_deleted', 0)}")
        print(f"Zmiany trybu: {self.metrics.get('mode_switches', 0)}")
        
        # UÅ¼ycie pamiÄ™ci
        memory_usage = self.get_memory_usage()
        if memory_usage:
            print(f"UÅ¼ycie pamiÄ™ci: {memory_usage:.1f} MB")
            
        # Checkpointy
        if self.checkpoints:
            print(f"\nPunkty kontrolne:")
            for i, cp in enumerate(self.checkpoints):
                print(f"  {i+1}. {cp['description']}: {cp['time']:.3f}s")

class ConfigManager:
    """MenedÅ¼er konfiguracji systemu"""
    
    @staticmethod
    def save_config(system, filename="config.json"):
        """Zapisz konfiguracjÄ™ systemu"""
        config = {
            'network': {
                'input_size': system.input_size,
                'hidden_size': system.hidden_size,
                'output_size': system.output_size
            },
            'drm': {
                'adaptation_threshold': system.drm.adaptation_threshold,
                'exploration_threshold': system.drm.exploration_threshold,
                'mode': system.drm.mode.value
            },
            'rls': {
                'threshold': system.rls.threshold,
                'window_size': system.rls.window_size
            },
            'optimizer': {
                'lr': system.optimizer.param_groups[0]['lr']
            }
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            print(f"âœ… Konfiguracja zapisana do {filename}")
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d zapisu konfiguracji: {e}")
            
    @staticmethod
    def load_config(filename="config.json"):
        """Wczytaj konfiguracjÄ™"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… Konfiguracja wczytana z {filename}")
            return config
        except FileNotFoundError:
            print(f"âŒ Plik {filename} nie istnieje")
            return None
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d wczytywania konfiguracji: {e}")
            return None
            
    @staticmethod
    def create_system_from_config(config):
        """UtwÃ³rz system na podstawie konfiguracji"""
        if not config:
            return None
            
        try:
            # UtwÃ³rz system
            system = AdvancedLoopDRMSystem(
                input_size=config['network']['input_size'],
                hidden_size=config['network']['hidden_size'],
                output_size=config['network']['output_size']
            )
            
            # Zastosuj konfiguracjÄ™ DRM
            system.drm.adaptation_threshold = config['drm']['adaptation_threshold']
            system.drm.exploration_threshold = config['drm']['exploration_threshold']
            system.drm.mode = SystemMode(config['drm']['mode'])
            
            # Zastosuj konfiguracjÄ™ RLS
            system.rls.threshold = config['rls']['threshold']
            system.rls.window_size = config['rls']['window_size']
            
            # Zastosuj konfiguracjÄ™ optymalizatora
            for param_group in system.optimizer.param_groups:
                param_group['lr'] = config['optimizer']['lr']
                
            print("âœ… System utworzony na podstawie konfiguracji")
            return system
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d tworzenia systemu z konfiguracji: {e}")
            return None

# Funkcje narzÄ™dziowe
def validate_system_integrity(system):
    """SprawdÅº integralnoÅ›Ä‡ systemu"""
    print("\nğŸ” SPRAWDZANIE INTEGRALNOÅšCI SYSTEMU")
    print("=" * 50)
    
    issues = []
    
    # SprawdÅº sieÄ‡ neuronowÄ…
    try:
        test_input = torch.randn(1, system.input_size)
        output = system.model(test_input)
        if output.shape[1] != system.output_size:
            issues.append("NieprawidÅ‚owy rozmiar wyjÅ›cia sieci")
    except Exception as e:
        issues.append(f"BÅ‚Ä…d sieci neuronowej: {e}")
    
    # SprawdÅº DRM
    if len(system.drm.rules) == 0:
        issues.append("Brak reguÅ‚ w DRM")
        
    for rule_id, rule in system.drm.rules.items():
        if not callable(rule['condition']) or not callable(rule['action']):
            issues.append(f"NieprawidÅ‚owa reguÅ‚a: {rule_id}")
            
        strength = system.drm.calculate_rule_strength(rule_id)
        if strength < 0:
            issues.append(f"Ujemna siÅ‚a reguÅ‚y: {rule_id}")
    
    # SprawdÅº RLS
    if system.rls.threshold <= 0:
        issues.append("NieprawidÅ‚owy prÃ³g RLS")
        
    if system.rls.window_size <= 0:
        issues.append("NieprawidÅ‚owy rozmiar okna RLS")
    
    # SprawdÅº dane treningowe
    if system.training_inputs and system.training_targets:
        if len(system.training_inputs) != len(system.training_targets):
            issues.append("NiezgodnoÅ›Ä‡ liczby danych wejÅ›ciowych i docelowych")
            
        for i, (inp, tgt) in enumerate(zip(system.training_inputs, system.training_targets)):
            if len(inp) != system.input_size:
                issues.append(f"NieprawidÅ‚owy rozmiar danych wejÅ›ciowych #{i}")
            if len(tgt) != system.output_size:
                issues.append(f"NieprawidÅ‚owy rozmiar danych docelowych #{i}")
    
    # WyÅ›wietl wyniki
    if issues:
        print("âŒ ZNALEZIONE PROBLEMY:")
        for i, issue in enumerate(issues, 1):
            print(f"  {i}. {issue}")
        return False
    else:
        print("âœ… System jest w peÅ‚ni sprawny")
        return True

def generate_test_data(input_size, output_size, num_samples=100, pattern="mixed"):
    """Generuj dane testowe z rÃ³Å¼nymi wzorcami"""
    inputs = []
    targets = []
    
    np.random.seed(42)  # Dla powtarzalnoÅ›ci
    
    for i in range(num_samples):
        if pattern == "linear":
            # Wzorzec liniowy
            inp = np.linspace(-2, 2, input_size).tolist()
            tgt = [np.sum(inp[:output_size])]
            while len(tgt) < output_size:
                tgt.append(np.mean(inp))
                
        
        
        elif pattern == "quadratic":
            # Wzorzec kwadratowy
            inp = (np.random.randn(input_size) ** 2).tolist()
            tgt = [np.max(inp[:output_size])]
            while len(tgt) < output_size:
                tgt.append(np.std(inp))
                
        elif pattern == "sinusoidal":
            # Wzorzec sinusoidalny
            x = np.linspace(0, 2*np.pi, input_size)
            inp = np.sin(x + i * 0.1).tolist()
            tgt = [np.mean(inp), np.amplitude(inp) if output_size > 1 else np.mean(inp)]
            while len(tgt) < output_size:
                tgt.append(np.var(inp))
                
        elif pattern == "mixed":
            # Wzorzec mieszany
            if i % 4 == 0:
                inp = np.random.randn(input_size).tolist()
                tgt = [np.mean(inp), np.std(inp)]
            elif i % 4 == 1:
                inp = (np.random.randn(input_size) * 2 + 1).tolist()
                tgt = [np.median(inp), np.max(inp)]
            elif i % 4 == 2:
                inp = np.random.exponential(1, input_size).tolist()
                tgt = [np.min(inp), np.sum(inp)]
            else:
                inp = np.random.uniform(-3, 3, input_size).tolist()
                tgt = [np.var(inp), np.mean(np.abs(inp))]
            
            # Dopasuj rozmiar targets
            while len(tgt) < output_size:
                tgt.append(np.random.randn())
            tgt = tgt[:output_size]
            
        else:  # random
            inp = np.random.randn(input_size).tolist()
            tgt = np.random.randn(output_size).tolist()
        
        inputs.append(inp)
        targets.append(tgt)
    
    return inputs, targets

def run_automated_test():
    """Uruchom zautomatyzowany test systemu"""
    print("ğŸ¤– ZAUTOMATYZOWANY TEST SYSTEMU")
    print("=" * 50)
    
    # UtwÃ³rz system testowy
    system = AdvancedLoopDRMSystem(input_size=6, hidden_size=12, output_size=3)
    monitor = PerformanceMonitor()
    
    monitor.start_monitoring()
    
    # SprawdÅº integralnoÅ›Ä‡
    monitor.checkpoint("Sprawdzanie integralnoÅ›ci")
    if not validate_system_integrity(system):
        print("âŒ Test przerwany - problemy z integralnoÅ›ciÄ…")
        return
    
    # Wygeneruj dane testowe
    monitor.checkpoint("Generowanie danych")
    inputs, targets = generate_test_data(6, 3, 80, "mixed")
    system.training_inputs = inputs
    system.training_targets = targets
    
    print(f"âœ… Wygenerowano {len(inputs)} prÃ³bek danych")
    
    # Trenowanie automatyczne
    monitor.checkpoint("RozpoczÄ™cie trenowania")
    
    inputs_tensor = torch.FloatTensor(system.training_inputs)
    targets_tensor = torch.FloatTensor(system.training_targets)
    
    epochs = 30
    print(f"ğŸ¯ Trenowanie przez {epochs} epok...")
    
    for epoch in range(epochs):
        # Forward pass
        outputs = system.model(inputs_tensor)
        loss = system.criterion(outputs, targets_tensor)
        
        # Oblicz dokÅ‚adnoÅ›Ä‡
        accuracy = system.calculate_prediction_accuracy(outputs, targets_tensor)
        
        # Oblicz FRZ
        FRZ = system.rls.calculate_FRZ(loss.item(), accuracy)
        
        # Backward pass
        system.optimizer.zero_grad()
        loss.backward()
        system.optimizer.step()
        
        # Zastosuj DRM dla prÃ³bki danych
        sample_contexts = system.training_inputs[::10]  # Co 10-ta prÃ³bka
        for context in sample_contexts:
            system.drm.apply_rules(context, FRZ)
        
        # Wykryj rÃ³Å¼nice
        difference_detected = system.rls.detect_difference(loss.item())
        
        # Zapisz metryki
        epoch_metrics = {
            'epoch': epoch + 1,
            'loss': loss.item(),
            'accuracy': accuracy,
            'FRZ': FRZ,
            'system_mode': system.drm.mode.value,
            'avg_rule_strength': system.drm.calculate_system_average_strength(),
            'active_rules': len(system.drm.rules),
            'difference_detected': difference_detected
        }
        system.training_history.append(epoch_metrics)
        
        # Aktualizuj monitor
        monitor.update_metrics(epochs_processed=epoch + 1)
        
        # WyÅ›wietl postÄ™p co 10 epok
        if epoch % 10 == 0 or epoch == epochs - 1:
            print(f"Epoka {epoch+1:2d}/{epochs} | "
                  f"Loss: {loss.item():.6f} | "
                  f"FRZ: {FRZ:.3f} | "
                  f"ReguÅ‚y: {len(system.drm.rules)} | "
                  f"Tryb: {system.drm.mode.value}")
    
    monitor.checkpoint("ZakoÅ„czenie trenowania")
    
    # Test predykcji
    monitor.checkpoint("Test predykcji")
    test_input = np.random.randn(6).tolist()
    
    with torch.no_grad():
        input_tensor = torch.FloatTensor([test_input])
        prediction = system.model(input_tensor)[0].tolist()
    
    print(f"\nğŸ§ª Test predykcji:")
    print(f"WejÅ›cie: {[f'{x:.3f}' for x in test_input]}")
    print(f"Predykcja: {[f'{x:.3f}' for x in prediction]}")
    
    # Analiza DRM
    current_FRZ = system.rls.get_current_FRZ()
    drm_result = system.drm.apply_rules(test_input, current_FRZ)
    
    print(f"\nğŸ”§ Analiza DRM:")
    print(f"Zastosowane reguÅ‚y: {len(drm_result['applied_rules'])}")
    print(f"Åšrednia siÅ‚a systemu: {drm_result['average_strength']:.3f}")
    print(f"Tryb systemu: {drm_result['system_mode']}")
    
    # Podsumowanie
    monitor.checkpoint("Finalizacja testu")
    monitor.print_summary()
    
    # Ocena wynikÃ³w
    print(f"\nğŸ† OCENA WYNIKÃ“W:")
    
    if system.training_history:
        initial_loss = system.training_history[0]['loss']
        final_loss = system.training_history[-1]['loss']
        improvement = ((initial_loss - final_loss) / initial_loss) * 100
        
        print(f"Poprawa loss: {improvement:.1f}%")
        
        if improvement > 50:
            print("âœ… DoskonaÅ‚y wynik!")
        elif improvement > 20:
            print("âœ… Dobry wynik!")
        elif improvement > 5:
            print("âš ï¸ Umiarkowany wynik")
        else:
            print("âŒ SÅ‚aby wynik - system wymaga dostrojenia")
        
        # Analiza stabilnoÅ›ci DRM
        mode_changes = 0
        for i in range(1, len(system.training_history)):
            if (system.training_history[i]['system_mode'] != 
                system.training_history[i-1]['system_mode']):
                mode_changes += 1
        
        print(f"Zmiany trybu DRM: {mode_changes}")
        
        if mode_changes > epochs * 0.3:
            print("âš ï¸ System czÄ™sto zmienia tryby - moÅ¼e byÄ‡ niestabilny")
        elif mode_changes < 2:
            print("âš ï¸ System rzadko zmienia tryby - moÅ¼e brakowaÄ‡ adaptacji")
        else:
            print("âœ… Optymalna czÄ™stotliwoÅ›Ä‡ zmian trybÃ³w")
    
    return system

# Funkcje diagnostyczne
def diagnose_training_issues(system):
    """Diagnozuj problemy z trenowaniem"""
    print("\nğŸ”¬ DIAGNOZA PROBLEMÃ“W TRENOWANIA")
    print("=" * 50)
    
    if not system.training_history:
        print("âŒ Brak historii trenowania do analizy")
        return
    
    issues = []
    recommendations = []
    
    # Analiza konwergencji
    losses = [h['loss'] for h in system.training_history]
    
    if len(losses) >= 10:
        recent_losses = losses[-10:]
        early_losses = losses[:10]
        
        recent_avg = np.mean(recent_losses)
        early_avg = np.mean(early_losses)
        
        if recent_avg >= early_avg * 0.95:  # Mniej niÅ¼ 5% poprawy
            issues.append("Brak znaczÄ…cej poprawy loss")
            recommendations.append("ZwiÄ™ksz learning rate lub liczbÄ™ epok")
        
        # SprawdÅº stabilnoÅ›Ä‡
        recent_std = np.std(recent_losses)
        if recent_std > recent_avg * 0.1:  # Odchylenie > 10% Å›redniej
            issues.append("Niestabilne trenowanie")
            recommendations.append("Zmniejsz learning rate")
    
    # Analiza FRZ
    frz_values = [h['FRZ'] for h in system.training_history]
    avg_frz = np.mean(frz_values)
    
    if avg_frz < 0.3:
        issues.append("Niski Å›redni FRZ")
        recommendations.append("SprawdÅº jakoÅ›Ä‡ danych lub architekturÄ™ sieci")
    elif avg_frz > 0.9:
        issues.append("Bardzo wysoki FRZ - moÅ¼liwe przeuczenie")
        recommendations.append("Dodaj regularyzacjÄ™ lub wiÄ™cej danych")
    
    # Analiza DRM
    rule_counts = [h['active_rules'] for h in system.training_history]
    
    if max(rule_counts) - min(rule_counts) > len(system.drm.rules) * 0.5:
        issues.append("DuÅ¼e wahania liczby aktywnych reguÅ‚")
        recommendations.append("Dostosuj progi adaptacji DRM")
    
    avg_strength = [h['avg_rule_strength'] for h in system.training_history]
    if np.mean(avg_strength) < 0.3:
        issues.append("Niska Å›rednia siÅ‚a reguÅ‚")
        recommendations.append("Zmniejsz prÃ³g adaptacji lub zwiÄ™ksz prÃ³g eksploracji")
    
    # WyÅ›wietl wyniki
    if issues:
        print("ğŸš¨ ZIDENTYFIKOWANE PROBLEMY:")
        for i, issue in enumerate(issues, 1):
            print(f"  {i}. {issue}")
        
        print(f"\nğŸ’¡ REKOMENDACJE:")
        for i, rec in enumerate(recommendations, 1):
            print(f"  {i}. {rec}")
    else:
        print("âœ… Nie wykryto problemÃ³w z trenowaniem")

# Ostatnie funkcje i zakoÅ„czenie
def create_quick_demo():
    """Szybka demonstracja moÅ¼liwoÅ›ci systemu"""
    print("âš¡ SZYBKA DEMONSTRACJA")
    print("=" * 30)
    
    # MaÅ‚y system do demonstracji
    system = AdvancedLoopDRMSystem(input_size=4, hidden_size=8, output_size=2)
    
    # Dodaj kilka prÃ³bek
    demo_data = [
        ([1, 2, 3, 4], [2.5, 1.2]),
        ([2, 4, 6, 8], [5.0, 2.4]),
        ([-1, -2, -3, -4], [-2.5, 1.2]),
        ([0.5, 1.5, 2.5, 3.5], [2.0, 1.0])
    ]
    
    for inp, tgt in demo_data:
        system.training_inputs.append(inp)
        system.training_targets.append(tgt)
    
    print(f"ğŸ“Š Dodano {len(demo_data)} prÃ³bek demonstracyjnych")
    
    # KrÃ³tkie trenowanie
    inputs_tensor = torch.FloatTensor(system.training_inputs)
    targets_tensor = torch.FloatTensor(system.training_targets)
    
    print("ğŸ¯ Trenowanie (10 epok)...")
    
    for epoch in range(10):
        outputs = system.model(inputs_tensor)
        loss = system.criterion(outputs, targets_tensor)
        
        system.optimizer.zero_grad()
        loss.backward()
        system.optimizer.step()
        
        # Zastosuj DRM
        FRZ = system.rls.calculate_FRZ(loss.item())
        for context in system.training_inputs:
            system.drm.apply_rules(context, FRZ)
        
        if epoch % 3 == 0:
            print(f"  Epoka {epoch+1}: Loss={loss.item():.4f}, "
                  f"FRZ={FRZ:.3f}, ReguÅ‚y={len(system.drm.rules)}")
    
    # Test
    test_input = [1.5, 2.5, 3.5, 4.5]
    with torch.no_grad():
        prediction = system.model(torch.FloatTensor([test_input]))[0].tolist()
    
    print(f"\nğŸ§ª Test: {test_input} â†’ {[f'{x:.3f}' for x in prediction]}")
    print("âœ… Demonstracja zakoÅ„czona!")

# Informacje o systemie
def print_system_info():
    """WyÅ›wietl informacje o systemie"""
    print("\n" + "="*60)
    print("    ğŸš€ ZAAWANSOWANY SYSTEM LOOPDRM")
    print("="*60)
    print("ğŸ“‹ OPIS SYSTEMU:")
    print("  â€¢ Implementacja Dynamicznej Matrycy ReguÅ‚ (DRM)")
    print("  â€¢ Rozszerzony System Uczenia ReguÅ‚ (RLS) z FRZ")
    print("  â€¢ Adaptacyjne sieci neuronowe z PyTorch")
    print("  â€¢ Automatyczna adaptacja trybu eksploracja/eksploatacja")
    print("  â€¢ Zaawansowana analiza i wizualizacja")
    
    print(f"\nğŸ”¬ WZORY MATEMATYCZNE:")
    print("  â€¢ SiÅ‚a reguÅ‚y: Si = Wi Â· log(Ci + 1) Â· (1 + Ui/T) Â· Ri")
    print("  â€¢ Åšrednia siÅ‚a systemu: SÌ„ = (1/n) Î£ Si")
    print("  â€¢ FRZ: Function Resonance Zone = 1 - normalized_loss + accuracy_bonus")
    print("  â€¢ RÃ³Å¼norodnoÅ›Ä‡ kontekstu: ||context - avg_context||")
    
    print(f"\nğŸ›ï¸ PARAMETRY SYSTEMU:")
    print("  â€¢ Wi - Waga skutecznoÅ›ci (0.1 - 5.0)")
    print("  â€¢ Ci - ZasiÄ™g kontekstowy (0.1 - 10.0)")
    print("  â€¢ Ui - UÅ¼ycie reguÅ‚y (liczba aktywacji)")
    print("  â€¢ Ri - Rezonans z pamiÄ™ciÄ… (0.0 - 1.0)")
    print("  â€¢ T - Czas systemowy (liczba iteracji)")
    
    print(f"\nğŸ”§ TRYBY DZIAÅANIA:")
    print("  â€¢ EKSPLORACJA - tworzenie nowych reguÅ‚, mutacje")
    print("  â€¢ EKSPLOATACJA - wykorzystanie najlepszych reguÅ‚")
    print("  â€¢ Automatyczne przeÅ‚Ä…czanie na podstawie SÌ„")
    
    print(f"\nğŸ“Š MOÅ»LIWOÅšCI:")
    print("  â€¢ Trenowanie sieci z adaptacyjnym DRM")
    print("  â€¢ Analiza wydajnoÅ›ci i diagnostyka")
    print("  â€¢ Eksport danych (JSON, CSV, TXT)")
    print("  â€¢ Zapis/wczytywanie modeli z metadanymi")
    print("  â€¢ Wizualizacja ewolucji reguÅ‚")
    print("  â€¢ Monitoring wydajnoÅ›ci w czasie rzeczywistym")
    
    print(f"\nğŸ‘¨â€ğŸ’» AUTOR: System LoopDRM v2.0")
    print(f"ğŸ“… WERSJA: Zaawansowana implementacja z matematycznymi wzorami")
    print("="*60)

# Funkcja pomocnicza do debugowania
def debug_system_state(system):
    """WyÅ›wietl szczegÃ³Å‚owy stan systemu do debugowania"""
    print("\nğŸ› STAN SYSTEMU (DEBUG)")
    print("="*50)
    
    # Stan sieci neuronowej
    print("ğŸ§  SIEÄ† NEURONOWA:")
    print(f"  Architektura: {system.input_size} â†’ {system.hidden_size} â†’ {system.output_size}")
    print(f"  Parametry: {sum(p.numel() for p in system.model.parameters())}")
    print(f"  Learning rate: {system.optimizer.param_groups[0]['lr']}")
    
    # Stan DRM
    print(f"\nğŸ”§ DRM:")
    print(f"  Czas systemowy T: {system.drm.T}")
    print(f"  Tryb: {system.drm.mode.value}")
    print(f"  PrÃ³g adaptacji: {system.drm.adaptation_threshold}")
    print(f"  PrÃ³g eksploracji: {system.drm.exploration_threshold}")
    print(f"  Liczba reguÅ‚: {len(system.drm.rules)}")
    print(f"  Rozmiar pamiÄ™ci: {len(system.drm.memory)}")
    
    # SzczegÃ³Å‚y reguÅ‚
    if system.drm.rules:
        print(f"\n  SzczegÃ³Å‚y reguÅ‚:")
        for rule_id, rule in system.drm.rules.items():
            meta = rule['metadata']
            strength = system.drm.calculate_rule_strength(rule_id)
            print(f"    {rule_id}:")
            print(f"      SiÅ‚a: {strength:.4f}")
            print(f"      W={meta.W:.2f}, C={meta.C:.2f}, U={meta.U:.1f}, R={meta.R:.3f}")
            print(f"      Sukcesy: {meta.success_count}/{meta.total_activations}")
    
    # Stan RLS
    print(f"\nğŸ¯ RLS:")
    print(f"  PrÃ³g: {system.rls.threshold}")
    print(f"  Rozmiar okna: {system.rls.window_size}")
    print(f"  Historia: {len(system.rls.history)} punktÃ³w")
    print(f"  Wykryte rÃ³Å¼nice: {len(system.rls.differences)}")
    print(f"  Historia FRZ: {len(system.rls.FRZ_history)} punktÃ³w")
    print(f"  Aktualny FRZ: {system.rls.get_current_FRZ():.4f}")
    
    # Dane treningowe
    print(f"\nğŸ“Š DANE:")
    print(f"  PrÃ³bki treningowe: {len(system.training_inputs)}")
    print(f"  Historia trenowania: {len(system.training_history)} epok")
    
    if system.training_history:
        last_epoch = system.training_history[-1]
        print(f"  Ostatnia epoka:")
        print(f"    Loss: {last_epoch['loss']:.6f}")
        print(f"    FRZ: {last_epoch['FRZ']:.4f}")
        print(f"    Accuracy: {last_epoch.get('accuracy', 'N/A')}")
    
    # PamiÄ™Ä‡ systemowa
    try:
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        print(f"\nğŸ’¾ PAMIÄ˜Ä†:")
        print(f"  UÅ¼ycie RAM: {memory_mb:.1f} MB")
    except ImportError:
        print(f"\nğŸ’¾ PAMIÄ˜Ä†: ModuÅ‚ psutil niedostÄ™pny")

# Funkcja do testowania konkretnych scenariuszy
def run_scenario_test(scenario="basic"):
    """Uruchom test konkretnego scenariusza"""
    print(f"\nğŸ¬ TEST SCENARIUSZA: {scenario.upper()}")
    print("="*50)
    
    if scenario == "basic":
        # Podstawowy test funkcjonalnoÅ›ci
        system = AdvancedLoopDRMSystem(4, 8, 2)
        inputs, targets = generate_test_data(4, 2, 20, "linear")
        system.training_inputs = inputs
        system.training_targets = targets
        
        # KrÃ³tkie trenowanie
        system._train_epochs(10)
        print("âœ… Podstawowy scenariusz zakoÅ„czony")
        
    elif scenario == "stress":
        # Test obciÄ…Å¼eniowy
        print("âš¡ Test obciÄ…Å¼eniowy - duÅ¼e dane")
        system = AdvancedLoopDRMSystem(20, 40, 10)
        inputs, targets = generate_test_data(20, 10, 500, "mixed")
        system.training_inputs = inputs
        system.training_targets = targets
        
        import time
        start_time = time.time()
        system._train_epochs(50)
        end_time = time.time()
        
        print(f"â±ï¸ Czas trenowania: {end_time - start_time:.2f}s")
        print("âœ… Test obciÄ…Å¼eniowy zakoÅ„czony")
        
    elif scenario == "adaptation":
        # Test adaptacji DRM
        print("ğŸ”„ Test adaptacji DRM")
        system = AdvancedLoopDRMSystem(6, 12, 3)
        
        # RÃ³Å¼ne fazy danych
        phase1_inputs, phase1_targets = generate_test_data(6, 3, 30, "linear")
        phase2_inputs, phase2_targets = generate_test_data(6, 3, 30, "quadratic")
        
        # Faza 1
        system.training_inputs = phase1_inputs
        system.training_targets = phase1_targets
        system._train_epochs(20)
        
        print("ğŸ“Š Zmiana wzorca danych...")
        
        # Faza 2
        system.training_inputs.extend(phase2_inputs)
        system.training_targets.extend(phase2_targets)
        system._train_epochs(20)
        
        print("âœ… Test adaptacji zakoÅ„czony")
        
    else:
        print(f"âŒ Nieznany scenariusz: {scenario}")

# Dodatkowa metoda do klasy AdvancedLoopDRMSystem
def _train_epochs(self, epochs):
    """Pomocnicza metoda do trenowania okreÅ›lonej liczby epok"""
    if not self.training_inputs or not self.training_targets:
        print("âŒ Brak danych treningowych")
        return
    
    inputs_tensor = torch.FloatTensor(self.training_inputs)
    targets_tensor = torch.FloatTensor(self.training_targets)
    
    for epoch in range(epochs):
        outputs = self.model(inputs_tensor)
        loss = self.criterion(outputs, targets_tensor)
        
        accuracy = self.calculate_prediction_accuracy(outputs, targets_tensor)
        FRZ = self.rls.calculate_FRZ(loss.item(), accuracy)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # Zastosuj DRM dla prÃ³bki kontekstÃ³w
        sample_size = min(10, len(self.training_inputs))
        for i in range(0, len(self.training_inputs), len(self.training_inputs)//sample_size):
            self.drm.apply_rules(self.training_inputs[i], FRZ)
        
        difference_detected = self.rls.detect_difference(loss.item())
        
        epoch_metrics = {
            'epoch': epoch + 1,
            'loss': loss.item(),
            'accuracy': accuracy,
            'FRZ': FRZ,
            'system_mode': self.drm.mode.value,
            'avg_rule_strength': self.drm.calculate_system_average_strength(),
            'active_rules': len(self.drm.rules),
            'difference_detected': difference_detected
        }
        self.training_history.append(epoch_metrics)

# Dodaj metodÄ™ do klasy AdvancedLoopDRMSystem
AdvancedLoopDRMSystem._train_epochs = _train_epochs

# Finalne menu rozszerzone
def show_extended_menu():
    """PokaÅ¼ rozszerzone menu opcji"""
    print("\n" + "="*60)
    print("           ğŸš€ ZAAWANSOWANY SYSTEM LOOPDRM")
    print("="*60)
    print("ğŸ“‹ MENU GÅÃ“WNE:")
    print("  1. ğŸ¯ Dodaj dane treningowe")
    print("  2. ğŸ§  Trenuj sieÄ‡ neuronowÄ…")
    print("  3. ğŸ§ª Testuj model")
    print("  4. ğŸ’¾ Zapisz model")
    print("  5. ğŸ“‚ Wczytaj model")
    print("  6. âš™ï¸  Konfiguracja systemu")
    print("  7. ğŸ“Š Analiza wydajnoÅ›ci")
    print("  8. ğŸ“¤ Eksport danych")
    print()
    print("ğŸ”§ NARZÄ˜DZIA ZAAWANSOWANE:")
    print("  9. ğŸ” SzczegÃ³Å‚owe statystyki DRM")
    print(" 10. ğŸ¬ Testy scenariuszy")
    print(" 11. âš¡ Szybka demonstracja")
    print(" 12. ğŸ¤– Test automatyczny")
    print(" 13. ğŸ”¬ Diagnoza problemÃ³w")
    print(" 14. ğŸ› Stan systemu (debug)")
    print(" 15. â„¹ï¸  Informacje o systemie")
    print()
    print("  0. ğŸšª WyjÅ›cie")
    print("="*60)

# Rozszerzona metoda run_advanced_menu
def run_extended_advanced_menu(self):
    """Uruchom rozszerzone menu zaawansowane"""
    while True:
        show_extended_menu()
        
        try:
            choice = input("\nğŸ¯ Wybierz opcjÄ™: ").strip()
            
            if choice == "1":
                self.add_training_data()
            elif choice == "2":
                self.train_network()
            elif choice == "3":
                self.test_model()
            elif choice == "4":
                self.save_model()
            elif choice == "5":
                self.load_model()
            elif choice == "6":
                self.configure_system()
            elif choice == "7":
                self.analyze_performance()
            elif choice == "8":
                self.export_data()
            elif choice == "9":
                stats = self.drm.get_detailed_statistics()
                print("\nğŸ“Š SZCZEGÃ“ÅOWE STATYSTYKI DRM:")
                print(json.dumps(stats, indent=2, ensure_ascii=False))
            elif choice == "10":
                print("\nDostÄ™pne scenariusze:")
                print("1. basic - Podstawowy test")
                print("2. stress - Test obciÄ…Å¼eniowy")
                print("3. adaptation - Test adaptacji")
                scenario_choice = input("Wybierz scenariusz: ").strip()
                scenarios = {"1": "basic", "2": "stress", "3": "adaptation"}
                run_scenario_test(scenarios.get(scenario_choice, "basic"))
            elif choice == "11":
                create_quick_demo()
            elif choice == "12":
                run_automated_test()
            elif choice == "13":
                diagnose_training_issues(self)
            elif choice == "14":
                debug_system_state(self)
            elif choice == "15":
                print_system_info()
            elif choice == "0":
                print("ğŸ‘‹ DziÄ™kujemy za korzystanie z systemu LoopDRM!")
                break
            else:
                print("âŒ NieprawidÅ‚owy wybÃ³r. SprÃ³buj ponownie.")
                
        except KeyboardInterrupt:
            print("\n\nâš ï¸ Przerwano przez uÅ¼ytkownika.")
            break
        except Exception as e:
            print(f"\nâŒ BÅ‚Ä…d: {e}")
            print("Kontynuowanie dziaÅ‚ania...")

# ZastÄ…p oryginalnÄ… metodÄ™
AdvancedLoopDRMSystem.run_advanced_menu = run_extended_advanced_menu

# KoÅ„cowe informacje
print("\n" + "ğŸ‰" * 20)
print("   SYSTEM LOOPDRM GOTOWY DO UÅ»YCIA!")
print("ğŸ‰" * 20)
print("\nğŸ“š Aby rozpoczÄ…Ä‡, uruchom:")
print("   python loopdrm.py")
print("\nğŸ”— Lub uÅ¼yj w kodzie:")
print("   from loopdrm import AdvancedLoopDRMSystem")
print("   system = AdvancedLoopDRMSystem()")
print("   system.run_advanced_menu()")
print("\nâœ¨ Powodzenia w eksperymentach z zaawansowanym systemem LoopDRM!")
print("ğŸš€ System zawiera peÅ‚nÄ… implementacjÄ™ matematycznych wzorÃ³w DRM")
print("ğŸ¯ Gotowy do badaÅ„ nad adaptacyjnymi systemami uczenia maszynowego")
print("\n" + "="*60)


