import torch
import torch.nn as nn
import torch.optim as optim
import json
import numpy as np
from typing import List, Dict, Any, Tuple, Callable
import os
import math
import random
from dataclasses import dataclass
from enum import Enum

class SystemMode(Enum):
    EXPLORATION = "exploration"
    EXPLOITATION = "exploitation"

@dataclass
class RuleMetadata:
    """Metadane regu≈Çy zgodnie ze wzorem DRM"""
    W: float  # Waga skuteczno≈õci (rezonans z dzia≈Çaniem)
    C: float  # Zasiƒôg kontekstowy 
    U: float  # U≈ºycie (aktywno≈õƒá)
    R: float  # ≈öredni rezonans z pamiƒôciƒÖ i obecnym FRZ
    creation_time: int = 0
    last_activation: int = 0
    success_count: int = 0
    total_activations: int = 0

class AdvancedDRM:
    """Zaawansowana implementacja DRM z matematycznymi wzorami"""
    
    def __init__(self, adaptation_threshold: float = 0.3, exploration_threshold: float = 0.5):
        self.rules = {}  # rule_id -> {'condition': callable, 'action': callable, 'metadata': RuleMetadata}
        self.T = 0  # Czas/iteracja
        self.adaptation_threshold = adaptation_threshold
        self.exploration_threshold = exploration_threshold
        self.mode = SystemMode.EXPLOITATION
        self.memory = []  # Pamiƒôƒá poprzednich stan√≥w
        self.current_FRZ = 0.0  # Obecny wska≈∫nik sukcesu
        self.rule_combinations = []  # Historia kombinacji regu≈Ç
        
    def add_rule(self, rule_id: str, condition: Callable, action: Callable, 
                 initial_W: float = 1.0, initial_C: float = 1.0):
        """Dodaj regu≈Çƒô z metadanymi DRM"""
        metadata = RuleMetadata(
            W=initial_W,
            C=initial_C, 
            U=0.0,
            R=0.5,  # Neutralny rezonans na start
            creation_time=self.T
        )
        
        self.rules[rule_id] = {
            'condition': condition,
            'action': action,
            'metadata': metadata
        }
        
    def calculate_rule_strength(self, rule_id: str) -> float:
        """
        Oblicz si≈Çƒô regu≈Çy wed≈Çug wzoru:
        Si = Wi ¬∑ log(Ci + 1) ¬∑ (1 + Ui/T) ¬∑ Ri
        """
        if rule_id not in self.rules:
            return 0.0
            
        meta = self.rules[rule_id]['metadata']
        
        # Zabezpieczenie przed dzieleniem przez zero
        time_factor = 1.0 if self.T == 0 else (1 + meta.U / max(self.T, 1))
        
        Si = (meta.W * 
              math.log(meta.C + 1) * 
              time_factor * 
              meta.R)
              
        return max(Si, 0.0)  # Nie mo≈ºe byƒá ujemna
    
    def update_rule_metadata(self, rule_id: str, success: bool, context_diversity: float):
        """Aktualizuj metadane regu≈Çy na podstawie wyniku"""
        if rule_id not in self.rules:
            return
            
        meta = self.rules[rule_id]['metadata']
        meta.total_activations += 1
        meta.last_activation = self.T
        meta.U += 1  # Zwiƒôksz u≈ºycie
        
        if success:
            meta.success_count += 1
            meta.W = min(meta.W * 1.1, 5.0)  # Zwiƒôksz wagƒô skuteczno≈õci (max 5.0)
        else:
            meta.W = max(meta.W * 0.9, 0.1)  # Zmniejsz wagƒô skuteczno≈õci (min 0.1)
            
        # Aktualizuj zasiƒôg kontekstowy na podstawie r√≥≈ºnorodno≈õci kontekst√≥w
        meta.C = 0.8 * meta.C + 0.2 * context_diversity
        
        # Aktualizuj rezonans na podstawie sukcesu
        success_rate = meta.success_count / max(meta.total_activations, 1)
        meta.R = 0.7 * meta.R + 0.3 * success_rate
    
    def calculate_system_average_strength(self) -> float:
        """
        Oblicz ≈õredniƒÖ skuteczno≈õƒá ca≈Çej DRM:
        SÃÑ = (1/n) Œ£ Si
        """
        if not self.rules:
            return 0.0
            
        total_strength = sum(self.calculate_rule_strength(rule_id) 
                           for rule_id in self.rules.keys())
        return total_strength / len(self.rules)
    
    def adapt_system_mode(self):
        """Adaptacja trybu systemu na podstawie ≈õredniej skuteczno≈õci"""
        avg_strength = self.calculate_system_average_strength()
        
        if avg_strength < self.exploration_threshold:
            if self.mode != SystemMode.EXPLORATION:
                print(f"üîç Prze≈ÇƒÖczanie na tryb EKSPLORACJI (SÃÑ={avg_strength:.3f})")
                self.mode = SystemMode.EXPLORATION
        else:
            if self.mode != SystemMode.EXPLOITATION:
                print(f"‚ö° Prze≈ÇƒÖczanie na tryb EKSPLOATACJI (SÃÑ={avg_strength:.3f})")
                self.mode = SystemMode.EXPLOITATION
    
    def mutate_rule(self, rule_id: str):
        """Mutacja regu≈Çy - zmiana zasiƒôgu kontekstowego"""
        if rule_id not in self.rules:
            return
            
        meta = self.rules[rule_id]['metadata']
        # Mutacja Ci - dodaj losowy szum
        mutation_factor = random.uniform(0.8, 1.2)
        meta.C = max(0.1, min(meta.C * mutation_factor, 10.0))
        print(f"üß¨ Mutacja regu≈Çy {rule_id}: nowy zasiƒôg kontekstowy C={meta.C:.3f}")
    
    def create_combined_rule(self, rule_id1: str, rule_id2: str) -> str:
        """Tworzenie nowej regu≈Çy przez kombinacjƒô dw√≥ch istniejƒÖcych"""
        if rule_id1 not in self.rules or rule_id2 not in self.rules:
            return None
            
        new_rule_id = f"combined_{rule_id1}_{rule_id2}_{self.T}"
        
        # Kombinacja metadanych
        meta1 = self.rules[rule_id1]['metadata']
        meta2 = self.rules[rule_id2]['metadata']
        
        combined_W = (meta1.W + meta2.W) / 2
        combined_C = max(meta1.C, meta2.C)  # Wiƒôkszy zasiƒôg
        combined_R = (meta1.R + meta2.R) / 2
        
        # Kombinacja warunk√≥w (AND)
        def combined_condition(context):
            return (self.rules[rule_id1]['condition'](context) and 
                   self.rules[rule_id2]['condition'](context))
        
        # Kombinacja akcji
        def combined_action(context):
            self.rules[rule_id1]['action'](context)
            self.rules[rule_id2]['action'](context)
            print(f"üîó Wykonano kombinowanƒÖ akcjƒô: {rule_id1} + {rule_id2}")
        
        self.add_rule(new_rule_id, combined_condition, combined_action, 
                     combined_W, combined_C)
        
        print(f"‚ú® Utworzono nowƒÖ regu≈Çƒô kombinowanƒÖ: {new_rule_id}")
        return new_rule_id
    
    def apply_rules(self, context: List[float], FRZ: float) -> Dict[str, Any]:
        """Zastosuj regu≈Çy z pe≈ÇnƒÖ logikƒÖ DRM"""
        self.T += 1
        self.current_FRZ = FRZ
        self.memory.append({'context': context.copy(), 'FRZ': FRZ, 'time': self.T})
        
        # Ogranicz pamiƒôƒá do ostatnich 100 stan√≥w
        if len(self.memory) > 100:
            self.memory.pop(0)
        
        applied_rules = []
        rule_strengths = {}
        
        # Oblicz si≈Çy wszystkich regu≈Ç
        for rule_id in self.rules.keys():
            strength = self.calculate_rule_strength(rule_id)
            rule_strengths[rule_id] = strength
        
        # Sortuj regu≈Çy wed≈Çug si≈Çy
        sorted_rules = sorted(rule_strengths.items(), key=lambda x: x[1], reverse=True)
        
        # Zastosuj regu≈Çy w kolejno≈õci si≈Çy
        for rule_id, strength in sorted_rules:
            if strength < self.adaptation_threshold:
                continue
                
            rule = self.rules[rule_id]
            try:
                if rule['condition'](context):
                    rule['action'](context)
                    applied_rules.append(rule_id)
                    
                    # Oblicz r√≥≈ºnorodno≈õƒá kontekstu
                    context_diversity = self._calculate_context_diversity(context)
                    
                    # Okre≈õl sukces na podstawie FRZ
                    success = FRZ > 0.5  # Pr√≥g sukcesu
                    
                    # Aktualizuj metadane
                    self.update_rule_metadata(rule_id, success, context_diversity)
                    
            except Exception as e:
                print(f"‚ùå B≈ÇƒÖd w regule {rule_id}: {e}")
        
        # Adaptacja systemu
        self._perform_adaptation()
        
        # Adaptuj tryb systemu
        self.adapt_system_mode()
        
        return {
            'applied_rules': applied_rules,
            'rule_strengths': rule_strengths,
            'system_mode': self.mode.value,
            'average_strength': self.calculate_system_average_strength(),
            'total_rules': len(self.rules)
        }
    
    def _calculate_context_diversity(self, context: List[float]) -> float:
        """Oblicz r√≥≈ºnorodno≈õƒá kontekstu w por√≥wnaniu z pamiƒôciƒÖ"""
        if len(self.memory) < 2:
            return 1.0
            
        # Por√≥wnaj z ostatnimi 10 kontekstami
        recent_contexts = [mem['context'] for mem in self.memory[-10:]]
        
        diversities = []
        for past_context in recent_contexts:
            if len(past_context) == len(context):
                # Oblicz odleg≈Ço≈õƒá euklidesowƒÖ
                distance = np.linalg.norm(np.array(context) - np.array(past_context))
                diversities.append(distance)
        
        return np.mean(diversities) if diversities else 1.0
    
    def _perform_adaptation(self):
        """Wykonaj adaptacjƒô regu≈Ç zgodnie z mechanizmem DRM"""
        rules_to_remove = []
        rules_to_mutate = []
        
        for rule_id in self.rules.keys():
            strength = self.calculate_rule_strength(rule_id)
            
            # Regu≈Çy poni≈ºej progu
            if strength < self.adaptation_threshold:
                if random.random() < 0.3:  # 30% szans na mutacjƒô
                    rules_to_mutate.append(rule_id)
                elif strength < self.adaptation_threshold * 0.5:  # Bardzo s≈Çabe regu≈Çy
                    rules_to_remove.append(rule_id)
        
        # Usu≈Ñ s≈Çabe regu≈Çy
        for rule_id in rules_to_remove:
            del self.rules[rule_id]
            print(f"üóëÔ∏è Usuniƒôto s≈ÇabƒÖ regu≈Çƒô: {rule_id}")
        
        # Mutuj regu≈Çy
        for rule_id in rules_to_mutate:
            self.mutate_rule(rule_id)
        
        # W trybie eksploracji - tw√≥rz nowe regu≈Çy
        if self.mode == SystemMode.EXPLORATION and len(self.rules) >= 2:
            if random.random() < 0.2:  # 20% szans na kombinacjƒô
                rule_ids = list(self.rules.keys())
                if len(rule_ids) >= 2:
                    rule1, rule2 = random.sample(rule_ids, 2)
                    self.create_combined_rule(rule1, rule2)
    
    def get_detailed_statistics(self) -> Dict[str, Any]:
        """Pobierz szczeg√≥≈Çowe statystyki DRM"""
        rule_stats = {}
        
        for rule_id, rule in self.rules.items():
            meta = rule['metadata']
            strength = self.calculate_rule_strength(rule_id)
            
            rule_stats[rule_id] = {
                'strength': strength,
                'effectiveness_weight': meta.W,
                'context_range': meta.C,
                'usage': meta.U,
                'resonance': meta.R,
                'success_rate': meta.success_count / max(meta.total_activations, 1),
                'total_activations': meta.total_activations,
                'age': self.T - meta.creation_time
            }
        
        return {
            'rules': rule_stats,
            'system_time': self.T,
            'system_mode': self.mode.value,
            'average_strength': self.calculate_system_average_strength(),
            'total_rules': len(self.rules),
            'memory_size': len(self.memory),
            'adaptation_threshold': self.adaptation_threshold,
            'exploration_threshold': self.exploration_threshold
        }

class EnhancedRLS:
    """Rozszerzony RLS z obliczaniem FRZ"""
    
    def __init__(self, threshold: float = 0.1, window_size: int = 10):
        self.threshold = threshold
        self.window_size = window_size
        self.history = []
        self.differences = []
        self.FRZ_history = []  # Historia wska≈∫nik√≥w sukcesu
        
    def calculate_FRZ(self, loss: float, prediction_accuracy: float = None) -> float:
        """
        Oblicz wska≈∫nik FRZ (Function Resonance Zone)
        FRZ = 1 - normalized_loss + accuracy_bonus
        """
        # Normalizuj loss do zakresu 0-1
        if len(self.history) > 0:
            max_loss = max(self.history)
            min_loss = min(self.history)
            if max_loss > min_loss:
                normalized_loss = (loss - min_loss) / (max_loss - min_loss)
            else:
                normalized_loss = 0.0
        else:
            normalized_loss = 1.0
        # Oblicz FRZ
        base_FRZ = 1.0 - normalized_loss
        
        # Dodaj bonus za dok≈Çadno≈õƒá predykcji je≈õli dostƒôpny
        accuracy_bonus = 0.0
        if prediction_accuracy is not None:
            accuracy_bonus = prediction_accuracy * 0.3  # 30% wp≈Çyw dok≈Çadno≈õci
        
        FRZ = max(0.0, min(1.0, base_FRZ + accuracy_bonus))
        self.FRZ_history.append(FRZ)
        
        # Ogranicz historiƒô FRZ
        if len(self.FRZ_history) > self.window_size * 2:
            self.FRZ_history.pop(0)
            
        return FRZ
    
    def detect_difference(self, value: float) -> bool:
        """Wykryj r√≥≈ºnice z obliczaniem FRZ"""
        self.history.append(value)
        
        if len(self.history) > self.window_size:
            self.history.pop(0)
            
        if len(self.history) < 2:
            return False
            
        recent_avg = np.mean(self.history[:-1])
        current_diff = abs(value - recent_avg)
        
        if current_diff > self.threshold:
            self.differences.append({
                'value': value,
                'average': recent_avg,
                'difference': current_diff,
                'timestamp': len(self.history)
            })
            return True
            
        return False
    
    def get_current_FRZ(self) -> float:
        """Pobierz aktualny FRZ"""
        return self.FRZ_history[-1] if self.FRZ_history else 0.5
    
    def get_average_FRZ(self) -> float:
        """Pobierz ≈õredni FRZ z ostatniego okna"""
        if not self.FRZ_history:
            return 0.5
        window_FRZ = self.FRZ_history[-self.window_size:]
        return np.mean(window_FRZ)
    
    def get_statistics(self) -> Dict[str, Any]:
        """Pobierz statystyki RLS z FRZ"""
        return {
            'history_length': len(self.history),
            'differences_detected': len(self.differences),
            'current_average': np.mean(self.history) if self.history else 0,
            'threshold': self.threshold,
            'current_FRZ': self.get_current_FRZ(),
            'average_FRZ': self.get_average_FRZ(),
            'FRZ_trend': self._calculate_FRZ_trend()
        }
    
    def _calculate_FRZ_trend(self) -> str:
        """Oblicz trend FRZ (rosnƒÖcy/malejƒÖcy/stabilny)"""
        if len(self.FRZ_history) < 3:
            return "insufficient_data"
        
        recent_FRZ = self.FRZ_history[-3:]
        if recent_FRZ[-1] > recent_FRZ[0] + 0.05:
            return "improving"
        elif recent_FRZ[-1] < recent_FRZ[0] - 0.05:
            return "declining"
        else:
            return "stable"

class SimpleNeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNeuralNetwork, self).__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.layer2 = nn.Linear(hidden_size, output_size)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = self.activation(self.layer1(x))
        x = self.dropout(x)
        x = self.layer2(x)
        return x

class AdvancedLoopDRMSystem:
    """Zaawansowany system LoopDRM z matematycznymi wzorami"""
    
    def __init__(self, input_size=10, hidden_size=20, output_size=3):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # Inicjalizuj sieƒá neuronowƒÖ
        self.model = SimpleNeuralNetwork(input_size, hidden_size, output_size)
        self.criterion = nn.MSELoss()
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
        
        # Inicjalizuj zaawansowane DRM i RLS
        self.drm = AdvancedDRM(adaptation_threshold=0.3, exploration_threshold=0.5)
        self.rls = EnhancedRLS(threshold=0.1)
        
        # Przechowywanie danych treningowych
        self.training_inputs = []
        self.training_targets = []
        
        # Metryki systemu
        self.training_history = []
        self.performance_metrics = []
        
        # Inicjalizuj zaawansowane regu≈Çy DRM
        self._initialize_advanced_rules()
        
    def _initialize_advanced_rules(self):
        """Inicjalizuj zaawansowane regu≈Çy DRM z matematycznymi wzorami"""
        
        # Regu≈Ça 1: Detekcja wysokiej wariancji z adaptacyjnym progiem
        def adaptive_variance_condition(context):
            variance = np.var(context)
            # Adaptacyjny pr√≥g na podstawie historii
            if len(self.rls.history) > 5:
                hist_variance = np.var(self.rls.history[-5:])
                threshold = max(0.5, hist_variance * 1.5)
            else:
                threshold = 1.0
            return variance > threshold
            
        def variance_action(context):
            variance = np.var(context)
            print(f"üìä Adaptacyjna detekcja wariancji: {variance:.4f}")
            # Dostosuj learning rate na podstawie wariancji
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = max(0.0001, min(0.01, 0.001 / (1 + variance)))
            
        self.drm.add_rule("adaptive_variance", adaptive_variance_condition, 
                         variance_action, initial_W=2.0, initial_C=1.5)
        
        # Regu≈Ça 2: Detekcja trendu z rezonansem
        def trend_resonance_condition(context):
            if len(self.rls.FRZ_history) < 3:
                return False
            recent_FRZ = self.rls.FRZ_history[-3:]
            trend_strength = abs(recent_FRZ[-1] - recent_FRZ[0])
            return trend_strength > 0.2
            
        def trend_action(context):
            trend = self.rls._calculate_FRZ_trend()
            print(f"üìà Trend rezonansu: {trend}")
            if trend == "declining":
                # Zwiƒôksz eksploracjƒô przy spadajƒÖcym trendzie
                self.drm.exploration_threshold *= 0.9
            elif trend == "improving":
                # Wzmocnij eksploatacjƒô przy poprawie
                self.drm.exploration_threshold *= 1.1
                
        self.drm.add_rule("trend_resonance", trend_resonance_condition,
                         trend_action, initial_W=1.8, initial_C=2.0)
        
        # Regu≈Ça 3: Meta-regu≈Ça optymalizacji
        def meta_optimization_condition(context):
            avg_strength = self.drm.calculate_system_average_strength()
            return avg_strength < 0.4  # Niska ≈õrednia si≈Ça systemu
            
        def meta_optimization_action(context):
            print("üîß Meta-optymalizacja: dostrajanie systemu")
            # Zmniejsz progi adaptacji dla wiƒôkszej elastyczno≈õci
            self.drm.adaptation_threshold *= 0.95
            # Zwiƒôksz szanse na mutacjƒô
            for rule_id in self.drm.rules.keys():
                if random.random() < 0.1:  # 10% szans
                    self.drm.mutate_rule(rule_id)
                    
        self.drm.add_rule("meta_optimization", meta_optimization_condition,
                         meta_optimization_action, initial_W=2.5, initial_C=3.0)
        
        # Regu≈Ça 4: Detekcja anomalii kontekstowych
        def context_anomaly_condition(context):
            if len(self.drm.memory) < 5:
                return False
            
            # Oblicz odleg≈Ço≈õƒá od ≈õredniego kontekstu
            recent_contexts = [mem['context'] for mem in self.drm.memory[-5:]]
            avg_context = np.mean(recent_contexts, axis=0)
            distance = np.linalg.norm(np.array(context) - avg_context)
            
            return distance > 2.0  # Pr√≥g anomalii
            
        def anomaly_action(context):
            print("üö® Anomalia kontekstowa wykryta!")
            # Prze≈ÇƒÖcz na tryb eksploracji
            self.drm.mode = SystemMode.EXPLORATION
            # Zwiƒôksz learning rate dla szybkiej adaptacji
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = min(param_group['lr'] * 1.5, 0.01)
                
        self.drm.add_rule("context_anomaly", context_anomaly_condition,
                         anomaly_action, initial_W=3.0, initial_C=1.0)
    
    def calculate_prediction_accuracy(self, outputs, targets):
        """Oblicz dok≈Çadno≈õƒá predykcji dla FRZ"""
        with torch.no_grad():
            # Dla regresji - u≈ºyj odwrotno≈õci ≈õredniego b≈Çƒôdu wzglƒôdnego
            relative_errors = torch.abs((outputs - targets) / (torch.abs(targets) + 1e-8))
            mean_relative_error = torch.mean(relative_errors)
            accuracy = 1.0 / (1.0 + mean_relative_error.item())
            return accuracy
    
    def add_training_data(self):
        """Interaktywne dodawanie danych treningowych"""
        print("\n=== Wprowadzanie danych treningowych ===")
        
        try:
            print("1. Wprowad≈∫ dane rƒôcznie")
            print("2. Generuj dane losowo")
            print("3. Generuj dane z wzorcem (dla testowania DRM)")
            choice = input("Wybierz opcjƒô (1/2/3): ").strip()
            
            if choice == "1":
                print(f"Wprowad≈∫ {self.input_size} warto≈õci wej≈õciowych (oddzielone spacjami):")
                input_str = input().strip()
                inputs = [float(x) for x in input_str.split()]
                
                if len(inputs) != self.input_size:
                    print(f"B≈ÇƒÖd: Oczekiwano {self.input_size} warto≈õci, otrzymano {len(inputs)}")
                    return
                
                print(f"Wprowad≈∫ {self.output_size} warto≈õci docelowych (oddzielone spacjami):")
                target_str = input().strip()
                targets = [float(x) for x in target_str.split()]
                
                if len(targets) != self.output_size:
                    print(f"B≈ÇƒÖd: Oczekiwano {self.output_size} warto≈õci, otrzymano {len(targets)}")
                    return
                    
                self.training_inputs.append(inputs)
                self.training_targets.append(targets)
                print("Dane dodane pomy≈õlnie!")
                
            elif choice == "2":
                num_samples = int(input("Ile pr√≥bek wygenerowaƒá? "))
                for _ in range(num_samples):
                    inputs = np.random.randn(self.input_size).tolist()
                    targets = np.random.randn(self.output_size).tolist()
                    self.training_inputs.append(inputs)
                    self.training_targets.append(targets)
                print(f"Wygenerowano {num_samples} pr√≥bek treningowych")
                
            elif choice == "3":
                num_samples = int(input("Ile pr√≥bek z wzorcem wygenerowaƒá? "))
                print("Generowanie danych z wzorcami dla testowania DRM...")
                
                for i in range(num_samples):
                    # Tw√≥rz dane z r√≥≈ºnymi wzorcami
                    if i % 4 == 0:  # Wysokie wariancje
                        inputs = (np.random.randn(self.input_size) * 3).tolist()
                    elif i % 4 == 1:  # Niskie warto≈õci
                        inputs = (np.random.randn(self.input_size) - 2).tolist()
                    elif i % 4 == 2:  # Outliers
                        inputs = np.random.randn(self.input_size).tolist()
                        inputs[0] = 10.0  # Outlier
                    else:  # Normalne dane
                        inputs = np.random.randn(self.input_size).tolist()
                    
                    # Targets na podstawie wzorca
                    targets = [np.mean(inputs), np.std(inputs), np.max(inputs)][:self.output_size]
                    
                    self.training_inputs.append(inputs)
                    self.training_targets.append(targets)
                    
                print(f"Wygenerowano {num_samples} pr√≥bek z wzorcami")
            else:
                print("Nieprawid≈Çowy wyb√≥r")
                return
                
        except ValueError as e:
            print(f"B≈ÇƒÖd: Nieprawid≈Çowy format danych - {e}")
        except Exception as e:
            print(f"B≈ÇƒÖd: {e}")
    
    def train_network(self):
        """Trenowanie sieci z zaawansowanym DRM"""
        if not self.training_inputs or not self.training_targets:
            print("Brak danych treningowych! Najpierw dodaj dane.")
            return
            
        print("\n=== Zaawansowane trenowanie z DRM ===")
        
        try:
            epochs = int(input("Liczba epok (domy≈õlnie 100): ") or "100")
            
            # Konwertuj na tensory
            inputs_tensor = torch.FloatTensor(self.training_inputs)
            targets_tensor = torch.FloatTensor(self.training_targets)
            
            print(f"Rozpoczynanie treningu na {len(self.training_inputs)} pr√≥bkach...")
            print(f"Tryb poczƒÖtkowy DRM: {self.drm.mode.value}")
            
            for epoch in range(epochs):
                # Forward pass
                outputs = self.model(inputs_tensor)
                loss = self.criterion(outputs, targets_tensor)
                
                # Oblicz dok≈Çadno≈õƒá predykcji
                accuracy = self.calculate_prediction_accuracy(outputs, targets_tensor)
                
                # Oblicz FRZ
                FRZ = self.rls.calculate_FRZ(loss.item(), accuracy)
                
                # Backward pass and optimization
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                
                # Zastosuj zaawansowane regu≈Çy DRM dla ka≈ºdego kontekstu
                drm_results = []
                for i, input_data in enumerate(self.training_inputs):
                    result = self.drm.apply_rules(input_data, FRZ)
                    drm_results.append(result)
                
                # Wykryj r√≥≈ºnice u≈ºywajƒÖc RLS
                difference_detected = self.rls.detect_difference(loss.item())
                
                # Zapisz metryki
                epoch_metrics = {
                    'epoch': epoch + 1,
                    'loss': loss.item(),
                    'accuracy': accuracy,
                    'FRZ': FRZ,
                    'system_mode': self.drm.mode.value,
                    'avg_rule_strength': self.drm.calculate_system_average_strength(),
                    'active_rules': len(self.drm.rules),
                    'difference_detected': difference_detected
                }
                self.training_history.append(epoch_metrics)
                
                # Wy≈õwietl postƒôp
                if difference_detected or epoch % (epochs // 10) == 0 or epoch == epochs - 1:
                    mode_icon = "üîç" if self.drm.mode == SystemMode.EXPLORATION else "‚ö°"
                    diff_icon = "üî•" if difference_detected else ""
                    
                    print(f'Epoka {epoch+1:3d}/{epochs} | '
                          f'Loss: {loss.item():.6f} | '
                          f'FRZ: {FRZ:.3f} | '
                          f'Acc: {accuracy:.3f} | '
                          f'{mode_icon} {self.drm.mode.value} | '
                          f'Regu≈Çy: {len(self.drm.rules)} | '
                          f'SÃÑ: {self.drm.calculate_system_average_strength():.3f} {diff_icon}')
                
                # Co 20 epok - poka≈º szczeg√≥≈Çy DRM
                if epoch > 0 and epoch % 20 == 0:
                    self._show_drm_status()
            
            print("\n‚úÖ Trenowanie zako≈Ñczone!")
            self._show_final_training_summary()
            
        except ValueError as e:
            print(f"B≈ÇƒÖd: Nieprawid≈Çowa liczba epok - {e}")
        except Exception as e:
            print(f"B≈ÇƒÖd podczas trenowania: {e}")
    
    def _show_drm_status(self):
        """Poka≈º aktualny status DRM"""
        print(f"\n--- Status DRM (T={self.drm.T}) ---")
        stats = self.drm.get_detailed_statistics()
        
        print(f"Tryb systemu: {stats['system_mode']} | "
              f"≈örednia si≈Ça: {stats['average_strength']:.3f} | "
              f"Regu≈Çy: {stats['total_rules']}")
        
        # Poka≈º top 3 najsilniejsze regu≈Çy
        rule_strengths = [(rule_id, data['strength']) 
                         for rule_id, data in stats['rules'].items()]
        rule_strengths.sort(key=lambda x: x[1], reverse=True)
        
        print("Top 3 najsilniejsze regu≈Çy:")
        for i, (rule_id, strength) in enumerate(rule_strengths[:3]):
            rule_data = stats['rules'][rule_id]
            print(f"  {i+1}. {rule_id}: S={strength:.3f} "
                  f"(W={rule_data['effectiveness_weight']:.2f}, "
                  f"C={rule_data['context_range']:.2f}, "
                  f"R={rule_data['resonance']:.2f})")
    
    def _show_final_training_summary(self):
        """Poka≈º podsumowanie trenowania"""
        if not self.training_history:
            return
            
        print("\n=== Podsumowanie trenowania ===")
        
        # Podstawowe statystyki
        final_metrics = self.training_history[-1]
        initial_loss = self.training_history[0]['loss']
        final_loss = final_metrics['loss']
        improvement = ((initial_loss - final_loss) / initial_loss) * 100
        
        print(f"üìä Poprawa loss: {improvement:.1f}% "
              f"({initial_loss:.6f} ‚Üí {final_loss:.6f})")
        print(f"üéØ Ko≈Ñcowy FRZ: {final_metrics['FRZ']:.3f}")
        print(f"üîß Tryb ko≈Ñcowy: {final_metrics['system_mode']}")
        print(f"üìã Aktywne regu≈Çy: {final_metrics['active_rules']}")
        
        # Statystyki RLS
        rls_stats = self.rls.get_statistics()
        print(f"üîç Wykryte r√≥≈ºnice: {rls_stats['differences_detected']}")
        print(f"üìà Trend FRZ: {rls_stats['FRZ_trend']}")
        
        # Statystyki DRM
        drm_stats = self.drm.get_detailed_statistics()
        print(f"‚ö° ≈örednia si≈Ça systemu: {drm_stats['average_strength']:.3f}")
        
        # Analiza ewolucji regu≈Ç
        mode_changes = 0
        exploration_epochs = 0
        for i in range(1, len(self.training_history)):
            if (self.training_history[i]['system_mode'] != 
                self.training_history[i-1]['system_mode']):
                mode_changes += 1
            if self.training_history[i]['system_mode'] == 'exploration':
                exploration_epochs += 1
        
        print(f"üîÑ Zmiany trybu: {mode_changes}")
        print(f"üîç Epoki eksploracji: {exploration_epochs}/{len(self.training_history)} "
              f"({exploration_epochs/len(self.training_history)*100:.1f}%)")
    
    def save_model(self):
        """Zapisz model z pe≈Çnymi danymi DRM"""
        print("\n=== Zapisywanie zaawansowanego modelu ===")
        
        try:
            filename = input("Nazwa pliku (bez rozszerzenia, domy≈õlnie 'advanced_model'): ").strip() or "advanced_model"
            
            # Przygotuj dane DRM do zapisu
            drm_data = {
                'rules_metadata': {},
                'system_time': self.drm.T,
                'mode': self.drm.mode.value,
                'adaptation_threshold': self.drm.adaptation_threshold,
                'exploration_threshold': self.drm.exploration_threshold,
                'memory': self.drm.memory,
                'rule_combinations': self.drm.rule_combinations
            }
            
            # Zapisz metadane regu≈Ç (bez funkcji callable)
            for rule_id, rule in self.drm.rules.items():
                meta = rule['metadata']
                drm_data['rules_metadata'][rule_id] = {
                    'W': meta.W,
                    'C': meta.C,
                    'U': meta.U,
                    'R': meta.R,
                    'creation_time': meta.creation_time,
                    'last_activation': meta.last_activation,
                    'success_count': meta.success_count,
                    'total_activations': meta.total_activations
                }
            
            # Kompletne dane modelu
            model_data = {
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'input_size': self.input_size,
                'hidden_size': self.hidden_size,
                'output_size': self.output_size,
                'training_inputs': self.training_inputs,
                'training_targets': self.training_targets,
                'training_history': self.training_history,
                'drm_data': drm_data,
                'rls_data': {
                    'threshold': self.rls.threshold,
                    'window_size': self.rls.window_size,
                    'history': self.rls.history,
                    'differences': self.rls.differences,
                    'FRZ_history': self.rls.FRZ_history
                }
            }
            
            torch.save(model_data, f"{filename}.pth")
            print(f"‚úÖ Zaawansowany model zapisany jako {filename}.pth")
            print(f"üìä Zapisano {len(self.training_history)} epok historii trenowania")
            print(f"üîß Zapisano {len(drm_data['rules_metadata'])} regu≈Ç DRM")
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd podczas zapisywania: {e}")
    
    def load_model(self):
        """Wczytaj model z danymi DRM"""
        print("\n=== Wczytywanie zaawansowanego modelu ===")
        
        try:
            filename = input("Nazwa pliku (bez rozszerzenia): ").strip()
            
            if not os.path.exists(f"{filename}.pth"):
                print(f"‚ùå Plik {filename}.pth nie istnieje!")
                return
            
            model_data = torch.load(f"{filename}.pth")
            
            # Odtw√≥rz architekturƒô sieci
            self.input_size = model_data['input_size']
            self.hidden_size = model_data['hidden_size']
            self.output_size = model_data['output_size']
            
            self.model = SimpleNeuralNetwork(self.input_size, self.hidden_size, self.output_size)
            self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
            
            # Wczytaj stany
            self.model.load_state_dict(model_data['model_state_dict'])
            self.optimizer.load_state_dict(model_data['optimizer_state_dict'])
            
            # Wczytaj dane treningowe
            self.training_inputs = model_data.get('training_inputs', [])
            self.training_targets = model_data.get('training_targets', [])
            self.training_history = model_data.get('training_history', [])
            
            # Odtw√≥rz DRM
            if 'drm_data' in model_data:
                drm_data = model_data['drm_data']
                self.drm.T = drm_data['system_time']
                self.drm.mode = SystemMode(drm_data['mode'])
                self.drm.adaptation_threshold = drm_data['adaptation_threshold']
                self.drm.exploration_threshold = drm_data['exploration_threshold']
                self.drm.memory = drm_data['memory']
                self.drm.rule_combinations = drm_data.get('rule_combinations', [])
                
                # Odtw√≥rz metadane regu≈Ç (regu≈Çy bƒôdƒÖ ponownie zainicjalizowane)
                self._initialize_advanced_rules()  # Odtw√≥rz funkcje regu≈Ç
                
                # Przywr√≥ƒá metadane
                for rule_id, meta_data in drm_data['rules_metadata'].items():
                    if rule_id in self.drm.rules:
                        meta = self.drm.rules[rule_id]['metadata']
                        meta.W = meta_data['W']
                        meta.C = meta_data['C']
                        meta.U = meta_data['U']
                        meta.R = meta_data['R']
                        meta.creation_time = meta_data['creation_time']
                        meta.last_activation = meta_data['last_activation']
                        meta.success_count = meta_data['success_count']
                        meta.total_activations = meta_data['total_activations']
            
            # Odtw√≥rz RLS
            if 'rls_data' in model_data:
                rls_data = model_data['rls_data']
                self.rls.threshold = rls_data['threshold']
                self.rls.window_size = rls_data['window_size']
                self.rls.history = rls_data['history']
                self.rls.differences = rls_data['differences']
                self.rls.FRZ_history = rls_data['FRZ_history']
            
            print(f"‚úÖ Model wczytany z {filename}.pth")
            print(f"üèóÔ∏è Architektura: {self.input_size} ‚Üí {self.hidden_size} ‚Üí {self.output_size}")
            print(f"üìä Dane treningowe: {len(self.training_inputs)} pr√≥bek")
            print(f"üìà Historia: {len(self.training_history)} epok")
            print(f"üîß DRM: {len(self.drm.rules)} regu≈Ç, T={self.drm.T}, tryb={self.drm.mode.value}")
            print(f"üéØ RLS: FRZ={self.rls.get_current_FRZ():.3f}")
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd podczas wczytywania: {e}")
    
    def test_model(self):
        """Testuj model z analizƒÖ DRM"""
        print("\n=== Testowanie modelu z analizƒÖ DRM ===")
        
        try:
            print(f"Wprowad≈∫ {self.input_size} warto≈õci testowych (oddzielone spacjami):")
            input_str = input().strip()
            test_input = [float(x) for x in input_str.split()]
            
            if len(test_input) != self.input_size:
                print(f"‚ùå B≈ÇƒÖd: Oczekiwano {self.input_size} warto≈õci, otrzymano {len(test_input)}")
                return
            
            # Predykcja
            with torch.no_grad():
                input_tensor = torch.FloatTensor([test_input])
                output = self.model(input_tensor)
                prediction = output[0].tolist()
            
            print(f"üéØ Predykcja: {[f'{x:.4f}' for x in prediction]}")
            
            # Analiza DRM dla danych testowych
            print("\n--- Analiza DRM ---")
            current_FRZ = self.rls.get_current_FRZ()
            drm_result = self.drm.apply_rules(test_input, current_FRZ)
            
            print(f"üéØ Aktualny FRZ: {current_FRZ:.3f}")
            print(f"üîß Tryb systemu: {drm_result['system_mode']}")
            print(f"‚ö° ≈örednia si≈Ça systemu: {drm_result['average_strength']:.3f}")
            print(f"üìã Zastosowane regu≈Çy: {len(drm_result['applied_rules'])}")
            
            if drm_result['applied_rules']:
                print("Aktywne regu≈Çy:")
                for rule_id in drm_result['applied_rules']:
                    strength = drm_result['rule_strengths'][rule_id]
                    print(f"  ‚Ä¢ {rule_id}: si≈Ça = {strength:.3f}")
            else:
                print("  Brak aktywnych regu≈Ç dla tego kontekstu")
            
            # Analiza kontekstu
            print(f"\n--- Analiza kontekstu ---")
            context_stats = {
                'mean': np.mean(test_input),
                'std': np.std(test_input),
                'var': np.var(test_input),
                'min': np.min(test_input),
                'max': np.max(test_input),
                'range': np.max(test_input) - np.min(test_input)
            }
            
            for stat_name, value in context_stats.items():
                print(f"  {stat_name}: {value:.4f}")
            
            # Por√≥wnanie z danymi treningowymi
            if self.training_inputs:
                print(f"\n--- Por√≥wnanie z danymi treningowymi ---")
                training_means = [np.mean(inp) for inp in self.training_inputs]
                training_stds = [np.std(inp) for inp in self.training_inputs]
                
                mean_similarity = 1.0 / (1.0 + abs(context_stats['mean'] - np.mean(training_means)))
                std_similarity = 1.0 / (1.0 + abs(context_stats['std'] - np.mean(training_stds)))
                
                print(f"  Podobie≈Ñstwo ≈õredniej: {mean_similarity:.3f}")
                print(f"  Podobie≈Ñstwo odchylenia: {std_similarity:.3f}")
                
                if mean_similarity < 0.5 or std_similarity < 0.5:
                    print("  ‚ö†Ô∏è Dane testowe znacznie r√≥≈ºniƒÖ siƒô od treningowych!")
            
        except ValueError as e:
            print(f"‚ùå B≈ÇƒÖd: Nieprawid≈Çowy format danych - {e}")
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd podczas testowania: {e}")
    
    def show_advanced_statistics(self):
        """Poka≈º zaawansowane statystyki systemu"""
        print("\n" + "="*60)
        print("    ZAAWANSOWANE STATYSTYKI SYSTEMU LOOPDRM")
        print("="*60)
        
        # Statystyki sieci neuronowej
        print(f"\nüß† SIEƒÜ NEURONOWA")
        print(f"Architektura: {self.input_size} ‚Üí {self.hidden_size} ‚Üí {self.output_size}")
        
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"Parametry: {total_params} (treningowe: {trainable_params})")
        
        if self.training_history:
            print(f"Epoki trenowania: {len(self.training_history)}")
            final_loss = self.training_history[-1]['loss']
            initial_loss = self.training_history[0]['loss']
            improvement = ((initial_loss - final_loss) / initial_loss) * 100
            print(f"Poprawa loss: {improvement:.2f}%")
        
        # Zaawansowane statystyki DRM
        print(f"\nüîß DYNAMICZNA MATRYCA REGU≈Å (DRM)")
        drm_stats = self.drm.get_detailed_statistics()
        
        print(f"Czas systemowy (T): {drm_stats['system_time']}")
        print(f"Tryb systemu: {drm_stats['system_mode']}")
        print(f"≈örednia si≈Ça systemu (SÃÑ): {drm_stats['average_strength']:.4f}")
        print(f"Pr√≥g adaptacji: {drm_stats['adaptation_threshold']:.3f}")
        print(f"Pr√≥g eksploracji: {drm_stats['exploration_threshold']:.3f}")
        print(f"Rozmiar pamiƒôci: {drm_stats['memory_size']}")
        
        print(f"\nüìä SZCZEG√ì≈ÅY REGU≈Å ({drm_stats['total_rules']} aktywnych):")
        print("-" * 80)
        print(f"{'ID Regu≈Çy':<20} {'Si≈Ça(Si)':<10} {'W':<8} {'C':<8} {'U':<8} {'R':<8} {'Sukces%':<10} {'Wiek':<6}")
        print("-" * 80)
        
        # Sortuj regu≈Çy wed≈Çug si≈Çy
        rules_by_strength = sorted(drm_stats['rules'].items(), 
                                 key=lambda x: x[1]['strength'], reverse=True)
        
        for rule_id, rule_data in rules_by_strength:
            print(f"{rule_id:<20} "
                  f"{rule_data['strength']:<10.3f} "
                  f"{rule_data['effectiveness_weight']:<8.2f} "
                  f"{rule_data['context_range']:<8.2f} "
                  f"{rule_data['usage']:<8.1f} "
                  f"{rule_data['resonance']:<8.3f} "
                  f"{rule_data['success_rate']*100:<10.1f} "
                  f"{rule_data['age']:<6}")
        
        # Statystyki RLS
        print(f"\nüéØ SYSTEM UCZENIA REGU≈Å (RLS)")
        rls_stats = self.rls.get_statistics()
        
        print(f"Pr√≥g detekcji: {rls_stats['threshold']}")
        print(f"Rozmiar okna: {self.rls.window_size}")
        print(f"Historia: {rls_stats['history_length']} punkt√≥w")
        print(f"Wykryte r√≥≈ºnice: {rls_stats['differences_detected']}")
        print(f"Aktualny FRZ: {rls_stats['current_FRZ']:.4f}")
        print(f"≈öredni FRZ: {rls_stats['average_FRZ']:.4f}")
        print(f"Trend FRZ: {rls_stats['FRZ_trend']}")
        
        # Analiza historii trenowania
        if self.training_history:
            print(f"\nüìà ANALIZA HISTORII TRENOWANIA")
            
            # Statystyki tryb√≥w
            exploration_count = sum(1 for h in self.training_history 
                                  if h['system_mode'] == 'exploration')
            exploitation_count = len(self.training_history) - exploration_count
            
            print(f"Epoki eksploracji: {exploration_count} ({exploration_count/len(self.training_history)*100:.1f}%)")
            print(f"Epoki eksploatacji: {exploitation_count} ({exploitation_count/len(self.training_history)*100:.1f}%)")
            
            # Zmiany trybu
            mode_changes = 0
            for i in range(1, len(self.training_history)):
                if (self.training_history[i]['system_mode'] != 
                    self.training_history[i-1]['system_mode']):
                    mode_changes += 1
            
            print(f"Zmiany trybu: {mode_changes}")
            
            # Statystyki FRZ
            frz_values = [h['FRZ'] for h in self.training_history]
            print(f"FRZ - min: {min(frz_values):.3f}, max: {max(frz_values):.3f}, "
                  f"≈õredni: {np.mean(frz_values):.3f}")
            
            # Ewolucja liczby regu≈Ç
            rule_counts = [h['active_rules'] for h in self.training_history]
            print(f"Regu≈Çy - min: {min(rule_counts)}, max: {max(rule_counts)}, "
                  f"ko≈Ñcowa: {rule_counts[-1]}")
            
            # Wykryte r√≥≈ºnice
            differences = sum(1 for h in self.training_history if h['difference_detected'])
            print(f"Wykryte r√≥≈ºnice: {differences} ({differences/len(self.training_history)*100:.1f}%)")
        
        # Analiza wzorc√≥w matematycznych
        print(f"\nüî¨ ANALIZA WZORC√ìW MATEMATYCZNYCH")
        
        if drm_stats['rules']:
            # Korelacje miƒôdzy parametrami regu≈Ç
            W_values = [data['effectiveness_weight'] for data in drm_stats['rules'].values()]
            C_values = [data['context_range'] for data in drm_stats['rules'].values()]
            R_values = [data['resonance'] for data in drm_stats['rules'].values()]
            S_values = [data['strength'] for data in drm_stats['rules'].values()]
            
            print(f"Rozk≈Çad wag skuteczno≈õci (W): min={min(W_values):.2f}, "
                  f"max={max(W_values):.2f}, ≈õrednia={np.mean(W_values):.2f}")
            print(f"Rozk≈Çad zasiƒôgu kontekstowego (C): min={min(C_values):.2f}, "
                  f"max={max(C_values):.2f}, ≈õrednia={np.mean(C_values):.2f}")
            print(f"Rozk≈Çad rezonansu (R): min={min(R_values):.3f}, "
                  f"max={max(R_values):.3f}, ≈õrednia={np.mean(R_values):.3f}")
            print(f"Rozk≈Çad si≈Çy (S): min={min(S_values):.3f}, "
                  f"max={max(S_values):.3f}, ≈õrednia={np.mean(S_values):.3f}")
            
            # Sprawd≈∫ wz√≥r Si = Wi ¬∑ log(Ci + 1) ¬∑ (1 + Ui/T) ¬∑ Ri
            print(f"\nüßÆ WERYFIKACJA WZORU MATEMATYCZNEGO:")
            print("Si = Wi ¬∑ log(Ci + 1) ¬∑ (1 + Ui/T) ¬∑ Ri")
            
            for rule_id, rule_data in list(drm_stats['rules'].items())[:3]:  # Poka≈º 3 przyk≈Çady
                W = rule_data['effectiveness_weight']
                C = rule_data['context_range']
                U = rule_data['usage']
                R = rule_data['resonance']
                T = max(drm_stats['system_time'], 1)
                
                calculated_S = W * math.log(C + 1) * (1 + U/T) * R
                actual_S = rule_data['strength']
                
                print(f"  {rule_id}: obliczone={calculated_S:.4f}, "
                      f"rzeczywiste={actual_S:.4f}, r√≥≈ºnica={abs(calculated_S-actual_S):.6f}")
        
        print("\n" + "="*60)
    
    def run_advanced_menu(self):
        """Zaawansowane menu g≈Ç√≥wne"""
        while True:
            print("\n" + "="*60)
            print("    üöÄ ZAAWANSOWANY LOOPDRM - SYSTEM DRM Z ZAMKNIƒòTƒÑ PƒòTLƒÑ")
            print("="*60)
            print("1. üìù Wprowadzenie danych treningowych")
            print("2. üéØ Trenowanie sieci z DRM")
            print("3. üíæ Zapisanie modelu")
            print("4. üìÇ Wczytanie modelu")
            print("5. üß™ Testowanie modelu")
            print("6. üìä Zaawansowane statystyki")
            print("7. üîß Konfiguracja DRM")
            print("8. üìà Analiza wydajno≈õci")
            print("9. üîç Eksport danych")
            print("0. üö™ Zako≈Ñczenie")
            print("-" * 60)
            
            try:
                choice = input("Wybierz opcjƒô (0-9): ").strip()
                
                if choice == "1":
                    self.add_training_data()
                elif choice == "2":
                    self.train_network()
                elif choice == "3":
                    self.save_model()
                elif choice == "4":
                    self.load_model()
                elif choice == "5":
                    self.test_model()
                elif choice == "6":
                    self.show_advanced_statistics()
                elif choice == "7":
                    self.configure_drm()
                elif choice == "8":
                    self.analyze_performance()
                elif choice == "9":
                    self.export_data()
                elif choice == "0":
                    print("üéâ Dziƒôkujemy za korzystanie z Zaawansowanego LoopDRM!")
                    print("üìä System wykona≈Ç {} iteracji DRM".format(self.drm.T))
                    if self.training_history:
                        print("üèÜ Najlepszy FRZ: {:.4f}".format(
                            max(h['FRZ'] for h in self.training_history)))
                    break
                else:
                    print("‚ùå Nieprawid≈Çowy wyb√≥r. Wybierz opcjƒô 0-9.")
                    
            except KeyboardInterrupt:
                print("\n\n‚ö†Ô∏è Program przerwany przez u≈ºytkownika.")
                break
            except Exception as e:
                print(f"‚ùå WystƒÖpi≈Ç nieoczekiwany b≈ÇƒÖd: {e}")
    
    def configure_drm(self):
        """Konfiguracja parametr√≥w DRM"""
        print("\n=== Konfiguracja DRM ===")
        
        try:
            print(f"Aktualne ustawienia:")
            print(f"  Pr√≥g adaptacji: {self.drm.adaptation_threshold}")
            print(f"  Pr√≥g eksploracji: {self.drm.exploration_threshold}")
            print(f"  Tryb systemu: {self.drm.mode.value}")
            
            print("\nCo chcesz zmieniƒá?")
            print("1. Pr√≥g adaptacji")
            print("2. Pr√≥g eksploracji")
            print("3. Wymuszenie trybu systemu")
            print("4. Parametry RLS")
            print("5. Reset systemu DRM")
            print("0. Powr√≥t")
            
            config_choice = input("Wybierz opcjƒô: ").strip()
            
            if config_choice == "1":
                new_threshold = float(input(f"Nowy pr√≥g adaptacji (aktualny: {self.drm.adaptation_threshold}): "))
                if 0.0 <= new_threshold <= 1.0:
                    self.drm.adaptation_threshold = new_threshold
                    print(f"‚úÖ Pr√≥g adaptacji zmieniony na {new_threshold}")
                else:
                    print("‚ùå Pr√≥g musi byƒá w zakresie 0.0-1.0")
                    
            elif config_choice == "2":
                new_threshold = float(input(f"Nowy pr√≥g eksploracji (aktualny: {self.drm.exploration_threshold}): "))
                if 0.0 <= new_threshold <= 2.0:
                    self.drm.exploration_threshold = new_threshold
                    print(f"‚úÖ Pr√≥g eksploracji zmieniony na {new_threshold}")
                else:
                    print("‚ùå Pr√≥g musi byƒá w zakresie 0.0-2.0")
                    
            elif config_choice == "3":
                print("1. Eksploracja")
                print("2. Eksploatacja")
                mode_choice = input("Wybierz tryb: ").strip()
                
                if mode_choice == "1":
                    self.drm.mode = SystemMode.EXPLORATION
                    print("‚úÖ Wymuszono tryb eksploracji")
                elif mode_choice == "2":
                    self.drm.mode = SystemMode.EXPLOITATION
                    print("‚úÖ Wymuszono tryb eksploatacji")
                else:
                    print("‚ùå Nieprawid≈Çowy wyb√≥r")
                    
            elif config_choice == "4":
                print(f"Aktualne parametry RLS:")
                print(f"  Pr√≥g: {self.rls.threshold}")
                print(f"  Rozmiar okna: {self.rls.window_size}")
                
                new_rls_threshold = float(input(f"Nowy pr√≥g RLS (aktualny: {self.rls.threshold}): "))
                new_window_size = int(input(f"Nowy rozmiar okna (aktualny: {self.rls.window_size}): "))
                
                if new_rls_threshold > 0 and new_window_size > 0:
                    self.rls.threshold = new_rls_threshold
                    self.rls.window_size = new_window_size
                    print("‚úÖ Parametry RLS zaktualizowane")
                else:
                    print("‚ùå Parametry muszƒÖ byƒá dodatnie")
                    
            elif config_choice == "5":
                confirm = input("‚ö†Ô∏è Czy na pewno chcesz zresetowaƒá system DRM? (tak/nie): ").strip().lower()
                if confirm in ['tak', 'yes', 'y']:
                    self.drm = AdvancedDRM(self.drm.adaptation_threshold, self.drm.exploration_threshold)
                    self._initialize_advanced_rules()
                    print("‚úÖ System DRM zosta≈Ç zresetowany")
                else:
                    print("‚ùå Reset anulowany")
                    
            elif config_choice == "0":
                return
            else:
                print("‚ùå Nieprawid≈Çowy wyb√≥r")
                
        except ValueError as e:
            print(f"‚ùå B≈ÇƒÖd: Nieprawid≈Çowa warto≈õƒá - {e}")
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd konfiguracji: {e}")
    
    def analyze_performance(self):
        """Analiza wydajno≈õci systemu"""
        print("\n=== Analiza wydajno≈õci systemu ===")
        
        if not self.training_history:
            print("‚ùå Brak danych historycznych do analizy")
            return
        
        try:
            # Analiza konwergencji
            print("üìà ANALIZA KONWERGENCJI")
            losses = [h['loss'] for h in self.training_history]
            frz_values = [h['FRZ'] for h in self.training_history]
            
            # Oblicz trendy
            if len(losses) >= 10:
                recent_losses = losses[-10:]
                early_losses = losses[:10]
                
                recent_avg = np.mean(recent_losses)
                early_avg = np.mean(early_losses)
                improvement = ((early_avg - recent_avg) / early_avg) * 100
                
                print(f"  Poprawa loss (ostatnie 10 vs pierwsze 10): {improvement:.2f}%")
                
                # Stabilno≈õƒá
                recent_std = np.std(recent_losses)
                print(f"  Stabilno≈õƒá (odchylenie ostatnich 10): {recent_std:.6f}")
                
                if recent_std < 0.001:
                    print("  ‚úÖ System osiƒÖgnƒÖ≈Ç wysokƒÖ stabilno≈õƒá")
                elif recent_std < 0.01:
                    print("  ‚ö†Ô∏è System jest umiarkowanie stabilny")
                else:
                    print("  ‚ùå System jest niestabilny")
            
            # Analiza efektywno≈õci DRM
            print(f"\nüîß ANALIZA EFEKTYWNO≈öCI DRM")
            
            rule_strength_history = [h['avg_rule_strength'] for h in self.training_history]
            rule_count_history = [h['active_rules'] for h in self.training_history]
            
            print(f"  ≈örednia si≈Ça regu≈Ç: {np.mean(rule_strength_history):.4f}")
            print(f"  Stabilno≈õƒá si≈Çy regu≈Ç: {np.std(rule_strength_history):.4f}")
            print(f"  ≈örednia liczba regu≈Ç: {np.mean(rule_count_history):.1f}")
            print(f"  Zmienno≈õƒá liczby regu≈Ç: {np.std(rule_count_history):.1f}")
            
            # Analiza tryb√≥w
            exploration_epochs = sum(1 for h in self.training_history if h['system_mode'] == 'exploration')
            exploitation_epochs = len(self.training_history) - exploration_epochs
            
            print(f"\nüîç ANALIZA TRYB√ìW SYSTEMU")
            print(f"  Eksploracja: {exploration_epochs} epok ({exploration_epochs/len(self.training_history)*100:.1f}%)")
            print(f"  Eksploatacja: {exploitation_epochs} epok ({exploitation_epochs/len(self.training_history)*100:.1f}%)")
            
            # Efektywno≈õƒá tryb√≥w
            exploration_frz = [h['FRZ'] for h in self.training_history if h['system_mode'] == 'exploration']
            exploitation_frz = [h['FRZ'] for h in self.training_history if h['system_mode'] == 'exploitation']
            
            if exploration_frz and exploitation_frz:
                print(f"  ≈öredni FRZ w eksploracji: {np.mean(exploration_frz):.4f}")
                print(f"  ≈öredni FRZ w eksploatacji: {np.mean(exploitation_frz):.4f}")
                
                if np.mean(exploitation_frz) > np.mean(exploration_frz):
                    print("  ‚úÖ Eksploatacja jest bardziej efektywna")
                else:
                    print("  ‚ö†Ô∏è Eksploracja daje lepsze wyniki")
            
            # Analiza r√≥≈ºnic wykrytych przez RLS
            differences_detected = sum(1 for h in self.training_history if h['difference_detected'])
            print(f"\nüéØ ANALIZA RLS")
            print(f"  Wykryte r√≥≈ºnice: {differences_detected} ({differences_detected/len(self.training_history)*100:.1f}%)")
            
            if differences_detected > 0:
                # Znajd≈∫ epoki z r√≥≈ºnicami
                diff_epochs = [i for i, h in enumerate(self.training_history) if h['difference_detected']]
                
                # Sprawd≈∫ czy r√≥≈ºnice korelujƒÖ z poprawƒÖ
                improvements_after_diff = 0
                for epoch in diff_epochs:
                    if epoch < len(self.training_history) - 5:  # Sprawd≈∫ 5 epok p√≥≈∫niej
                        before_frz = self.training_history[epoch]['FRZ']
                        after_frz = np.mean([self.training_history[i]['FRZ'] 
                                           for i in range(epoch+1, min(epoch+6, len(self.training_history)))])
                        if after_frz > before_frz:
                            improvements_after_diff += 1
                
                improvement_rate = improvements_after_diff / len(diff_epochs) * 100
                print(f"  Poprawa po wykryciu r√≥≈ºnic: {improvement_rate:.1f}%")
                
                if improvement_rate > 60:
                    print("  ‚úÖ RLS skutecznie wykrywa momenty poprawy")
                else:
                    print("  ‚ö†Ô∏è RLS mo≈ºe wymagaƒá dostrojenia")
            
            # Rekomendacje
            print(f"\nüí° REKOMENDACJE")
            
            if improvement < 10:
                print("  ‚Ä¢ Rozwa≈º zwiƒôkszenie learning rate lub liczby epok")
            
            if recent_std > 0.01:
                print("  ‚Ä¢ System jest niestabilny - rozwa≈º zmniejszenie learning rate")
            
            if np.mean(rule_strength_history) < 0.5:
                print("  ‚Ä¢ Niska si≈Ça regu≈Ç - rozwa≈º zmniejszenie progu adaptacji")
            
            if exploration_epochs / len(self.training_history) > 0.7:
                print("  ‚Ä¢ Zbyt du≈ºo eksploracji - rozwa≈º zwiƒôkszenie progu eksploracji")
            elif exploration_epochs / len(self.training_history) < 0.1:
                print("  ‚Ä¢ Zbyt ma≈Ço eksploracji - rozwa≈º zmniejszenie progu eksploracji")
            
            if differences_detected / len(self.training_history) > 0.5:
                print("  ‚Ä¢ RLS wykrywa zbyt wiele r√≥≈ºnic - rozwa≈º zwiƒôkszenie progu")
            elif differences_detected / len(self.training_history) < 0.05:
                print("  ‚Ä¢ RLS wykrywa zbyt ma≈Ço r√≥≈ºnic - rozwa≈º zmniejszenie progu")
                
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd podczas analizy: {e}")
    
    def export_data(self):
        """Eksport danych do analizy"""
        print("\n=== Eksport danych ===")
        
        try:
            print("1. Eksport historii trenowania (JSON)")
            print("2. Eksport statystyk DRM (JSON)")
            print("3. Eksport danych do CSV")
            print("4. Eksport pe≈Çnego raportu (TXT)")
            print("0. Powr√≥t")
            
            export_choice = input("Wybierz opcjƒô: ").strip()
            
            if export_choice == "1":
                filename = input("Nazwa pliku (bez rozszerzenia): ").strip() or "training_history"
                
                with open(f"{filename}.json", 'w', encoding='utf-8') as f:
                    json.dump(self.training_history, f, indent=2, ensure_ascii=False)
                
                print(f"‚úÖ Historia trenowania zapisana do {filename}.json")
                
            elif export_choice == "2":
                filename = input("Nazwa pliku (bez rozszerzenia): ").strip() or "drm_statistics"
                
                drm_stats = self.drm.get_detailed_statistics()
                
                with open(f"{filename}.json", 'w', encoding='utf-8') as f:
                    json.dump(drm_stats, f, indent=2, ensure_ascii=False)
                
                print(f"‚úÖ Statystyki DRM zapisane do {filename}.json")
                
            elif export_choice == "3":
                filename = input("Nazwa pliku (bez rozszerzenia): ").strip() or "training_data"
                
                if not self.training_history:
                    print("‚ùå Brak danych do eksportu")
                    return
                
                import csv
                
                with open(f"{filename}.csv", 'w', newline='', encoding='utf-8') as f:
                    writer = csv.writer(f)
                    
                    # Nag≈Ç√≥wki
                    headers = ['epoch', 'loss', 'accuracy', 'FRZ', 'system_mode', 
                              'avg_rule_strength', 'active_rules', 'difference_detected']
                    writer.writerow(headers)
                    
                    # Dane
                    for h in self.training_history:
                        row = [h.get(header, '') for header in headers]
                        writer.writerow(row)
                
                print(f"‚úÖ Dane treningowe zapisane do {filename}.csv")
                
            elif export_choice == "4":
                filename = input("Nazwa pliku (bez rozszerzenia): ").strip() or "full_report"
                
                with open(f"{filename}.txt", 'w', encoding='utf-8') as f:
                    f.write("PE≈ÅNY RAPORT SYSTEMU LOOPDRM\n")
                    f.write("=" * 50 + "\n\n")
                    
                    # Podstawowe informacje
                    f.write(f"Data generowania: {np.datetime64('now')}\n")
                    f.write(f"Architektura sieci: {self.input_size} ‚Üí {self.hidden_size} ‚Üí {self.output_size}\n")
                    f.write(f"Liczba pr√≥bek treningowych: {len(self.training_inputs)}\n")
                    f.write(f"Liczba epok: {len(self.training_history)}\n\n")
                    
                    # Statystyki DRM
                    drm_stats = self.drm.get_detailed_statistics()
                    f.write("STATYSTYKI DRM:\n")
                    f.write(f"Czas systemowy: {drm_stats['system_time']}\n")
                    f.write(f"Tryb: {drm_stats['system_mode']}\n")
                    f.write(f"≈örednia si≈Ça: {drm_stats['average_strength']:.4f}\n")
                    f.write(f"Liczba regu≈Ç: {drm_stats['total_rules']}\n")
                    f.write(f"Pr√≥g adaptacji: {drm_stats['adaptation_threshold']}\n")
                    f.write(f"Pr√≥g eksploracji: {drm_stats['exploration_threshold']}\n\n")
                    
                    # Szczeg√≥≈Çy regu≈Ç
                    f.write("SZCZEG√ì≈ÅY REGU≈Å:\n")
                    f.write("-" * 80 + "\n")
                    f.write(f"{'ID':<20} {'Si≈Ça':<10} {'W':<8} {'C':<8} {'U':<8} {'R':<8} {'Sukces%':<10} {'Wiek':<6}\n")
                    f.write("-" * 80 + "\n")
                    
                    for rule_id, rule_data in drm_stats['rules'].items():
                        f.write(f"{rule_id:<20} "
                               f"{rule_data['strength']:<10.3f} "
                               f"{rule_data['effectiveness_weight']:<8.2f} "
                               f"{rule_data['context_range']:<8.2f} "
                               f"{rule_data['usage']:<8.1f} "
                               f"{rule_data['resonance']:<8.3f} "
                               f"{rule_data['success_rate']*100:<10.1f} "
                               f"{rule_data['age']:<6}\n")
                    
                    # Statystyki RLS
                    rls_stats = self.rls.get_statistics()
                    f.write(f"\nSTATYSTYKI RLS:\n")
                    f.write(f"Pr√≥g: {rls_stats['threshold']}\n")
                    f.write(f"Rozmiar okna: {self.rls.window_size}\n")
                    f.write(f"Wykryte r√≥≈ºnice: {rls_stats['differences_detected']}\n")
                    f.write(f"Aktualny FRZ: {rls_stats['current_FRZ']:.4f}\n")
                    f.write(f"≈öredni FRZ: {rls_stats['average_FRZ']:.4f}\n")
                    f.write(f"Trend FRZ: {rls_stats['FRZ_trend']}\n\n")
                    
                    # Historia trenowania (ostatnie 20 epok)
                    if self.training_history:
                        f.write("OSTATNIE 20 EPOK TRENOWANIA:\n")
                        f.write("-" * 60 + "\n")
                        f.write(f"{'Epoka':<8} {'Loss':<12} {'FRZ':<8} {'Tryb':<12} {'Regu≈Çy':<8}\n")
                        f.write("-" * 60 + "\n")
                        
                        for h in self.training_history[-20:]:
                            f.write(f"{h['epoch']:<8} "
                                   f"{h['loss']:<12.6f} "
                                   f"{h['FRZ']:<8.3f} "
                                   f"{h['system_mode']:<12} "
                                   f"{h['active_rules']:<8}\n")
                
                print(f"‚úÖ Pe≈Çny raport zapisany do {filename}.txt")
                
            elif export_choice == "0":
                return
            else:
                print("‚ùå Nieprawid≈Çowy wyb√≥r")
                
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd podczas eksportu: {e}")

# Funkcja g≈Ç√≥wna do uruchomienia systemu
def main():
    """Funkcja g≈Ç√≥wna uruchamiajƒÖca zaawansowany system LoopDRM"""
    print("üöÄ Inicjalizacja Zaawansowanego Systemu LoopDRM...")
    
    try:
        # Zapytaj o konfiguracjƒô sieci
        print("\n=== Konfiguracja sieci neuronowej ===")
        input_size = int(input("Rozmiar warstwy wej≈õciowej (domy≈õlnie 10): ") or "10")
        hidden_size = int(input("Rozmiar warstwy ukrytej (domy≈õlnie 20): ") or "20")
        output_size = int(input("Rozmiar warstwy wyj≈õciowej (domy≈õlnie 3): ") or "3")
        
        # Inicjalizuj system
        system = AdvancedLoopDRMSystem(input_size, hidden_size, output_size)
        
        print(f"\n‚úÖ System zainicjalizowany!")
        print(f"üß† Architektura sieci: {input_size} ‚Üí {hidden_size} ‚Üí {output_size}")
        print(f"üîß DRM: {len(system.drm.rules)} regu≈Ç poczƒÖtkowych")
        print(f"üéØ RLS: pr√≥g = {system.rls.threshold}, okno = {system.rls.window_size}")
        
        # Uruchom menu g≈Ç√≥wne
        system.run_advanced_menu()
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Program przerwany przez u≈ºytkownika.")
    except ValueError as e:
        print(f"‚ùå B≈ÇƒÖd konfiguracji: {e}")
        print("U≈ºywam domy≈õlnych warto≈õci...")
        system = AdvancedLoopDRMSystem()
        system.run_advanced_menu()
    except Exception as e:
        print(f"‚ùå Krytyczny b≈ÇƒÖd systemu: {e}")
        print("Sprawd≈∫ instalacjƒô wymaganych bibliotek:")
        print("pip install torch numpy")

# Dodatkowe funkcje pomocnicze
def create_demo_system():
    """Utw√≥rz system demonstracyjny z przyk≈Çadowymi danymi"""
    print("üé≠ Tworzenie systemu demonstracyjnego...")
    
    system = AdvancedLoopDRMSystem(input_size=5, hidden_size=10, output_size=2)
    
    # Dodaj przyk≈Çadowe dane
    np.random.seed(42)  # Dla powtarzalno≈õci
    for i in range(50):
        # R√≥≈ºne wzorce danych
        if i % 3 == 0:  # Wzorzec liniowy
            inputs = np.linspace(-2, 2, 5).tolist()
            targets = [np.sum(inputs), np.mean(inputs)]
        elif i % 3 == 1:  # Wzorzec kwadratowy
            inputs = (np.random.randn(5) ** 2).tolist()
            targets = [np.max(inputs), np.std(inputs)]
        else:  # Wzorzec losowy
            inputs = np.random.randn(5).tolist()
            targets = [np.median(inputs), np.var(inputs)]
        
        system.training_inputs.append(inputs)
        system.training_targets.append(targets)
    
    print(f"‚úÖ System demonstracyjny utworzony z {len(system.training_inputs)} pr√≥bkami")
    return system

def run_benchmark():
    """Uruchom test wydajno≈õci systemu"""
    print("‚è±Ô∏è Uruchamianie testu wydajno≈õci...")
    
    import time
    
    system = AdvancedLoopDRMSystem(input_size=8, hidden_size=16, output_size=4)
    
    # Generuj dane testowe
    start_time = time.time()
    
    for i in range(100):
        inputs = np.random.randn(8).tolist()
        targets = np.random.randn(4).tolist()
        system.training_inputs.append(inputs)
        system.training_targets.append(targets)
    
    data_gen_time = time.time() - start_time
    
    # Test trenowania
    start_time = time.time()
    
    inputs_tensor = torch.FloatTensor(system.training_inputs)
    targets_tensor = torch.FloatTensor(system.training_targets)
    
    for epoch in range(50):
        outputs = system.model(inputs_tensor)
        loss = system.criterion(outputs, targets_tensor)
        
        system.optimizer.zero_grad()
        loss.backward()
        system.optimizer.step()
        
        # Symuluj DRM
        FRZ = system.rls.calculate_FRZ(loss.item())
        for input_data in system.training_inputs[:5]:  # Tylko pierwsze 5 dla szybko≈õci
            system.drm.apply_rules(input_data, FRZ)
    
    training_time = time.time() - start_time
    
    print(f"üìä Wyniki benchmarku:")
    print(f"  Generowanie danych: {data_gen_time:.4f}s")
    print(f"  Trenowanie (50 epok): {training_time:.4f}s")
    print(f"  ≈öredni czas na epokƒô: {training_time/50:.4f}s")
    print(f"  Regu≈Çy DRM: {len(system.drm.rules)}")
    print(f"  Czas systemowy DRM: {system.drm.T}")

# Uruchomienie programu
if __name__ == "__main__":
    print("üåü Witaj w Zaawansowanym Systemie LoopDRM!")
    print("=" * 60)
    print("Ten system implementuje:")
    print("‚Ä¢ üîß DynamicznƒÖ Matrycƒô Regu≈Ç (DRM) z matematycznymi wzorami")
    print("‚Ä¢ üéØ Rozszerzony System Uczenia Regu≈Ç (RLS) z FRZ")
    print("‚Ä¢ üß† Sieƒá neuronowƒÖ z adaptacyjnym uczeniem")
    print("‚Ä¢ üìä ZaawansowanƒÖ analizƒô i eksport danych")
    print("=" * 60)
    
    print("\nWybierz tryb uruchomienia:")
    print("1. üöÄ Normalny start")
    print("2. üé≠ System demonstracyjny")
    print("3. ‚è±Ô∏è Test wydajno≈õci")
    print("0. üö™ Wyj≈õcie")
    
    try:
        choice = input("\nTw√≥j wyb√≥r: ").strip()
        
        if choice == "1":
            main()
        elif choice == "2":
            demo_system = create_demo_system()
            demo_system.run_advanced_menu()
        elif choice == "3":
            run_benchmark()
        elif choice == "0":
            print("üëã Do zobaczenia!")
        else:
            print("‚ùå Nieprawid≈Çowy wyb√≥r, uruchamiam normalny start...")
            main()
            
    except KeyboardInterrupt:
        print("\n\nüëã Program zako≈Ñczony przez u≈ºytkownika.")
    except Exception as e:
        print(f"\n‚ùå B≈ÇƒÖd: {e}")
        print("Uruchamiam system w trybie awaryjnym...")
        try:
            system = AdvancedLoopDRMSystem()
            system.run_advanced_menu()
        except:
            print("‚ùå Nie mo≈ºna uruchomiƒá systemu. Sprawd≈∫ instalacjƒô bibliotek.")
            print("\nüîß Wymagane biblioteki:")
            print("  ‚Ä¢ torch (PyTorch)")
            print("  ‚Ä¢ numpy")
            print("  ‚Ä¢ json (wbudowana)")
            print("  ‚Ä¢ dataclasses (wbudowana)")
            print("  ‚Ä¢ enum (wbudowana)")
            print("\nüì¶ Instalacja:")
            print("  pip install torch numpy")
            print("\nüÜò Je≈õli problem nadal wystƒôpuje, sprawd≈∫:")
            print("  ‚Ä¢ Wersjƒô Pythona (wymagana 3.7+)")
            print("  ‚Ä¢ Dostƒôpno≈õƒá pamiƒôci RAM")
            print("  ‚Ä¢ Uprawnienia do zapisu plik√≥w")

# Klasy pomocnicze i dodatkowe funkcjonalno≈õci

class DRMVisualizer:
    """Klasa do wizualizacji dzia≈Çania DRM"""
    
    def __init__(self, drm_system):
        self.drm = drm_system
        
    def print_rule_evolution(self, history_length=10):
        """Wy≈õwietl ewolucjƒô regu≈Ç"""
        print(f"\nüìà EWOLUCJA REGU≈Å (ostatnie {history_length} krok√≥w)")
        print("=" * 80)
        
        if len(self.drm.memory) < history_length:
            print("‚ö†Ô∏è NiewystarczajƒÖca historia do analizy")
            return
        
        # Analiza zmian si≈Çy regu≈Ç w czasie
        recent_memory = self.drm.memory[-history_length:]
        
        for i, mem_point in enumerate(recent_memory):
            print(f"\nKrok {mem_point['time']} (FRZ: {mem_point['FRZ']:.3f}):")
            
            # Oblicz si≈Çy regu≈Ç dla tego momentu
            temp_T = self.drm.T
            self.drm.T = mem_point['time']
            
            rule_strengths = {}
            for rule_id in self.drm.rules.keys():
                strength = self.drm.calculate_rule_strength(rule_id)
                rule_strengths[rule_id] = strength
            
            # Przywr√≥ƒá oryginalny czas
            self.drm.T = temp_T
            
            # Poka≈º top 3 regu≈Çy
            sorted_rules = sorted(rule_strengths.items(), key=lambda x: x[1], reverse=True)
            for j, (rule_id, strength) in enumerate(sorted_rules[:3]):
                bar_length = int(strength * 20)  # Skala 0-20 znak√≥w
                bar = "‚ñà" * bar_length + "‚ñë" * (20 - bar_length)
                print(f"  {j+1}. {rule_id:<20} [{bar}] {strength:.3f}")
    
    def print_context_analysis(self, context):
        """Analiza kontekstu wej≈õciowego"""
        print(f"\nüîç ANALIZA KONTEKSTU")
        print("=" * 50)
        
        print(f"Warto≈õci: {[f'{x:.3f}' for x in context]}")
        print(f"Statystyki:")
        print(f"  ‚Ä¢ ≈örednia: {np.mean(context):.4f}")
        print(f"  ‚Ä¢ Odchylenie: {np.std(context):.4f}")
        print(f"  ‚Ä¢ Minimum: {np.min(context):.4f}")
        print(f"  ‚Ä¢ Maksimum: {np.max(context):.4f}")
        print(f"  ‚Ä¢ Zakres: {np.max(context) - np.min(context):.4f}")
        
        # Por√≥wnanie z historiƒÖ
        if len(self.drm.memory) > 0:
            print(f"\nPor√≥wnanie z historiƒÖ:")
            historical_contexts = [mem['context'] for mem in self.drm.memory[-10:]]
            
            if historical_contexts:
                avg_historical = np.mean([np.mean(ctx) for ctx in historical_contexts])
                current_avg = np.mean(context)
                
                difference = abs(current_avg - avg_historical)
                similarity = 1.0 / (1.0 + difference)
                
                print(f"  ‚Ä¢ Podobie≈Ñstwo do historii: {similarity:.3f}")
                
                if similarity > 0.8:
                    print("  ‚úÖ Kontekst bardzo podobny do historycznych")
                elif similarity > 0.5:
                    print("  ‚ö†Ô∏è Kontekst umiarkowanie podobny")
                else:
                    print("  üö® Kontekst znacznie r√≥≈ºni siƒô od historycznych")

class PerformanceMonitor:
    """Monitor wydajno≈õci systemu"""
    
    def __init__(self):
        self.start_time = None
        self.metrics = {}
        self.checkpoints = []
        
    def start_monitoring(self):
        """Rozpocznij monitorowanie"""
        import time
        self.start_time = time.time()
        self.metrics = {
            'total_time': 0,
            'epochs_processed': 0,
            'rules_created': 0,
            'rules_deleted': 0,
            'mode_switches': 0,
            'memory_usage': []
        }
        print("üìä Monitoring wydajno≈õci rozpoczƒôty")
        
    def checkpoint(self, description=""):
        """Utw√≥rz punkt kontrolny"""
        if self.start_time is None:
            return
            
        import time
        current_time = time.time()
        elapsed = current_time - self.start_time
        
        checkpoint_data = {
            'time': elapsed,
            'description': description,
            'timestamp': current_time
        }
        
        self.checkpoints.append(checkpoint_data)
        print(f"‚è±Ô∏è Checkpoint: {description} ({elapsed:.3f}s)")
        
    def update_metrics(self, **kwargs):
        """Aktualizuj metryki"""
        for key, value in kwargs.items():
            if key in self.metrics:
                if isinstance(self.metrics[key], list):
                    self.metrics[key].append(value)
                else:
                    self.metrics[key] = value
                    
    def get_memory_usage(self):
        """Pobierz u≈ºycie pamiƒôci (je≈õli dostƒôpne)"""
        try:
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            return memory_mb
        except ImportError:
            return None
            
    def print_summary(self):
        """Wy≈õwietl podsumowanie wydajno≈õci"""
        if self.start_time is None:
            print("‚ùå Monitoring nie zosta≈Ç rozpoczƒôty")
            return
            
        import time
        total_time = time.time() - self.start_time
        
        print(f"\nüìä PODSUMOWANIE WYDAJNO≈öCI")
        print("=" * 50)
        print(f"Ca≈Çkowity czas: {total_time:.3f}s")
        print(f"Przetworzone epoki: {self.metrics.get('epochs_processed', 0)}")
        
        if self.metrics.get('epochs_processed', 0) > 0:
            avg_time_per_epoch = total_time / self.metrics['epochs_processed']
            print(f"≈öredni czas na epokƒô: {avg_time_per_epoch:.4f}s")
            
        print(f"Utworzone regu≈Çy: {self.metrics.get('rules_created', 0)}")
        print(f"Usuniƒôte regu≈Çy: {self.metrics.get('rules_deleted', 0)}")
        print(f"Zmiany trybu: {self.metrics.get('mode_switches', 0)}")
        
        # U≈ºycie pamiƒôci
        memory_usage = self.get_memory_usage()
        if memory_usage:
            print(f"U≈ºycie pamiƒôci: {memory_usage:.1f} MB")
            
        # Checkpointy
        if self.checkpoints:
            print(f"\nPunkty kontrolne:")
            for i, cp in enumerate(self.checkpoints):
                print(f"  {i+1}. {cp['description']}: {cp['time']:.3f}s")

class ConfigManager:
    """Mened≈ºer konfiguracji systemu"""
    
    @staticmethod
    def save_config(system, filename="config.json"):
        """Zapisz konfiguracjƒô systemu"""
        config = {
            'network': {
                'input_size': system.input_size,
                'hidden_size': system.hidden_size,
                'output_size': system.output_size
            },
            'drm': {
                'adaptation_threshold': system.drm.adaptation_threshold,
                'exploration_threshold': system.drm.exploration_threshold,
                'mode': system.drm.mode.value
            },
            'rls': {
                'threshold': system.rls.threshold,
                'window_size': system.rls.window_size
            },
            'optimizer': {
                'lr': system.optimizer.param_groups[0]['lr']
            }
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            print(f"‚úÖ Konfiguracja zapisana do {filename}")
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd zapisu konfiguracji: {e}")
            
    @staticmethod
    def load_config(filename="config.json"):
        """Wczytaj konfiguracjƒô"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"‚úÖ Konfiguracja wczytana z {filename}")
            return config
        except FileNotFoundError:
            print(f"‚ùå Plik {filename} nie istnieje")
            return None
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd wczytywania konfiguracji: {e}")
            return None
            
    @staticmethod
    def create_system_from_config(config):
        """Utw√≥rz system na podstawie konfiguracji"""
        if not config:
            return None
            
        try:
            # Utw√≥rz system
            system = AdvancedLoopDRMSystem(
                input_size=config['network']['input_size'],
                hidden_size=config['network']['hidden_size'],
                output_size=config['network']['output_size']
            )
            
            # Zastosuj konfiguracjƒô DRM
            system.drm.adaptation_threshold = config['drm']['adaptation_threshold']
            system.drm.exploration_threshold = config['drm']['exploration_threshold']
            system.drm.mode = SystemMode(config['drm']['mode'])
            
            # Zastosuj konfiguracjƒô RLS
            system.rls.threshold = config['rls']['threshold']
            system.rls.window_size = config['rls']['window_size']
            
            # Zastosuj konfiguracjƒô optymalizatora
            for param_group in system.optimizer.param_groups:
                param_group['lr'] = config['optimizer']['lr']
                
            print("‚úÖ System utworzony na podstawie konfiguracji")
            return system
            
        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd tworzenia systemu z konfiguracji: {e}")
            return None

# Funkcje narzƒôdziowe
def validate_system_integrity(system):
    """Sprawd≈∫ integralno≈õƒá systemu"""
    print("\nüîç SPRAWDZANIE INTEGRALNO≈öCI SYSTEMU")
    print("=" * 50)
    
    issues = []
    
    # Sprawd≈∫ sieƒá neuronowƒÖ
    try:
        test_input = torch.randn(1, system.input_size)
        output = system.model(test_input)
        if output.shape[1] != system.output_size:
            issues.append("Nieprawid≈Çowy rozmiar wyj≈õcia sieci")
    except Exception as e:
        issues.append(f"B≈ÇƒÖd sieci neuronowej: {e}")
    
    # Sprawd≈∫ DRM
    if len(system.drm.rules) == 0:
        issues.append("Brak regu≈Ç w DRM")
        
    for rule_id, rule in system.drm.rules.items():
        if not callable(rule['condition']) or not callable(rule['action']):
            issues.append(f"Nieprawid≈Çowa regu≈Ça: {rule_id}")
            
        strength = system.drm.calculate_rule_strength(rule_id)
        if strength < 0:
            issues.append(f"Ujemna si≈Ça regu≈Çy: {rule_id}")
    
    # Sprawd≈∫ RLS
    if system.rls.threshold <= 0:
        issues.append("Nieprawid≈Çowy pr√≥g RLS")
        
    if system.rls.window_size <= 0:
        issues.append("Nieprawid≈Çowy rozmiar okna RLS")
    
    # Sprawd≈∫ dane treningowe
    if system.training_inputs and system.training_targets:
        if len(system.training_inputs) != len(system.training_targets):
            issues.append("Niezgodno≈õƒá liczby danych wej≈õciowych i docelowych")
            
        for i, (inp, tgt) in enumerate(zip(system.training_inputs, system.training_targets)):
            if len(inp) != system.input_size:
                issues.append(f"Nieprawid≈Çowy rozmiar danych wej≈õciowych #{i}")
            if len(tgt) != system.output_size:
                issues.append(f"Nieprawid≈Çowy rozmiar danych docelowych #{i}")
    
    # Wy≈õwietl wyniki
    if issues:
        print("‚ùå ZNALEZIONE PROBLEMY:")
        for i, issue in enumerate(issues, 1):
            print(f"  {i}. {issue}")
        return False
    else:
        print("‚úÖ System jest w pe≈Çni sprawny")
        return True

def generate_test_data(input_size, output_size, num_samples=100, pattern="mixed"):
    """Generuj dane testowe z r√≥≈ºnymi wzorcami"""
    inputs = []
    targets = []
    
    np.random.seed(42)  # Dla powtarzalno≈õci
    
    for i in range(num_samples):
        if pattern == "linear":
            # Wzorzec liniowy
            inp = np.linspace(-2, 2, input_size).tolist()
            tgt = [np.sum(inp[:output_size])]
            while len(tgt) < output_size:
                tgt.append(np.mean(inp))
                
        
        
        elif pattern == "quadratic":
            # Wzorzec kwadratowy
            inp = (np.random.randn(input_size) ** 2).tolist()
            tgt = [np.max(inp[:output_size])]
            while len(tgt) < output_size:
                tgt.append(np.std(inp))
                
        elif pattern == "sinusoidal":
            # Wzorzec sinusoidalny
            x = np.linspace(0, 2*np.pi, input_size)
            inp = np.sin(x + i * 0.1).tolist()
            tgt = [np.mean(inp), np.amplitude(inp) if output_size > 1 else np.mean(inp)]
            while len(tgt) < output_size:
                tgt.append(np.var(inp))
                
        elif pattern == "mixed":
            # Wzorzec mieszany
            if i % 4 == 0:
                inp = np.random.randn(input_size).tolist()
                tgt = [np.mean(inp), np.std(inp)]
            elif i % 4 == 1:
                inp = (np.random.randn(input_size) * 2 + 1).tolist()
                tgt = [np.median(inp), np.max(inp)]
            elif i % 4 == 2:
                inp = np.random.exponential(1, input_size).tolist()
                tgt = [np.min(inp), np.sum(inp)]
            else:
                inp = np.random.uniform(-3, 3, input_size).tolist()
                tgt = [np.var(inp), np.mean(np.abs(inp))]
            
            # Dopasuj rozmiar targets
            while len(tgt) < output_size:
                tgt.append(np.random.randn())
            tgt = tgt[:output_size]
            
        else:  # random
            inp = np.random.randn(input_size).tolist()
            tgt = np.random.randn(output_size).tolist()
        
        inputs.append(inp)
        targets.append(tgt)
    
    return inputs, targets

def run_automated_test():
    """Uruchom zautomatyzowany test systemu"""
    print("ü§ñ ZAUTOMATYZOWANY TEST SYSTEMU")
    print("=" * 50)
    
    # Utw√≥rz system testowy
    system = AdvancedLoopDRMSystem(input_size=6, hidden_size=12, output_size=3)
    monitor = PerformanceMonitor()
    
    monitor.start_monitoring()
    
    # Sprawd≈∫ integralno≈õƒá
    monitor.checkpoint("Sprawdzanie integralno≈õci")
    if not validate_system_integrity(system):
        print("‚ùå Test przerwany - problemy z integralno≈õciƒÖ")
        return
    
    # Wygeneruj dane testowe
    monitor.checkpoint("Generowanie danych")
    inputs, targets = generate_test_data(6, 3, 80, "mixed")
    system.training_inputs = inputs
    system.training_targets = targets
    
    print(f"‚úÖ Wygenerowano {len(inputs)} pr√≥bek danych")
    
    # Trenowanie automatyczne
    monitor.checkpoint("Rozpoczƒôcie trenowania")
    
    inputs_tensor = torch.FloatTensor(system.training_inputs)
    targets_tensor = torch.FloatTensor(system.training_targets)
    
    epochs = 30
    print(f"üéØ Trenowanie przez {epochs} epok...")
    
    for epoch in range(epochs):
        # Forward pass
        outputs = system.model(inputs_tensor)
        loss = system.criterion(outputs, targets_tensor)
        
        # Oblicz dok≈Çadno≈õƒá
        accuracy = system.calculate_prediction_accuracy(outputs, targets_tensor)
        
        # Oblicz FRZ
        FRZ = system.rls.calculate_FRZ(loss.item(), accuracy)
        
        # Backward pass
        system.optimizer.zero_grad()
        loss.backward()
        system.optimizer.step()
        
        # Zastosuj DRM dla pr√≥bki danych
        sample_contexts = system.training_inputs[::10]  # Co 10-ta pr√≥bka
        for context in sample_contexts:
            system.drm.apply_rules(context, FRZ)
        
        # Wykryj r√≥≈ºnice
        difference_detected = system.rls.detect_difference(loss.item())
        
        # Zapisz metryki
        epoch_metrics = {
            'epoch': epoch + 1,
            'loss': loss.item(),
            'accuracy': accuracy,
            'FRZ': FRZ,
            'system_mode': system.drm.mode.value,
            'avg_rule_strength': system.drm.calculate_system_average_strength(),
            'active_rules': len(system.drm.rules),
            'difference_detected': difference_detected
        }
        system.training_history.append(epoch_metrics)
        
        # Aktualizuj monitor
        monitor.update_metrics(epochs_processed=epoch + 1)
        
        # Wy≈õwietl postƒôp co 10 epok
        if epoch % 10 == 0 or epoch == epochs - 1:
            print(f"Epoka {epoch+1:2d}/{epochs} | "
                  f"Loss: {loss.item():.6f} | "
                  f"FRZ: {FRZ:.3f} | "
                  f"Regu≈Çy: {len(system.drm.rules)} | "
                  f"Tryb: {system.drm.mode.value}")
    
    monitor.checkpoint("Zako≈Ñczenie trenowania")
    
    # Test predykcji
    monitor.checkpoint("Test predykcji")
    test_input = np.random.randn(6).tolist()
    
    with torch.no_grad():
        input_tensor = torch.FloatTensor([test_input])
        prediction = system.model(input_tensor)[0].tolist()
    
    print(f"\nüß™ Test predykcji:")
    print(f"Wej≈õcie: {[f'{x:.3f}' for x in test_input]}")
    print(f"Predykcja: {[f'{x:.3f}' for x in prediction]}")
    
    # Analiza DRM
    current_FRZ = system.rls.get_current_FRZ()
    drm_result = system.drm.apply_rules(test_input, current_FRZ)
    
    print(f"\nüîß Analiza DRM:")
    print(f"Zastosowane regu≈Çy: {len(drm_result['applied_rules'])}")
    print(f"≈örednia si≈Ça systemu: {drm_result['average_strength']:.3f}")
    print(f"Tryb systemu: {drm_result['system_mode']}")
    
    # Podsumowanie
    monitor.checkpoint("Finalizacja testu")
    monitor.print_summary()
    
    # Ocena wynik√≥w
    print(f"\nüèÜ OCENA WYNIK√ìW:")
    
    if system.training_history:
        initial_loss = system.training_history[0]['loss']
        final_loss = system.training_history[-1]['loss']
        improvement = ((initial_loss - final_loss) / initial_loss) * 100
        
        print(f"Poprawa loss: {improvement:.1f}%")
        
        if improvement > 50:
            print("‚úÖ Doskona≈Çy wynik!")
        elif improvement > 20:
            print("‚úÖ Dobry wynik!")
        elif improvement > 5:
            print("‚ö†Ô∏è Umiarkowany wynik")
        else:
            print("‚ùå S≈Çaby wynik - system wymaga dostrojenia")
        
        # Analiza stabilno≈õci DRM
        mode_changes = 0
        for i in range(1, len(system.training_history)):
            if (system.training_history[i]['system_mode'] != 
                system.training_history[i-1]['system_mode']):
                mode_changes += 1
        
        print(f"Zmiany trybu DRM: {mode_changes}")
        
        if mode_changes > epochs * 0.3:
            print("‚ö†Ô∏è System czƒôsto zmienia tryby - mo≈ºe byƒá niestabilny")
        elif mode_changes < 2:
            print("‚ö†Ô∏è System rzadko zmienia tryby - mo≈ºe brakowaƒá adaptacji")
        else:
            print("‚úÖ Optymalna czƒôstotliwo≈õƒá zmian tryb√≥w")
    
    return system

# Funkcje diagnostyczne
def diagnose_training_issues(system):
    """Diagnozuj problemy z trenowaniem"""
    print("\nüî¨ DIAGNOZA PROBLEM√ìW TRENOWANIA")
    print("=" * 50)
    
    if not system.training_history:
        print("‚ùå Brak historii trenowania do analizy")
        return
    
    issues = []
    recommendations = []
    
    # Analiza konwergencji
    losses = [h['loss'] for h in system.training_history]
    
    if len(losses) >= 10:
        recent_losses = losses[-10:]
        early_losses = losses[:10]
        
        recent_avg = np.mean(recent_losses)
        early_avg = np.mean(early_losses)
        
        if recent_avg >= early_avg * 0.95:  # Mniej ni≈º 5% poprawy
            issues.append("Brak znaczƒÖcej poprawy loss")
            recommendations.append("Zwiƒôksz learning rate lub liczbƒô epok")
        
        # Sprawd≈∫ stabilno≈õƒá
        recent_std = np.std(recent_losses)
        if recent_std > recent_avg * 0.1:  # Odchylenie > 10% ≈õredniej
            issues.append("Niestabilne trenowanie")
            recommendations.append("Zmniejsz learning rate")
    
    # Analiza FRZ
    frz_values = [h['FRZ'] for h in system.training_history]
    avg_frz = np.mean(frz_values)
    
    if avg_frz < 0.3:
        issues.append("Niski ≈õredni FRZ")
        recommendations.append("Sprawd≈∫ jako≈õƒá danych lub architekturƒô sieci")
    elif avg_frz > 0.9:
        issues.append("Bardzo wysoki FRZ - mo≈ºliwe przeuczenie")
        recommendations.append("Dodaj regularyzacjƒô lub wiƒôcej danych")
    
    # Analiza DRM
    rule_counts = [h['active_rules'] for h in system.training_history]
    
    if max(rule_counts) - min(rule_counts) > len(system.drm.rules) * 0.5:
        issues.append("Du≈ºe wahania liczby aktywnych regu≈Ç")
        recommendations.append("Dostosuj progi adaptacji DRM")
    
    avg_strength = [h['avg_rule_strength'] for h in system.training_history]
    if np.mean(avg_strength) < 0.3:
        issues.append("Niska ≈õrednia si≈Ça regu≈Ç")
        recommendations.append("Zmniejsz pr√≥g adaptacji lub zwiƒôksz pr√≥g eksploracji")
    
    # Wy≈õwietl wyniki
    if issues:
        print("üö® ZIDENTYFIKOWANE PROBLEMY:")
        for i, issue in enumerate(issues, 1):
            print(f"  {i}. {issue}")
        
        print(f"\nüí° REKOMENDACJE:")
        for i, rec in enumerate(recommendations, 1):
            print(f"  {i}. {rec}")
    else:
        print("‚úÖ Nie wykryto problem√≥w z trenowaniem")

# Ostatnie funkcje i zako≈Ñczenie
def create_quick_demo():
    """Szybka demonstracja mo≈ºliwo≈õci systemu"""
    print("‚ö° SZYBKA DEMONSTRACJA")
    print("=" * 30)
    
    # Ma≈Çy system do demonstracji
    system = AdvancedLoopDRMSystem(input_size=4, hidden_size=8, output_size=2)
    
    # Dodaj kilka pr√≥bek
    demo_data = [
        ([1, 2, 3, 4], [2.5, 1.2]),
        ([2, 4, 6, 8], [5.0, 2.4]),
        ([-1, -2, -3, -4], [-2.5, 1.2]),
        ([0.5, 1.5, 2.5, 3.5], [2.0, 1.0])
    ]
    
    for inp, tgt in demo_data:
        system.training_inputs.append(inp)
        system.training_targets.append(tgt)
    
    print(f"üìä Dodano {len(demo_data)} pr√≥bek demonstracyjnych")
    
    # Kr√≥tkie trenowanie
    inputs_tensor = torch.FloatTensor(system.training_inputs)
    targets_tensor = torch.FloatTensor(system.training_targets)
    
    print("üéØ Trenowanie (10 epok)...")
    
    for epoch in range(10):
        outputs = system.model(inputs_tensor)
        loss = system.criterion(outputs, targets_tensor)
        
        system.optimizer.zero_grad()
        loss.backward()
        system.optimizer.step()
        
        # Zastosuj DRM
        FRZ = system.rls.calculate_FRZ(loss.item())
        for context in system.training_inputs:
            system.drm.apply_rules(context, FRZ)
        
        if epoch % 3 == 0:
            print(f"  Epoka {epoch+1}: Loss={loss.item():.4f}, "
                  f"FRZ={FRZ:.3f}, Regu≈Çy={len(system.drm.rules)}")
    
    # Test
    test_input = [1.5, 2.5, 3.5, 4.5]
    with torch.no_grad():
        prediction = system.model(torch.FloatTensor([test_input]))[0].tolist()
    
    print(f"\nüß™ Test: {test_input} ‚Üí {[f'{x:.3f}' for x in prediction]}")
    print("‚úÖ Demonstracja zako≈Ñczona!")

# Informacje o systemie
def print_system_info():
    """Wy≈õwietl informacje o systemie"""
    print("\n" + "="*60)
    print("    üöÄ ZAAWANSOWANY SYSTEM LOOPDRM")
    print("="*60)
    print("üìã OPIS SYSTEMU:")
    print("  ‚Ä¢ Implementacja Dynamicznej Matrycy Regu≈Ç (DRM)")
    print("  ‚Ä¢ Rozszerzony System Uczenia Regu≈Ç (RLS) z FRZ")
    print("  ‚Ä¢ Adaptacyjne sieci neuronowe z PyTorch")
    print("  ‚Ä¢ Automatyczna adaptacja trybu eksploracja/eksploatacja")
    print("  ‚Ä¢ Zaawansowana analiza i wizualizacja")
    
    print(f"\nüî¨ WZORY MATEMATYCZNE:")
    print("  ‚Ä¢ Si≈Ça regu≈Çy: Si = Wi ¬∑ log(Ci + 1) ¬∑ (1 + Ui/T) ¬∑ Ri")
    print("  ‚Ä¢ ≈örednia si≈Ça systemu: SÃÑ = (1/n) Œ£ Si")
    print("  ‚Ä¢ FRZ: Function Resonance Zone = 1 - normalized_loss + accuracy_bonus")
    print("  ‚Ä¢ R√≥≈ºnorodno≈õƒá kontekstu: ||context - avg_context||")
    
    print(f"\nüéõÔ∏è PARAMETRY SYSTEMU:")
    print("  ‚Ä¢ Wi - Waga skuteczno≈õci (0.1 - 5.0)")
    print("  ‚Ä¢ Ci - Zasiƒôg kontekstowy (0.1 - 10.0)")
    print("  ‚Ä¢ Ui - U≈ºycie regu≈Çy (liczba aktywacji)")
    print("  ‚Ä¢ Ri - Rezonans z pamiƒôciƒÖ (0.0 - 1.0)")
    print("  ‚Ä¢ T - Czas systemowy (liczba iteracji)")
    
    print(f"\nüîß TRYBY DZIA≈ÅANIA:")
    print("  ‚Ä¢ EKSPLORACJA - tworzenie nowych regu≈Ç, mutacje")
    print("  ‚Ä¢ EKSPLOATACJA - wykorzystanie najlepszych regu≈Ç")
    print("  ‚Ä¢ Automatyczne prze≈ÇƒÖczanie na podstawie SÃÑ")
    
    print(f"\nüìä MO≈ªLIWO≈öCI:")
    print("  ‚Ä¢ Trenowanie sieci z adaptacyjnym DRM")
    print("  ‚Ä¢ Analiza wydajno≈õci i diagnostyka")
    print("  ‚Ä¢ Eksport danych (JSON, CSV, TXT)")
    print("  ‚Ä¢ Zapis/wczytywanie modeli z metadanymi")
    print("  ‚Ä¢ Wizualizacja ewolucji regu≈Ç")
    print("  ‚Ä¢ Monitoring wydajno≈õci w czasie rzeczywistym")
    
    print(f"\nüë®‚Äçüíª AUTOR: System LoopDRM v2.0")
    print(f"üìÖ WERSJA: Zaawansowana implementacja z matematycznymi wzorami")
    print("="*60)

# Funkcja pomocnicza do debugowania
def debug_system_state(system):
    """Wy≈õwietl szczeg√≥≈Çowy stan systemu do debugowania"""
    print("\nüêõ STAN SYSTEMU (DEBUG)")
    print("="*50)
    
    # Stan sieci neuronowej
    print("üß† SIEƒÜ NEURONOWA:")
    print(f"  Architektura: {system.input_size} ‚Üí {system.hidden_size} ‚Üí {system.output_size}")
    print(f"  Parametry: {sum(p.numel() for p in system.model.parameters())}")
    print(f"  Learning rate: {system.optimizer.param_groups[0]['lr']}")
    
    # Stan DRM
    print(f"\nüîß DRM:")
    print(f"  Czas systemowy T: {system.drm.T}")
    print(f"  Tryb: {system.drm.mode.value}")
    print(f"  Pr√≥g adaptacji: {system.drm.adaptation_threshold}")
    print(f"  Pr√≥g eksploracji: {system.drm.exploration_threshold}")
    print(f"  Liczba regu≈Ç: {len(system.drm.rules)}")
    print(f"  Rozmiar pamiƒôci: {len(system.drm.memory)}")
    
    # Szczeg√≥≈Çy regu≈Ç
    if system.drm.rules:
        print(f"\n  Szczeg√≥≈Çy regu≈Ç:")
        for rule_id, rule in system.drm.rules.items():
            meta = rule['metadata']
            strength = system.drm.calculate_rule_strength(rule_id)
            print(f"    {rule_id}:")
            print(f"      Si≈Ça: {strength:.4f}")
            print(f"      W={meta.W:.2f}, C={meta.C:.2f}, U={meta.U:.1f}, R={meta.R:.3f}")
            print(f"      Sukcesy: {meta.success_count}/{meta.total_activations}")
    
    # Stan RLS
    print(f"\nüéØ RLS:")
    print(f"  Pr√≥g: {system.rls.threshold}")
    print(f"  Rozmiar okna: {system.rls.window_size}")
    print(f"  Historia: {len(system.rls.history)} punkt√≥w")
    print(f"  Wykryte r√≥≈ºnice: {len(system.rls.differences)}")
    print(f"  Historia FRZ: {len(system.rls.FRZ_history)} punkt√≥w")
    print(f"  Aktualny FRZ: {system.rls.get_current_FRZ():.4f}")
    
    # Dane treningowe
    print(f"\nüìä DANE:")
    print(f"  Pr√≥bki treningowe: {len(system.training_inputs)}")
    print(f"  Historia trenowania: {len(system.training_history)} epok")
    
    if system.training_history:
        last_epoch = system.training_history[-1]
        print(f"  Ostatnia epoka:")
        print(f"    Loss: {last_epoch['loss']:.6f}")
        print(f"    FRZ: {last_epoch['FRZ']:.4f}")
        print(f"    Accuracy: {last_epoch.get('accuracy', 'N/A')}")
    
    # Pamiƒôƒá systemowa
    try:
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        print(f"\nüíæ PAMIƒòƒÜ:")
        print(f"  U≈ºycie RAM: {memory_mb:.1f} MB")
    except ImportError:
        print(f"\nüíæ PAMIƒòƒÜ: Modu≈Ç psutil niedostƒôpny")

# Funkcja do testowania konkretnych scenariuszy
def run_scenario_test(scenario="basic"):
    """Uruchom test konkretnego scenariusza"""
    print(f"\nüé¨ TEST SCENARIUSZA: {scenario.upper()}")
    print("="*50)
    
    if scenario == "basic":
        # Podstawowy test funkcjonalno≈õci
        system = AdvancedLoopDRMSystem(4, 8, 2)
        inputs, targets = generate_test_data(4, 2, 20, "linear")
        system.training_inputs = inputs
        system.training_targets = targets
        
        # Kr√≥tkie trenowanie
        system._train_epochs(10)
        print("‚úÖ Podstawowy scenariusz zako≈Ñczony")
        
    elif scenario == "stress":
        # Test obciƒÖ≈ºeniowy
        print("‚ö° Test obciƒÖ≈ºeniowy - du≈ºe dane")
        system = AdvancedLoopDRMSystem(20, 40, 10)
        inputs, targets = generate_test_data(20, 10, 500, "mixed")
        system.training_inputs = inputs
        system.training_targets = targets
        
        import time
        start_time = time.time()
        system._train_epochs(50)
        end_time = time.time()
        
        print(f"‚è±Ô∏è Czas trenowania: {end_time - start_time:.2f}s")
        print("‚úÖ Test obciƒÖ≈ºeniowy zako≈Ñczony")
        
    elif scenario == "adaptation":
        # Test adaptacji DRM
        print("üîÑ Test adaptacji DRM")
        system = AdvancedLoopDRMSystem(6, 12, 3)
        
        # R√≥≈ºne fazy danych
        phase1_inputs, phase1_targets = generate_test_data(6, 3, 30, "linear")
        phase2_inputs, phase2_targets = generate_test_data(6, 3, 30, "quadratic")
        
        # Faza 1
        system.training_inputs = phase1_inputs
        system.training_targets = phase1_targets
        system._train_epochs(20)
        
        print("üìä Zmiana wzorca danych...")
        
        # Faza 2
        system.training_inputs.extend(phase2_inputs)
        system.training_targets.extend(phase2_targets)
        system._train_epochs(20)
        
        print("‚úÖ Test adaptacji zako≈Ñczony")
        
    else:
        print(f"‚ùå Nieznany scenariusz: {scenario}")

# Dodatkowa metoda do klasy AdvancedLoopDRMSystem
def _train_epochs(self, epochs):
    """Pomocnicza metoda do trenowania okre≈õlonej liczby epok"""
    if not self.training_inputs or not self.training_targets:
        print("‚ùå Brak danych treningowych")
        return
    
    inputs_tensor = torch.FloatTensor(self.training_inputs)
    targets_tensor = torch.FloatTensor(self.training_targets)
    
    for epoch in range(epochs):
        outputs = self.model(inputs_tensor)
        loss = self.criterion(outputs, targets_tensor)
        
        accuracy = self.calculate_prediction_accuracy(outputs, targets_tensor)
        FRZ = self.rls.calculate_FRZ(loss.item(), accuracy)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # Zastosuj DRM dla pr√≥bki kontekst√≥w
        sample_size = min(10, len(self.training_inputs))
        for i in range(0, len(self.training_inputs), len(self.training_inputs)//sample_size):
            self.drm.apply_rules(self.training_inputs[i], FRZ)
        
        difference_detected = self.rls.detect_difference(loss.item())
        
        epoch_metrics = {
            'epoch': epoch + 1,
            'loss': loss.item(),
            'accuracy': accuracy,
            'FRZ': FRZ,
            'system_mode': self.drm.mode.value,
            'avg_rule_strength': self.drm.calculate_system_average_strength(),
            'active_rules': len(self.drm.rules),
            'difference_detected': difference_detected
        }
        self.training_history.append(epoch_metrics)

# Dodaj metodƒô do klasy AdvancedLoopDRMSystem
AdvancedLoopDRMSystem._train_epochs = _train_epochs

# Finalne menu rozszerzone
def show_extended_menu():
    """Poka≈º rozszerzone menu opcji"""
    print("\n" + "="*60)
    print("           üöÄ ZAAWANSOWANY SYSTEM LOOPDRM")
    print("="*60)
    print("üìã MENU G≈Å√ìWNE:")
    print("  1. üéØ Dodaj dane treningowe")
    print("  2. üß† Trenuj sieƒá neuronowƒÖ")
    print("  3. üß™ Testuj model")
    print("  4. üíæ Zapisz model")
    print("  5. üìÇ Wczytaj model")
    print("  6. ‚öôÔ∏è  Konfiguracja systemu")
    print("  7. üìä Analiza wydajno≈õci")
    print("  8. üì§ Eksport danych")
    print()
    print("üîß NARZƒòDZIA ZAAWANSOWANE:")
    print("  9. üîç Szczeg√≥≈Çowe statystyki DRM")
    print(" 10. üé¨ Testy scenariuszy")
    print(" 11. ‚ö° Szybka demonstracja")
    print(" 12. ü§ñ Test automatyczny")
    print(" 13. üî¨ Diagnoza problem√≥w")
    print(" 14. üêõ Stan systemu (debug)")
    print(" 15. ‚ÑπÔ∏è  Informacje o systemie")
    print()
    print("  0. üö™ Wyj≈õcie")
    print("="*60)

# Rozszerzona metoda run_advanced_menu
def run_extended_advanced_menu(self):
    """Uruchom rozszerzone menu zaawansowane"""
    while True:
        show_extended_menu()
        
        try:
            choice = input("\nüéØ Wybierz opcjƒô: ").strip()
            
            if choice == "1":
                self.add_training_data()
            elif choice == "2":
                self.train_network()
            elif choice == "3":
                self.test_model()
            elif choice == "4":
                self.save_model()
            elif choice == "5":
                self.load_model()
            elif choice == "6":
                self.configure_system()
            elif choice == "7":
                self.analyze_performance()
            elif choice == "8":
                self.export_data()
            elif choice == "9":
                stats = self.drm.get_detailed_statistics()
                print("\nüìä SZCZEG√ì≈ÅOWE STATYSTYKI DRM:")
                print(json.dumps(stats, indent=2, ensure_ascii=False))
            elif choice == "10":
                print("\nDostƒôpne scenariusze:")
                print("1. basic - Podstawowy test")
                print("2. stress - Test obciƒÖ≈ºeniowy")
                print("3. adaptation - Test adaptacji")
                scenario_choice = input("Wybierz scenariusz: ").strip()
                scenarios = {"1": "basic", "2": "stress", "3": "adaptation"}
                run_scenario_test(scenarios.get(scenario_choice, "basic"))
            elif choice == "11":
                create_quick_demo()
            elif choice == "12":
                run_automated_test()
            elif choice == "13":
                diagnose_training_issues(self)
            elif choice == "14":
                debug_system_state(self)
            elif choice == "15":
                print_system_info()
            elif choice == "0":
                print("üëã Dziƒôkujemy za korzystanie z systemu LoopDRM!")
                break
            else:
                print("‚ùå Nieprawid≈Çowy wyb√≥r. Spr√≥buj ponownie.")
                
        except KeyboardInterrupt:
            print("\n\n‚ö†Ô∏è Przerwano przez u≈ºytkownika.")
            break
        except Exception as e:
            print(f"\n‚ùå B≈ÇƒÖd: {e}")
            print("Kontynuowanie dzia≈Çania...")

# ZastƒÖp oryginalnƒÖ metodƒô
AdvancedLoopDRMSystem.run_advanced_menu = run_extended_advanced_menu

# Ko≈Ñcowe informacje
print("\n" + "üéâ" * 20)
print("   SYSTEM LOOPDRM GOTOWY DO U≈ªYCIA!")
print("üéâ" * 20)
print("\nüìö Aby rozpoczƒÖƒá, uruchom:")
print("   python loopdrm.py")
print("\nüîó Lub u≈ºyj w kodzie:")
print("   from loopdrm import AdvancedLoopDRMSystem")
print("   system = AdvancedLoopDRMSystem()")
print("   system.run_advanced_menu()")
print("\n‚ú® Powodzenia w eksperymentach z zaawansowanym systemem LoopDRM!")
print("üöÄ System zawiera pe≈ÇnƒÖ implementacjƒô matematycznych wzor√≥w DRM")
print("üéØ Gotowy do bada≈Ñ nad adaptacyjnymi systemami uczenia maszynowego")
print("\n" + "="*60)


